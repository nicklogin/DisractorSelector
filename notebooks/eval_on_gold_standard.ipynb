{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard = pd.read_csv(\n",
    "    \"gold_standard/gold_standard_annot_final.csv\",\n",
    "    sep=\";\",\n",
    "    index_col=\"Unnamed: 0\"\n",
    ")\n",
    "gold_standard[\"variants\"] = gold_standard[\"variants\"].apply(literal_eval)\n",
    "gold_standard[\"Appropriate\"] = gold_standard[\"Appropriate\"].apply(literal_eval)\n",
    "gold_standard[\"Too good\"] = gold_standard[\"Too good\"].apply(literal_eval)\n",
    "gold_standard[\"Too bad\"] = gold_standard[\"Too bad\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Masked_sentence</th>\n",
       "      <th>Right_answer</th>\n",
       "      <th>Wrong_answer</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Delete</th>\n",
       "      <th>variants</th>\n",
       "      <th>Appropriate</th>\n",
       "      <th>Too bad</th>\n",
       "      <th>Too good</th>\n",
       "      <th>Consistent</th>\n",
       "      <th>In duplicate names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153493</th>\n",
       "      <td>The amount of people who has no occupation in...</td>\n",
       "      <td>stable</td>\n",
       "      <td>the same</td>\n",
       "      <td>exam/Exam2017/OBy_100-200/2017_OBy_120_1</td>\n",
       "      <td>0</td>\n",
       "      <td>[state, dependable, consistent, steady, prospe...</td>\n",
       "      <td>[consistent, harmonious, coherent]</td>\n",
       "      <td>[state, dependable, prosperous, volatile, reli...</td>\n",
       "      <td>[steady]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83294</th>\n",
       "      <td>Some politicians have come up with an idea to ...</td>\n",
       "      <td>disadvantages</td>\n",
       "      <td>backwards</td>\n",
       "      <td>exam/Exam2017/EGe_1-99/2017_EGe_19_2</td>\n",
       "      <td>0</td>\n",
       "      <td>[cons, limitations, shortcomings, weaknesses, ...</td>\n",
       "      <td>[cons, limitations, weaknesses, pitfalls, prob...</td>\n",
       "      <td>[benefits, characteristics, alternatives, opti...</td>\n",
       "      <td>[shortcomings, risks, challenges, dangers, haz...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77723</th>\n",
       "      <td>As for disadvantages, global warming and air ...</td>\n",
       "      <td>number</td>\n",
       "      <td>amount</td>\n",
       "      <td>exam/Exam2016/2016_MTsy_8_2</td>\n",
       "      <td>0</td>\n",
       "      <td>[amount, quantity, level, part, value, member,...</td>\n",
       "      <td>[amount, quantity, count, rate, multiplicity]</td>\n",
       "      <td>[level, part, value, member, mark, category, p...</td>\n",
       "      <td>[proportion, multitude]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74220</th>\n",
       "      <td>It is slightly below 30°C in Yakutsk and 30°C...</td>\n",
       "      <td>trend</td>\n",
       "      <td>tendency</td>\n",
       "      <td>exam/Exam2017/ESa_1-69/2017_ESa_69_1</td>\n",
       "      <td>0</td>\n",
       "      <td>[tendency, consistency, phenomenon, resurgence...</td>\n",
       "      <td>[tendency, upsurge, pattern, paradigm]</td>\n",
       "      <td>[consistency, phenomenon, resurgence, craze, f...</td>\n",
       "      <td>[shift]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53390</th>\n",
       "      <td>The number of men who are aged between 15 and...</td>\n",
       "      <td>number</td>\n",
       "      <td>part</td>\n",
       "      <td>exam/Exam2014/2014_EPa_22_1</td>\n",
       "      <td>0</td>\n",
       "      <td>[amount, quantity, level, value, member, count...</td>\n",
       "      <td>[amount, quantity, count, rate, portion, total]</td>\n",
       "      <td>[level, value, member, mark, category, quality...</td>\n",
       "      <td>[proportion]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153675</th>\n",
       "      <td>So, the national population usually becomes p...</td>\n",
       "      <td>take</td>\n",
       "      <td>move</td>\n",
       "      <td>exam/Exam2020/Task_2_Essays_919_1896/2020_MLa_...</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, drink, get, move, share, go, taking, gi...</td>\n",
       "      <td>[get, move, hold, bring, send, carry]</td>\n",
       "      <td>[make, drink, share, go, taking, give, relinqu...</td>\n",
       "      <td>[keep]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138436</th>\n",
       "      <td>7 minutes). Just in one case women's group of ...</td>\n",
       "      <td>doing</td>\n",
       "      <td>of goind</td>\n",
       "      <td>exam/Exam2020/Task_2_Essays_919_1896/2020_MLa_...</td>\n",
       "      <td>0</td>\n",
       "      <td>[making, for, getting, pursuing, accomplishing...</td>\n",
       "      <td>[making, getting, pursuing, performing, going,...</td>\n",
       "      <td>[for, accomplishing, happening, seeing, indulg...</td>\n",
       "      <td>[enjoying, practicing]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73447</th>\n",
       "      <td>The decreasing unemployment in Latin America ...</td>\n",
       "      <td>acute</td>\n",
       "      <td>sharp</td>\n",
       "      <td>exam/Exam2017/NMya_1-108/2017_NMya_77_1</td>\n",
       "      <td>0</td>\n",
       "      <td>[sharp, chronic, symptomatic, febrile, respira...</td>\n",
       "      <td>[sharp, incurable]</td>\n",
       "      <td>[chronic, symptomatic, febrile, respiratory, e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160682</th>\n",
       "      <td>Low discipline in schools tends to result in ...</td>\n",
       "      <td>improving</td>\n",
       "      <td>repairing</td>\n",
       "      <td>exam/Old_Exam2014/2014_ZEv_5_2</td>\n",
       "      <td>0</td>\n",
       "      <td>[repairing, enhancing, reducing, strengthening...</td>\n",
       "      <td>[repairing, enhancing, strengthening, boosting...</td>\n",
       "      <td>[reducing, alleviating, modernizing, lowering]</td>\n",
       "      <td>[maintaining, furthering, fostering, ensuring,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42611</th>\n",
       "      <td>Nowadays many people talk about food productio...</td>\n",
       "      <td>food</td>\n",
       "      <td>meal</td>\n",
       "      <td>exam/Exam2017/ESa_1-69/2017_ESa_31_2</td>\n",
       "      <td>0</td>\n",
       "      <td>[meal, goal, seafood, meat, nutrition, beverag...</td>\n",
       "      <td>[meal, nutrition, eating, sustenance]</td>\n",
       "      <td>[goal, seafood, meat, beverage, bread, vegetab...</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Masked_sentence   Right_answer  \\\n",
       "153493   The amount of people who has no occupation in...         stable   \n",
       "83294   Some politicians have come up with an idea to ...  disadvantages   \n",
       "77723    As for disadvantages, global warming and air ...         number   \n",
       "74220    It is slightly below 30°C in Yakutsk and 30°C...          trend   \n",
       "53390    The number of men who are aged between 15 and...         number   \n",
       "...                                                   ...            ...   \n",
       "153675   So, the national population usually becomes p...           take   \n",
       "138436  7 minutes). Just in one case women's group of ...          doing   \n",
       "73447    The decreasing unemployment in Latin America ...          acute   \n",
       "160682   Low discipline in schools tends to result in ...      improving   \n",
       "42611   Nowadays many people talk about food productio...           food   \n",
       "\n",
       "       Wrong_answer                                           Filename  \\\n",
       "153493     the same           exam/Exam2017/OBy_100-200/2017_OBy_120_1   \n",
       "83294     backwards               exam/Exam2017/EGe_1-99/2017_EGe_19_2   \n",
       "77723        amount                        exam/Exam2016/2016_MTsy_8_2   \n",
       "74220      tendency               exam/Exam2017/ESa_1-69/2017_ESa_69_1   \n",
       "53390          part                        exam/Exam2014/2014_EPa_22_1   \n",
       "...             ...                                                ...   \n",
       "153675         move  exam/Exam2020/Task_2_Essays_919_1896/2020_MLa_...   \n",
       "138436     of goind  exam/Exam2020/Task_2_Essays_919_1896/2020_MLa_...   \n",
       "73447         sharp            exam/Exam2017/NMya_1-108/2017_NMya_77_1   \n",
       "160682    repairing                     exam/Old_Exam2014/2014_ZEv_5_2   \n",
       "42611          meal               exam/Exam2017/ESa_1-69/2017_ESa_31_2   \n",
       "\n",
       "        Delete                                           variants  \\\n",
       "153493       0  [state, dependable, consistent, steady, prospe...   \n",
       "83294        0  [cons, limitations, shortcomings, weaknesses, ...   \n",
       "77723        0  [amount, quantity, level, part, value, member,...   \n",
       "74220        0  [tendency, consistency, phenomenon, resurgence...   \n",
       "53390        0  [amount, quantity, level, value, member, count...   \n",
       "...        ...                                                ...   \n",
       "153675       0  [make, drink, get, move, share, go, taking, gi...   \n",
       "138436       0  [making, for, getting, pursuing, accomplishing...   \n",
       "73447        0  [sharp, chronic, symptomatic, febrile, respira...   \n",
       "160682       0  [repairing, enhancing, reducing, strengthening...   \n",
       "42611        0  [meal, goal, seafood, meat, nutrition, beverag...   \n",
       "\n",
       "                                              Appropriate  \\\n",
       "153493                 [consistent, harmonious, coherent]   \n",
       "83294   [cons, limitations, weaknesses, pitfalls, prob...   \n",
       "77723       [amount, quantity, count, rate, multiplicity]   \n",
       "74220              [tendency, upsurge, pattern, paradigm]   \n",
       "53390     [amount, quantity, count, rate, portion, total]   \n",
       "...                                                   ...   \n",
       "153675              [get, move, hold, bring, send, carry]   \n",
       "138436  [making, getting, pursuing, performing, going,...   \n",
       "73447                                  [sharp, incurable]   \n",
       "160682  [repairing, enhancing, strengthening, boosting...   \n",
       "42611               [meal, nutrition, eating, sustenance]   \n",
       "\n",
       "                                                  Too bad  \\\n",
       "153493  [state, dependable, prosperous, volatile, reli...   \n",
       "83294   [benefits, characteristics, alternatives, opti...   \n",
       "77723   [level, part, value, member, mark, category, p...   \n",
       "74220   [consistency, phenomenon, resurgence, craze, f...   \n",
       "53390   [level, value, member, mark, category, quality...   \n",
       "...                                                   ...   \n",
       "153675  [make, drink, share, go, taking, give, relinqu...   \n",
       "138436  [for, accomplishing, happening, seeing, indulg...   \n",
       "73447   [chronic, symptomatic, febrile, respiratory, e...   \n",
       "160682     [reducing, alleviating, modernizing, lowering]   \n",
       "42611   [goal, seafood, meat, beverage, bread, vegetab...   \n",
       "\n",
       "                                                 Too good  Consistent  \\\n",
       "153493                                           [steady]        True   \n",
       "83294   [shortcomings, risks, challenges, dangers, haz...        True   \n",
       "77723                             [proportion, multitude]        True   \n",
       "74220                                             [shift]        True   \n",
       "53390                                        [proportion]        True   \n",
       "...                                                   ...         ...   \n",
       "153675                                             [keep]        True   \n",
       "138436                             [enjoying, practicing]        True   \n",
       "73447                                                  []        True   \n",
       "160682  [maintaining, furthering, fostering, ensuring,...        True   \n",
       "42611                                                  []        True   \n",
       "\n",
       "        In duplicate names  \n",
       "153493               False  \n",
       "83294                False  \n",
       "77723                False  \n",
       "74220                False  \n",
       "53390                False  \n",
       "...                    ...  \n",
       "153675               False  \n",
       "138436               False  \n",
       "73447                False  \n",
       "160682               False  \n",
       "42611                False  \n",
       "\n",
       "[76 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для итогового тестирования на золотом стандарте были отобраны модели с лучшей F1-мерой в различных условиях – модель XGBСlassifier, обученная на датасете со всеми признаками (XGBAllFeats), модель Случайного леса, обученная только на частотах (RandomForestFreqsOnly), модель CatBoost, обученная только на векторных представлениях (CatBoostVecsOnly) и модель CatBoost, обученная без учёта признака «Word2Vec-вектор слова-исправления» (CatBoostFeatDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, json\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed_dataset_final.csv\",sep=';',index_col=\"index\")\n",
    "\n",
    "train_sents, test_sents = train_test_split(\n",
    "    df[\"sent_id\"].unique(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_train = df.loc[\n",
    "    df[\"sent_id\"].isin(train_sents)\n",
    "]\n",
    "df_test = df.loc[\n",
    "    df[\"sent_id\"].isin(test_sents)\n",
    "]\n",
    "\n",
    "X_train, y_train = df_train.drop(\n",
    "    [\"target\", \"target_true\", \"sent_id\"],\n",
    "    axis=1\n",
    "), df_train[\"target_true\"]\n",
    "X_test, y_test = df_test.drop(\n",
    "    [\"target\", \"target_true\", \"sent_id\"],\n",
    "    axis=1,\n",
    "), df_test[\"target_true\"]\n",
    "\n",
    "feats = [\n",
    "    \"bm\",\n",
    "    \"wvc\",\n",
    "    \"wve\",\n",
    "    \"freq_corr\", \n",
    "    \"freq_err_corr\",\n",
    "    \"freq_corr_corp\",\n",
    "    \"freq_err_corp\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_clf(\n",
    "    clf: Any,\n",
    "    cols: List,\n",
    "    clf_name: str\n",
    "):\n",
    "    clf.fit(X_train[cols], y_train)\n",
    "\n",
    "    if not os.path.exists(clf_name):\n",
    "        os.mkdir(clf_name)\n",
    "    \n",
    "    with open(f\"{clf_name}/clf.pkl\", 'wb') as outp:\n",
    "        pickle.dump(clf, outp)\n",
    "    \n",
    "    with open(f\"{clf_name}/cols.json\", 'w', encoding='utf8') as outp:\n",
    "        json.dump(cols, outp, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBAllFeats\n",
    "cols = [col for col in X_train.columns]\n",
    "XGBAllFeats = XGBClassifier(random_state=42)\n",
    "train_and_save_clf(XGBAllFeats, cols, \"XGBAllFeats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestFreqsOnly\n",
    "cols = [\"freq_err_corp\",\"freq_err_corr\",\"freq_corr\",\"freq_corr_corp\"]\n",
    "RandomForestFreqsOnly = RandomForestClassifier(random_state=42)\n",
    "train_and_save_clf(RandomForestFreqsOnly, cols, \"RandomForestFreqsOnly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.014415\n",
      "0:\tlearn: 0.6883157\ttotal: 430ms\tremaining: 7m 9s\n",
      "1:\tlearn: 0.6843094\ttotal: 779ms\tremaining: 6m 28s\n",
      "2:\tlearn: 0.6807621\ttotal: 1.15s\tremaining: 6m 23s\n",
      "3:\tlearn: 0.6775071\ttotal: 1.54s\tremaining: 6m 22s\n",
      "4:\tlearn: 0.6735267\ttotal: 1.89s\tremaining: 6m 16s\n",
      "5:\tlearn: 0.6693692\ttotal: 2.35s\tremaining: 6m 29s\n",
      "6:\tlearn: 0.6666885\ttotal: 2.77s\tremaining: 6m 33s\n",
      "7:\tlearn: 0.6633389\ttotal: 3.11s\tremaining: 6m 25s\n",
      "8:\tlearn: 0.6597938\ttotal: 3.44s\tremaining: 6m 18s\n",
      "9:\tlearn: 0.6562094\ttotal: 3.81s\tremaining: 6m 17s\n",
      "10:\tlearn: 0.6534068\ttotal: 4.12s\tremaining: 6m 10s\n",
      "11:\tlearn: 0.6507970\ttotal: 4.5s\tremaining: 6m 10s\n",
      "12:\tlearn: 0.6487412\ttotal: 4.83s\tremaining: 6m 6s\n",
      "13:\tlearn: 0.6463601\ttotal: 5.19s\tremaining: 6m 5s\n",
      "14:\tlearn: 0.6437416\ttotal: 5.52s\tremaining: 6m 2s\n",
      "15:\tlearn: 0.6413600\ttotal: 5.94s\tremaining: 6m 5s\n",
      "16:\tlearn: 0.6391060\ttotal: 6.29s\tremaining: 6m 3s\n",
      "17:\tlearn: 0.6373321\ttotal: 6.64s\tremaining: 6m 2s\n",
      "18:\tlearn: 0.6345117\ttotal: 6.98s\tremaining: 6m\n",
      "19:\tlearn: 0.6324453\ttotal: 7.28s\tremaining: 5m 56s\n",
      "20:\tlearn: 0.6306713\ttotal: 7.56s\tremaining: 5m 52s\n",
      "21:\tlearn: 0.6285990\ttotal: 7.84s\tremaining: 5m 48s\n",
      "22:\tlearn: 0.6267787\ttotal: 8.13s\tremaining: 5m 45s\n",
      "23:\tlearn: 0.6248742\ttotal: 8.35s\tremaining: 5m 39s\n",
      "24:\tlearn: 0.6230858\ttotal: 8.57s\tremaining: 5m 34s\n",
      "25:\tlearn: 0.6212655\ttotal: 8.78s\tremaining: 5m 29s\n",
      "26:\tlearn: 0.6192335\ttotal: 9s\tremaining: 5m 24s\n",
      "27:\tlearn: 0.6175246\ttotal: 9.21s\tremaining: 5m 19s\n",
      "28:\tlearn: 0.6153846\ttotal: 9.42s\tremaining: 5m 15s\n",
      "29:\tlearn: 0.6137549\ttotal: 9.64s\tremaining: 5m 11s\n",
      "30:\tlearn: 0.6119623\ttotal: 9.85s\tremaining: 5m 7s\n",
      "31:\tlearn: 0.6102793\ttotal: 10.1s\tremaining: 5m 4s\n",
      "32:\tlearn: 0.6078317\ttotal: 10.3s\tremaining: 5m 1s\n",
      "33:\tlearn: 0.6062627\ttotal: 10.5s\tremaining: 4m 58s\n",
      "34:\tlearn: 0.6045927\ttotal: 10.7s\tremaining: 4m 55s\n",
      "35:\tlearn: 0.6030146\ttotal: 10.9s\tremaining: 4m 52s\n",
      "36:\tlearn: 0.6015114\ttotal: 11.1s\tremaining: 4m 49s\n",
      "37:\tlearn: 0.5995217\ttotal: 11.4s\tremaining: 4m 47s\n",
      "38:\tlearn: 0.5977033\ttotal: 11.6s\tremaining: 4m 44s\n",
      "39:\tlearn: 0.5955099\ttotal: 11.8s\tremaining: 4m 42s\n",
      "40:\tlearn: 0.5937041\ttotal: 12s\tremaining: 4m 40s\n",
      "41:\tlearn: 0.5923844\ttotal: 12.2s\tremaining: 4m 38s\n",
      "42:\tlearn: 0.5911384\ttotal: 12.4s\tremaining: 4m 36s\n",
      "43:\tlearn: 0.5898546\ttotal: 12.6s\tremaining: 4m 34s\n",
      "44:\tlearn: 0.5884975\ttotal: 12.9s\tremaining: 4m 32s\n",
      "45:\tlearn: 0.5877195\ttotal: 13.1s\tremaining: 4m 30s\n",
      "46:\tlearn: 0.5864953\ttotal: 13.3s\tremaining: 4m 29s\n",
      "47:\tlearn: 0.5850406\ttotal: 13.5s\tremaining: 4m 27s\n",
      "48:\tlearn: 0.5834450\ttotal: 13.7s\tremaining: 4m 26s\n",
      "49:\tlearn: 0.5821077\ttotal: 13.9s\tremaining: 4m 24s\n",
      "50:\tlearn: 0.5811434\ttotal: 14.2s\tremaining: 4m 23s\n",
      "51:\tlearn: 0.5800879\ttotal: 14.4s\tremaining: 4m 22s\n",
      "52:\tlearn: 0.5786849\ttotal: 14.6s\tremaining: 4m 20s\n",
      "53:\tlearn: 0.5773916\ttotal: 14.8s\tremaining: 4m 19s\n",
      "54:\tlearn: 0.5760980\ttotal: 15s\tremaining: 4m 18s\n",
      "55:\tlearn: 0.5745607\ttotal: 15.2s\tremaining: 4m 17s\n",
      "56:\tlearn: 0.5732063\ttotal: 15.5s\tremaining: 4m 15s\n",
      "57:\tlearn: 0.5721156\ttotal: 15.7s\tremaining: 4m 14s\n",
      "58:\tlearn: 0.5707645\ttotal: 15.9s\tremaining: 4m 13s\n",
      "59:\tlearn: 0.5698491\ttotal: 16.1s\tremaining: 4m 12s\n",
      "60:\tlearn: 0.5689790\ttotal: 16.3s\tremaining: 4m 11s\n",
      "61:\tlearn: 0.5680862\ttotal: 16.5s\tremaining: 4m 10s\n",
      "62:\tlearn: 0.5670036\ttotal: 16.8s\tremaining: 4m 9s\n",
      "63:\tlearn: 0.5658459\ttotal: 17s\tremaining: 4m 8s\n",
      "64:\tlearn: 0.5650853\ttotal: 17.2s\tremaining: 4m 7s\n",
      "65:\tlearn: 0.5640534\ttotal: 17.4s\tremaining: 4m 6s\n",
      "66:\tlearn: 0.5629986\ttotal: 17.6s\tremaining: 4m 5s\n",
      "67:\tlearn: 0.5618488\ttotal: 17.8s\tremaining: 4m 4s\n",
      "68:\tlearn: 0.5605806\ttotal: 18s\tremaining: 4m 3s\n",
      "69:\tlearn: 0.5591452\ttotal: 18.3s\tremaining: 4m 2s\n",
      "70:\tlearn: 0.5575861\ttotal: 18.5s\tremaining: 4m 1s\n",
      "71:\tlearn: 0.5565411\ttotal: 18.7s\tremaining: 4m\n",
      "72:\tlearn: 0.5557000\ttotal: 18.9s\tremaining: 4m\n",
      "73:\tlearn: 0.5547413\ttotal: 19.1s\tremaining: 3m 59s\n",
      "74:\tlearn: 0.5538818\ttotal: 19.3s\tremaining: 3m 58s\n",
      "75:\tlearn: 0.5531343\ttotal: 19.5s\tremaining: 3m 57s\n",
      "76:\tlearn: 0.5519520\ttotal: 19.8s\tremaining: 3m 56s\n",
      "77:\tlearn: 0.5510028\ttotal: 20s\tremaining: 3m 56s\n",
      "78:\tlearn: 0.5497926\ttotal: 20.2s\tremaining: 3m 55s\n",
      "79:\tlearn: 0.5487971\ttotal: 20.4s\tremaining: 3m 54s\n",
      "80:\tlearn: 0.5475151\ttotal: 20.6s\tremaining: 3m 53s\n",
      "81:\tlearn: 0.5465361\ttotal: 20.8s\tremaining: 3m 53s\n",
      "82:\tlearn: 0.5457641\ttotal: 21.1s\tremaining: 3m 52s\n",
      "83:\tlearn: 0.5448308\ttotal: 21.3s\tremaining: 3m 51s\n",
      "84:\tlearn: 0.5438023\ttotal: 21.5s\tremaining: 3m 51s\n",
      "85:\tlearn: 0.5428740\ttotal: 21.7s\tremaining: 3m 50s\n",
      "86:\tlearn: 0.5421208\ttotal: 21.9s\tremaining: 3m 49s\n",
      "87:\tlearn: 0.5413930\ttotal: 22.1s\tremaining: 3m 49s\n",
      "88:\tlearn: 0.5401097\ttotal: 22.3s\tremaining: 3m 48s\n",
      "89:\tlearn: 0.5386256\ttotal: 22.6s\tremaining: 3m 48s\n",
      "90:\tlearn: 0.5376784\ttotal: 22.8s\tremaining: 3m 47s\n",
      "91:\tlearn: 0.5368359\ttotal: 23s\tremaining: 3m 46s\n",
      "92:\tlearn: 0.5363599\ttotal: 23.2s\tremaining: 3m 46s\n",
      "93:\tlearn: 0.5352927\ttotal: 23.4s\tremaining: 3m 45s\n",
      "94:\tlearn: 0.5343319\ttotal: 23.6s\tremaining: 3m 45s\n",
      "95:\tlearn: 0.5332071\ttotal: 23.8s\tremaining: 3m 44s\n",
      "96:\tlearn: 0.5323081\ttotal: 24s\tremaining: 3m 43s\n",
      "97:\tlearn: 0.5315197\ttotal: 24.3s\tremaining: 3m 43s\n",
      "98:\tlearn: 0.5309604\ttotal: 24.5s\tremaining: 3m 42s\n",
      "99:\tlearn: 0.5301794\ttotal: 24.7s\tremaining: 3m 42s\n",
      "100:\tlearn: 0.5295082\ttotal: 24.9s\tremaining: 3m 41s\n",
      "101:\tlearn: 0.5285562\ttotal: 25.1s\tremaining: 3m 41s\n",
      "102:\tlearn: 0.5275902\ttotal: 25.3s\tremaining: 3m 40s\n",
      "103:\tlearn: 0.5268397\ttotal: 25.5s\tremaining: 3m 40s\n",
      "104:\tlearn: 0.5262238\ttotal: 25.8s\tremaining: 3m 39s\n",
      "105:\tlearn: 0.5253972\ttotal: 26s\tremaining: 3m 39s\n",
      "106:\tlearn: 0.5246394\ttotal: 26.2s\tremaining: 3m 38s\n",
      "107:\tlearn: 0.5237658\ttotal: 26.4s\tremaining: 3m 38s\n",
      "108:\tlearn: 0.5231349\ttotal: 26.6s\tremaining: 3m 37s\n",
      "109:\tlearn: 0.5224524\ttotal: 26.8s\tremaining: 3m 37s\n",
      "110:\tlearn: 0.5215365\ttotal: 27s\tremaining: 3m 36s\n",
      "111:\tlearn: 0.5208242\ttotal: 27.3s\tremaining: 3m 36s\n",
      "112:\tlearn: 0.5200423\ttotal: 27.5s\tremaining: 3m 35s\n",
      "113:\tlearn: 0.5190546\ttotal: 27.7s\tremaining: 3m 35s\n",
      "114:\tlearn: 0.5182367\ttotal: 27.9s\tremaining: 3m 34s\n",
      "115:\tlearn: 0.5175860\ttotal: 28.1s\tremaining: 3m 34s\n",
      "116:\tlearn: 0.5167937\ttotal: 28.3s\tremaining: 3m 33s\n",
      "117:\tlearn: 0.5161761\ttotal: 28.5s\tremaining: 3m 33s\n",
      "118:\tlearn: 0.5155626\ttotal: 28.8s\tremaining: 3m 32s\n",
      "119:\tlearn: 0.5148421\ttotal: 29s\tremaining: 3m 32s\n",
      "120:\tlearn: 0.5141908\ttotal: 29.2s\tremaining: 3m 32s\n",
      "121:\tlearn: 0.5133579\ttotal: 29.4s\tremaining: 3m 31s\n",
      "122:\tlearn: 0.5124714\ttotal: 29.6s\tremaining: 3m 31s\n",
      "123:\tlearn: 0.5115175\ttotal: 29.8s\tremaining: 3m 30s\n",
      "124:\tlearn: 0.5108543\ttotal: 30s\tremaining: 3m 30s\n",
      "125:\tlearn: 0.5101719\ttotal: 30.3s\tremaining: 3m 29s\n",
      "126:\tlearn: 0.5094922\ttotal: 30.5s\tremaining: 3m 29s\n",
      "127:\tlearn: 0.5083763\ttotal: 30.7s\tremaining: 3m 29s\n",
      "128:\tlearn: 0.5076553\ttotal: 30.9s\tremaining: 3m 28s\n",
      "129:\tlearn: 0.5072305\ttotal: 31.1s\tremaining: 3m 28s\n",
      "130:\tlearn: 0.5065958\ttotal: 31.3s\tremaining: 3m 27s\n",
      "131:\tlearn: 0.5058678\ttotal: 31.5s\tremaining: 3m 27s\n",
      "132:\tlearn: 0.5052046\ttotal: 31.8s\tremaining: 3m 26s\n",
      "133:\tlearn: 0.5044407\ttotal: 32s\tremaining: 3m 26s\n",
      "134:\tlearn: 0.5036905\ttotal: 32.2s\tremaining: 3m 26s\n",
      "135:\tlearn: 0.5030488\ttotal: 32.4s\tremaining: 3m 25s\n",
      "136:\tlearn: 0.5021071\ttotal: 32.6s\tremaining: 3m 25s\n",
      "137:\tlearn: 0.5016605\ttotal: 32.8s\tremaining: 3m 24s\n",
      "138:\tlearn: 0.5009611\ttotal: 33s\tremaining: 3m 24s\n",
      "139:\tlearn: 0.5001269\ttotal: 33.3s\tremaining: 3m 24s\n",
      "140:\tlearn: 0.4996780\ttotal: 33.5s\tremaining: 3m 23s\n",
      "141:\tlearn: 0.4991422\ttotal: 33.7s\tremaining: 3m 23s\n",
      "142:\tlearn: 0.4982236\ttotal: 33.9s\tremaining: 3m 23s\n",
      "143:\tlearn: 0.4974537\ttotal: 34.1s\tremaining: 3m 22s\n",
      "144:\tlearn: 0.4967844\ttotal: 34.3s\tremaining: 3m 22s\n",
      "145:\tlearn: 0.4962775\ttotal: 34.5s\tremaining: 3m 22s\n",
      "146:\tlearn: 0.4956979\ttotal: 34.8s\tremaining: 3m 21s\n",
      "147:\tlearn: 0.4951535\ttotal: 35s\tremaining: 3m 21s\n",
      "148:\tlearn: 0.4941782\ttotal: 35.2s\tremaining: 3m 20s\n",
      "149:\tlearn: 0.4933431\ttotal: 35.4s\tremaining: 3m 20s\n",
      "150:\tlearn: 0.4927605\ttotal: 35.6s\tremaining: 3m 20s\n",
      "151:\tlearn: 0.4923405\ttotal: 35.8s\tremaining: 3m 19s\n",
      "152:\tlearn: 0.4919005\ttotal: 36s\tremaining: 3m 19s\n",
      "153:\tlearn: 0.4912112\ttotal: 36.3s\tremaining: 3m 19s\n",
      "154:\tlearn: 0.4905467\ttotal: 36.5s\tremaining: 3m 18s\n",
      "155:\tlearn: 0.4898920\ttotal: 36.7s\tremaining: 3m 18s\n",
      "156:\tlearn: 0.4893121\ttotal: 36.9s\tremaining: 3m 18s\n",
      "157:\tlearn: 0.4887777\ttotal: 37.1s\tremaining: 3m 17s\n",
      "158:\tlearn: 0.4881835\ttotal: 37.3s\tremaining: 3m 17s\n",
      "159:\tlearn: 0.4874568\ttotal: 37.5s\tremaining: 3m 17s\n",
      "160:\tlearn: 0.4869292\ttotal: 37.8s\tremaining: 3m 16s\n",
      "161:\tlearn: 0.4861470\ttotal: 38s\tremaining: 3m 16s\n",
      "162:\tlearn: 0.4856288\ttotal: 38.2s\tremaining: 3m 16s\n",
      "163:\tlearn: 0.4849458\ttotal: 38.4s\tremaining: 3m 15s\n",
      "164:\tlearn: 0.4842584\ttotal: 38.6s\tremaining: 3m 15s\n",
      "165:\tlearn: 0.4837137\ttotal: 38.8s\tremaining: 3m 15s\n",
      "166:\tlearn: 0.4831899\ttotal: 39s\tremaining: 3m 14s\n",
      "167:\tlearn: 0.4826036\ttotal: 39.3s\tremaining: 3m 14s\n",
      "168:\tlearn: 0.4820957\ttotal: 39.5s\tremaining: 3m 14s\n",
      "169:\tlearn: 0.4814421\ttotal: 39.7s\tremaining: 3m 13s\n",
      "170:\tlearn: 0.4805938\ttotal: 39.9s\tremaining: 3m 13s\n",
      "171:\tlearn: 0.4802713\ttotal: 40.1s\tremaining: 3m 13s\n",
      "172:\tlearn: 0.4795944\ttotal: 40.3s\tremaining: 3m 12s\n",
      "173:\tlearn: 0.4790710\ttotal: 40.5s\tremaining: 3m 12s\n",
      "174:\tlearn: 0.4785672\ttotal: 40.8s\tremaining: 3m 12s\n",
      "175:\tlearn: 0.4783097\ttotal: 41s\tremaining: 3m 11s\n",
      "176:\tlearn: 0.4777226\ttotal: 41.2s\tremaining: 3m 11s\n",
      "177:\tlearn: 0.4771205\ttotal: 41.4s\tremaining: 3m 11s\n",
      "178:\tlearn: 0.4764485\ttotal: 41.6s\tremaining: 3m 10s\n",
      "179:\tlearn: 0.4756780\ttotal: 41.8s\tremaining: 3m 10s\n",
      "180:\tlearn: 0.4752932\ttotal: 42s\tremaining: 3m 10s\n",
      "181:\tlearn: 0.4745025\ttotal: 42.3s\tremaining: 3m 9s\n",
      "182:\tlearn: 0.4739942\ttotal: 42.5s\tremaining: 3m 9s\n",
      "183:\tlearn: 0.4733839\ttotal: 42.7s\tremaining: 3m 9s\n",
      "184:\tlearn: 0.4725534\ttotal: 42.9s\tremaining: 3m 8s\n",
      "185:\tlearn: 0.4719379\ttotal: 43.1s\tremaining: 3m 8s\n",
      "186:\tlearn: 0.4712397\ttotal: 43.3s\tremaining: 3m 8s\n",
      "187:\tlearn: 0.4707048\ttotal: 43.5s\tremaining: 3m 8s\n",
      "188:\tlearn: 0.4703271\ttotal: 43.7s\tremaining: 3m 7s\n",
      "189:\tlearn: 0.4698048\ttotal: 44s\tremaining: 3m 7s\n",
      "190:\tlearn: 0.4691262\ttotal: 44.2s\tremaining: 3m 7s\n",
      "191:\tlearn: 0.4686580\ttotal: 44.4s\tremaining: 3m 6s\n",
      "192:\tlearn: 0.4681717\ttotal: 44.6s\tremaining: 3m 6s\n",
      "193:\tlearn: 0.4677078\ttotal: 44.8s\tremaining: 3m 6s\n",
      "194:\tlearn: 0.4669055\ttotal: 45s\tremaining: 3m 5s\n",
      "195:\tlearn: 0.4663137\ttotal: 45.2s\tremaining: 3m 5s\n",
      "196:\tlearn: 0.4657442\ttotal: 45.5s\tremaining: 3m 5s\n",
      "197:\tlearn: 0.4652188\ttotal: 45.7s\tremaining: 3m 4s\n",
      "198:\tlearn: 0.4645877\ttotal: 45.9s\tremaining: 3m 4s\n",
      "199:\tlearn: 0.4639396\ttotal: 46.1s\tremaining: 3m 4s\n",
      "200:\tlearn: 0.4635314\ttotal: 46.3s\tremaining: 3m 4s\n",
      "201:\tlearn: 0.4629899\ttotal: 46.5s\tremaining: 3m 3s\n",
      "202:\tlearn: 0.4623755\ttotal: 46.7s\tremaining: 3m 3s\n",
      "203:\tlearn: 0.4618458\ttotal: 46.9s\tremaining: 3m 3s\n",
      "204:\tlearn: 0.4613209\ttotal: 47.2s\tremaining: 3m 2s\n",
      "205:\tlearn: 0.4606585\ttotal: 47.4s\tremaining: 3m 2s\n",
      "206:\tlearn: 0.4601069\ttotal: 47.6s\tremaining: 3m 2s\n",
      "207:\tlearn: 0.4595403\ttotal: 47.8s\tremaining: 3m 2s\n",
      "208:\tlearn: 0.4589631\ttotal: 48s\tremaining: 3m 1s\n",
      "209:\tlearn: 0.4583533\ttotal: 48.2s\tremaining: 3m 1s\n",
      "210:\tlearn: 0.4576649\ttotal: 48.5s\tremaining: 3m 1s\n",
      "211:\tlearn: 0.4573099\ttotal: 48.7s\tremaining: 3m\n",
      "212:\tlearn: 0.4567842\ttotal: 48.9s\tremaining: 3m\n",
      "213:\tlearn: 0.4563748\ttotal: 49.1s\tremaining: 3m\n",
      "214:\tlearn: 0.4557701\ttotal: 49.3s\tremaining: 3m\n",
      "215:\tlearn: 0.4553725\ttotal: 49.5s\tremaining: 2m 59s\n",
      "216:\tlearn: 0.4548086\ttotal: 49.8s\tremaining: 2m 59s\n",
      "217:\tlearn: 0.4541109\ttotal: 50s\tremaining: 2m 59s\n",
      "218:\tlearn: 0.4535798\ttotal: 50.2s\tremaining: 2m 58s\n",
      "219:\tlearn: 0.4531069\ttotal: 50.4s\tremaining: 2m 58s\n",
      "220:\tlearn: 0.4525485\ttotal: 50.6s\tremaining: 2m 58s\n",
      "221:\tlearn: 0.4522290\ttotal: 50.8s\tremaining: 2m 58s\n",
      "222:\tlearn: 0.4518342\ttotal: 51.1s\tremaining: 2m 57s\n",
      "223:\tlearn: 0.4513407\ttotal: 51.3s\tremaining: 2m 57s\n",
      "224:\tlearn: 0.4509408\ttotal: 51.5s\tremaining: 2m 57s\n",
      "225:\tlearn: 0.4505231\ttotal: 51.7s\tremaining: 2m 57s\n",
      "226:\tlearn: 0.4499652\ttotal: 51.9s\tremaining: 2m 56s\n",
      "227:\tlearn: 0.4493963\ttotal: 52.1s\tremaining: 2m 56s\n",
      "228:\tlearn: 0.4488484\ttotal: 52.3s\tremaining: 2m 56s\n",
      "229:\tlearn: 0.4480795\ttotal: 52.5s\tremaining: 2m 55s\n",
      "230:\tlearn: 0.4474259\ttotal: 52.8s\tremaining: 2m 55s\n",
      "231:\tlearn: 0.4470464\ttotal: 53s\tremaining: 2m 55s\n",
      "232:\tlearn: 0.4465428\ttotal: 53.2s\tremaining: 2m 55s\n",
      "233:\tlearn: 0.4459506\ttotal: 53.5s\tremaining: 2m 54s\n",
      "234:\tlearn: 0.4454876\ttotal: 53.7s\tremaining: 2m 54s\n",
      "235:\tlearn: 0.4450703\ttotal: 53.9s\tremaining: 2m 54s\n",
      "236:\tlearn: 0.4444349\ttotal: 54.1s\tremaining: 2m 54s\n",
      "237:\tlearn: 0.4437330\ttotal: 54.3s\tremaining: 2m 53s\n",
      "238:\tlearn: 0.4432443\ttotal: 54.6s\tremaining: 2m 53s\n",
      "239:\tlearn: 0.4427427\ttotal: 54.8s\tremaining: 2m 53s\n",
      "240:\tlearn: 0.4423265\ttotal: 55s\tremaining: 2m 53s\n",
      "241:\tlearn: 0.4420197\ttotal: 55.2s\tremaining: 2m 52s\n",
      "242:\tlearn: 0.4414755\ttotal: 55.4s\tremaining: 2m 52s\n",
      "243:\tlearn: 0.4410801\ttotal: 55.7s\tremaining: 2m 52s\n",
      "244:\tlearn: 0.4405759\ttotal: 55.9s\tremaining: 2m 52s\n",
      "245:\tlearn: 0.4400728\ttotal: 56.1s\tremaining: 2m 51s\n",
      "246:\tlearn: 0.4396713\ttotal: 56.3s\tremaining: 2m 51s\n",
      "247:\tlearn: 0.4390386\ttotal: 56.5s\tremaining: 2m 51s\n",
      "248:\tlearn: 0.4385815\ttotal: 56.7s\tremaining: 2m 51s\n",
      "249:\tlearn: 0.4380841\ttotal: 57s\tremaining: 2m 50s\n",
      "250:\tlearn: 0.4375968\ttotal: 57.2s\tremaining: 2m 50s\n",
      "251:\tlearn: 0.4369826\ttotal: 57.4s\tremaining: 2m 50s\n",
      "252:\tlearn: 0.4366972\ttotal: 57.6s\tremaining: 2m 50s\n",
      "253:\tlearn: 0.4362424\ttotal: 57.8s\tremaining: 2m 49s\n",
      "254:\tlearn: 0.4357803\ttotal: 58.1s\tremaining: 2m 49s\n",
      "255:\tlearn: 0.4351050\ttotal: 58.3s\tremaining: 2m 49s\n",
      "256:\tlearn: 0.4346412\ttotal: 58.5s\tremaining: 2m 49s\n",
      "257:\tlearn: 0.4340542\ttotal: 58.7s\tremaining: 2m 48s\n",
      "258:\tlearn: 0.4335934\ttotal: 58.9s\tremaining: 2m 48s\n",
      "259:\tlearn: 0.4331001\ttotal: 59.1s\tremaining: 2m 48s\n",
      "260:\tlearn: 0.4328310\ttotal: 59.4s\tremaining: 2m 48s\n",
      "261:\tlearn: 0.4321988\ttotal: 59.6s\tremaining: 2m 47s\n",
      "262:\tlearn: 0.4317534\ttotal: 59.8s\tremaining: 2m 47s\n",
      "263:\tlearn: 0.4312038\ttotal: 1m\tremaining: 2m 47s\n",
      "264:\tlearn: 0.4307608\ttotal: 1m\tremaining: 2m 47s\n",
      "265:\tlearn: 0.4304184\ttotal: 1m\tremaining: 2m 46s\n",
      "266:\tlearn: 0.4298310\ttotal: 1m\tremaining: 2m 46s\n",
      "267:\tlearn: 0.4293968\ttotal: 1m\tremaining: 2m 46s\n",
      "268:\tlearn: 0.4288110\ttotal: 1m 1s\tremaining: 2m 45s\n",
      "269:\tlearn: 0.4282776\ttotal: 1m 1s\tremaining: 2m 45s\n",
      "270:\tlearn: 0.4278299\ttotal: 1m 1s\tremaining: 2m 45s\n",
      "271:\tlearn: 0.4275646\ttotal: 1m 1s\tremaining: 2m 45s\n",
      "272:\tlearn: 0.4268598\ttotal: 1m 1s\tremaining: 2m 44s\n",
      "273:\tlearn: 0.4262883\ttotal: 1m 2s\tremaining: 2m 44s\n",
      "274:\tlearn: 0.4258289\ttotal: 1m 2s\tremaining: 2m 44s\n",
      "275:\tlearn: 0.4255058\ttotal: 1m 2s\tremaining: 2m 44s\n",
      "276:\tlearn: 0.4250444\ttotal: 1m 2s\tremaining: 2m 43s\n",
      "277:\tlearn: 0.4245800\ttotal: 1m 3s\tremaining: 2m 43s\n",
      "278:\tlearn: 0.4241612\ttotal: 1m 3s\tremaining: 2m 43s\n",
      "279:\tlearn: 0.4238435\ttotal: 1m 3s\tremaining: 2m 43s\n",
      "280:\tlearn: 0.4234140\ttotal: 1m 3s\tremaining: 2m 42s\n",
      "281:\tlearn: 0.4230921\ttotal: 1m 3s\tremaining: 2m 42s\n",
      "282:\tlearn: 0.4224962\ttotal: 1m 4s\tremaining: 2m 42s\n",
      "283:\tlearn: 0.4220443\ttotal: 1m 4s\tremaining: 2m 42s\n",
      "284:\tlearn: 0.4216684\ttotal: 1m 4s\tremaining: 2m 41s\n",
      "285:\tlearn: 0.4209186\ttotal: 1m 4s\tremaining: 2m 41s\n",
      "286:\tlearn: 0.4203978\ttotal: 1m 4s\tremaining: 2m 41s\n",
      "287:\tlearn: 0.4199902\ttotal: 1m 5s\tremaining: 2m 41s\n",
      "288:\tlearn: 0.4196279\ttotal: 1m 5s\tremaining: 2m 40s\n",
      "289:\tlearn: 0.4190682\ttotal: 1m 5s\tremaining: 2m 40s\n",
      "290:\tlearn: 0.4187349\ttotal: 1m 5s\tremaining: 2m 40s\n",
      "291:\tlearn: 0.4182948\ttotal: 1m 5s\tremaining: 2m 40s\n",
      "292:\tlearn: 0.4178396\ttotal: 1m 6s\tremaining: 2m 39s\n",
      "293:\tlearn: 0.4172006\ttotal: 1m 6s\tremaining: 2m 39s\n",
      "294:\tlearn: 0.4168470\ttotal: 1m 6s\tremaining: 2m 39s\n",
      "295:\tlearn: 0.4164055\ttotal: 1m 6s\tremaining: 2m 38s\n",
      "296:\tlearn: 0.4159420\ttotal: 1m 7s\tremaining: 2m 38s\n",
      "297:\tlearn: 0.4156095\ttotal: 1m 7s\tremaining: 2m 38s\n",
      "298:\tlearn: 0.4151880\ttotal: 1m 7s\tremaining: 2m 38s\n",
      "299:\tlearn: 0.4148832\ttotal: 1m 7s\tremaining: 2m 37s\n",
      "300:\tlearn: 0.4144830\ttotal: 1m 7s\tremaining: 2m 37s\n",
      "301:\tlearn: 0.4141592\ttotal: 1m 8s\tremaining: 2m 37s\n",
      "302:\tlearn: 0.4139579\ttotal: 1m 8s\tremaining: 2m 37s\n",
      "303:\tlearn: 0.4135284\ttotal: 1m 8s\tremaining: 2m 36s\n",
      "304:\tlearn: 0.4131528\ttotal: 1m 8s\tremaining: 2m 36s\n",
      "305:\tlearn: 0.4128356\ttotal: 1m 8s\tremaining: 2m 36s\n",
      "306:\tlearn: 0.4125255\ttotal: 1m 9s\tremaining: 2m 36s\n",
      "307:\tlearn: 0.4117584\ttotal: 1m 9s\tremaining: 2m 35s\n",
      "308:\tlearn: 0.4112773\ttotal: 1m 9s\tremaining: 2m 35s\n",
      "309:\tlearn: 0.4107649\ttotal: 1m 9s\tremaining: 2m 35s\n",
      "310:\tlearn: 0.4102397\ttotal: 1m 10s\tremaining: 2m 35s\n",
      "311:\tlearn: 0.4098345\ttotal: 1m 10s\tremaining: 2m 34s\n",
      "312:\tlearn: 0.4094978\ttotal: 1m 10s\tremaining: 2m 34s\n",
      "313:\tlearn: 0.4090784\ttotal: 1m 10s\tremaining: 2m 34s\n",
      "314:\tlearn: 0.4087941\ttotal: 1m 10s\tremaining: 2m 34s\n",
      "315:\tlearn: 0.4084077\ttotal: 1m 11s\tremaining: 2m 34s\n",
      "316:\tlearn: 0.4078879\ttotal: 1m 11s\tremaining: 2m 33s\n",
      "317:\tlearn: 0.4074721\ttotal: 1m 11s\tremaining: 2m 33s\n",
      "318:\tlearn: 0.4069882\ttotal: 1m 11s\tremaining: 2m 33s\n",
      "319:\tlearn: 0.4065979\ttotal: 1m 12s\tremaining: 2m 33s\n",
      "320:\tlearn: 0.4062032\ttotal: 1m 12s\tremaining: 2m 32s\n",
      "321:\tlearn: 0.4058412\ttotal: 1m 12s\tremaining: 2m 32s\n",
      "322:\tlearn: 0.4053401\ttotal: 1m 12s\tremaining: 2m 32s\n",
      "323:\tlearn: 0.4047780\ttotal: 1m 12s\tremaining: 2m 32s\n",
      "324:\tlearn: 0.4044757\ttotal: 1m 13s\tremaining: 2m 31s\n",
      "325:\tlearn: 0.4041450\ttotal: 1m 13s\tremaining: 2m 31s\n",
      "326:\tlearn: 0.4036719\ttotal: 1m 13s\tremaining: 2m 31s\n",
      "327:\tlearn: 0.4031064\ttotal: 1m 13s\tremaining: 2m 31s\n",
      "328:\tlearn: 0.4027923\ttotal: 1m 13s\tremaining: 2m 30s\n",
      "329:\tlearn: 0.4023964\ttotal: 1m 14s\tremaining: 2m 30s\n",
      "330:\tlearn: 0.4021579\ttotal: 1m 14s\tremaining: 2m 30s\n",
      "331:\tlearn: 0.4017336\ttotal: 1m 14s\tremaining: 2m 30s\n",
      "332:\tlearn: 0.4013696\ttotal: 1m 14s\tremaining: 2m 29s\n",
      "333:\tlearn: 0.4010213\ttotal: 1m 15s\tremaining: 2m 29s\n",
      "334:\tlearn: 0.4005598\ttotal: 1m 15s\tremaining: 2m 29s\n",
      "335:\tlearn: 0.4001402\ttotal: 1m 15s\tremaining: 2m 29s\n",
      "336:\tlearn: 0.3995970\ttotal: 1m 15s\tremaining: 2m 28s\n",
      "337:\tlearn: 0.3992600\ttotal: 1m 15s\tremaining: 2m 28s\n",
      "338:\tlearn: 0.3988150\ttotal: 1m 16s\tremaining: 2m 28s\n",
      "339:\tlearn: 0.3983009\ttotal: 1m 16s\tremaining: 2m 28s\n",
      "340:\tlearn: 0.3980013\ttotal: 1m 16s\tremaining: 2m 27s\n",
      "341:\tlearn: 0.3975243\ttotal: 1m 16s\tremaining: 2m 27s\n",
      "342:\tlearn: 0.3970153\ttotal: 1m 16s\tremaining: 2m 27s\n",
      "343:\tlearn: 0.3965325\ttotal: 1m 17s\tremaining: 2m 27s\n",
      "344:\tlearn: 0.3961565\ttotal: 1m 17s\tremaining: 2m 26s\n",
      "345:\tlearn: 0.3957046\ttotal: 1m 17s\tremaining: 2m 26s\n",
      "346:\tlearn: 0.3953408\ttotal: 1m 17s\tremaining: 2m 26s\n",
      "347:\tlearn: 0.3950776\ttotal: 1m 18s\tremaining: 2m 26s\n",
      "348:\tlearn: 0.3945937\ttotal: 1m 18s\tremaining: 2m 25s\n",
      "349:\tlearn: 0.3943140\ttotal: 1m 18s\tremaining: 2m 25s\n",
      "350:\tlearn: 0.3938023\ttotal: 1m 18s\tremaining: 2m 25s\n",
      "351:\tlearn: 0.3934851\ttotal: 1m 18s\tremaining: 2m 25s\n",
      "352:\tlearn: 0.3930677\ttotal: 1m 19s\tremaining: 2m 24s\n",
      "353:\tlearn: 0.3926446\ttotal: 1m 19s\tremaining: 2m 24s\n",
      "354:\tlearn: 0.3922839\ttotal: 1m 19s\tremaining: 2m 24s\n",
      "355:\tlearn: 0.3919147\ttotal: 1m 19s\tremaining: 2m 24s\n",
      "356:\tlearn: 0.3916561\ttotal: 1m 19s\tremaining: 2m 23s\n",
      "357:\tlearn: 0.3911930\ttotal: 1m 20s\tremaining: 2m 23s\n",
      "358:\tlearn: 0.3908745\ttotal: 1m 20s\tremaining: 2m 23s\n",
      "359:\tlearn: 0.3905650\ttotal: 1m 20s\tremaining: 2m 23s\n",
      "360:\tlearn: 0.3901828\ttotal: 1m 20s\tremaining: 2m 23s\n",
      "361:\tlearn: 0.3898315\ttotal: 1m 21s\tremaining: 2m 22s\n",
      "362:\tlearn: 0.3894432\ttotal: 1m 21s\tremaining: 2m 22s\n",
      "363:\tlearn: 0.3890238\ttotal: 1m 21s\tremaining: 2m 22s\n",
      "364:\tlearn: 0.3886305\ttotal: 1m 21s\tremaining: 2m 22s\n",
      "365:\tlearn: 0.3884457\ttotal: 1m 21s\tremaining: 2m 21s\n",
      "366:\tlearn: 0.3879976\ttotal: 1m 22s\tremaining: 2m 21s\n",
      "367:\tlearn: 0.3876460\ttotal: 1m 22s\tremaining: 2m 21s\n",
      "368:\tlearn: 0.3872426\ttotal: 1m 22s\tremaining: 2m 21s\n",
      "369:\tlearn: 0.3868547\ttotal: 1m 22s\tremaining: 2m 20s\n",
      "370:\tlearn: 0.3863688\ttotal: 1m 22s\tremaining: 2m 20s\n",
      "371:\tlearn: 0.3860150\ttotal: 1m 23s\tremaining: 2m 20s\n",
      "372:\tlearn: 0.3855422\ttotal: 1m 23s\tremaining: 2m 20s\n",
      "373:\tlearn: 0.3852239\ttotal: 1m 23s\tremaining: 2m 19s\n",
      "374:\tlearn: 0.3848025\ttotal: 1m 23s\tremaining: 2m 19s\n",
      "375:\tlearn: 0.3843223\ttotal: 1m 23s\tremaining: 2m 19s\n",
      "376:\tlearn: 0.3839779\ttotal: 1m 24s\tremaining: 2m 19s\n",
      "377:\tlearn: 0.3835037\ttotal: 1m 24s\tremaining: 2m 18s\n",
      "378:\tlearn: 0.3830466\ttotal: 1m 24s\tremaining: 2m 18s\n",
      "379:\tlearn: 0.3827466\ttotal: 1m 24s\tremaining: 2m 18s\n",
      "380:\tlearn: 0.3821209\ttotal: 1m 25s\tremaining: 2m 18s\n",
      "381:\tlearn: 0.3816955\ttotal: 1m 25s\tremaining: 2m 17s\n",
      "382:\tlearn: 0.3813992\ttotal: 1m 25s\tremaining: 2m 17s\n",
      "383:\tlearn: 0.3811074\ttotal: 1m 25s\tremaining: 2m 17s\n",
      "384:\tlearn: 0.3807541\ttotal: 1m 25s\tremaining: 2m 17s\n",
      "385:\tlearn: 0.3804703\ttotal: 1m 26s\tremaining: 2m 17s\n",
      "386:\tlearn: 0.3800669\ttotal: 1m 26s\tremaining: 2m 16s\n",
      "387:\tlearn: 0.3797686\ttotal: 1m 26s\tremaining: 2m 16s\n",
      "388:\tlearn: 0.3792832\ttotal: 1m 26s\tremaining: 2m 16s\n",
      "389:\tlearn: 0.3788386\ttotal: 1m 26s\tremaining: 2m 16s\n",
      "390:\tlearn: 0.3784693\ttotal: 1m 27s\tremaining: 2m 15s\n",
      "391:\tlearn: 0.3782030\ttotal: 1m 27s\tremaining: 2m 15s\n",
      "392:\tlearn: 0.3777758\ttotal: 1m 27s\tremaining: 2m 15s\n",
      "393:\tlearn: 0.3774638\ttotal: 1m 27s\tremaining: 2m 15s\n",
      "394:\tlearn: 0.3771352\ttotal: 1m 28s\tremaining: 2m 14s\n",
      "395:\tlearn: 0.3767952\ttotal: 1m 28s\tremaining: 2m 14s\n",
      "396:\tlearn: 0.3763825\ttotal: 1m 28s\tremaining: 2m 14s\n",
      "397:\tlearn: 0.3759144\ttotal: 1m 28s\tremaining: 2m 14s\n",
      "398:\tlearn: 0.3755205\ttotal: 1m 28s\tremaining: 2m 14s\n",
      "399:\tlearn: 0.3752633\ttotal: 1m 29s\tremaining: 2m 13s\n",
      "400:\tlearn: 0.3750155\ttotal: 1m 29s\tremaining: 2m 13s\n",
      "401:\tlearn: 0.3745303\ttotal: 1m 29s\tremaining: 2m 13s\n",
      "402:\tlearn: 0.3741249\ttotal: 1m 29s\tremaining: 2m 13s\n",
      "403:\tlearn: 0.3736724\ttotal: 1m 30s\tremaining: 2m 12s\n",
      "404:\tlearn: 0.3731564\ttotal: 1m 30s\tremaining: 2m 12s\n",
      "405:\tlearn: 0.3727375\ttotal: 1m 30s\tremaining: 2m 12s\n",
      "406:\tlearn: 0.3725362\ttotal: 1m 30s\tremaining: 2m 12s\n",
      "407:\tlearn: 0.3721711\ttotal: 1m 31s\tremaining: 2m 12s\n",
      "408:\tlearn: 0.3717800\ttotal: 1m 31s\tremaining: 2m 11s\n",
      "409:\tlearn: 0.3714606\ttotal: 1m 31s\tremaining: 2m 11s\n",
      "410:\tlearn: 0.3711240\ttotal: 1m 31s\tremaining: 2m 11s\n",
      "411:\tlearn: 0.3706905\ttotal: 1m 31s\tremaining: 2m 11s\n",
      "412:\tlearn: 0.3704957\ttotal: 1m 32s\tremaining: 2m 10s\n",
      "413:\tlearn: 0.3700323\ttotal: 1m 32s\tremaining: 2m 10s\n",
      "414:\tlearn: 0.3696462\ttotal: 1m 32s\tremaining: 2m 10s\n",
      "415:\tlearn: 0.3692861\ttotal: 1m 32s\tremaining: 2m 10s\n",
      "416:\tlearn: 0.3688395\ttotal: 1m 32s\tremaining: 2m 9s\n",
      "417:\tlearn: 0.3685891\ttotal: 1m 33s\tremaining: 2m 9s\n",
      "418:\tlearn: 0.3680820\ttotal: 1m 33s\tremaining: 2m 9s\n",
      "419:\tlearn: 0.3677976\ttotal: 1m 33s\tremaining: 2m 9s\n",
      "420:\tlearn: 0.3673239\ttotal: 1m 33s\tremaining: 2m 9s\n",
      "421:\tlearn: 0.3669129\ttotal: 1m 34s\tremaining: 2m 8s\n",
      "422:\tlearn: 0.3665874\ttotal: 1m 34s\tremaining: 2m 8s\n",
      "423:\tlearn: 0.3660903\ttotal: 1m 34s\tremaining: 2m 8s\n",
      "424:\tlearn: 0.3659246\ttotal: 1m 34s\tremaining: 2m 8s\n",
      "425:\tlearn: 0.3656445\ttotal: 1m 34s\tremaining: 2m 7s\n",
      "426:\tlearn: 0.3653353\ttotal: 1m 35s\tremaining: 2m 7s\n",
      "427:\tlearn: 0.3649095\ttotal: 1m 35s\tremaining: 2m 7s\n",
      "428:\tlearn: 0.3645742\ttotal: 1m 35s\tremaining: 2m 7s\n",
      "429:\tlearn: 0.3642143\ttotal: 1m 35s\tremaining: 2m 6s\n",
      "430:\tlearn: 0.3637552\ttotal: 1m 35s\tremaining: 2m 6s\n",
      "431:\tlearn: 0.3632873\ttotal: 1m 36s\tremaining: 2m 6s\n",
      "432:\tlearn: 0.3628957\ttotal: 1m 36s\tremaining: 2m 6s\n",
      "433:\tlearn: 0.3625629\ttotal: 1m 36s\tremaining: 2m 6s\n",
      "434:\tlearn: 0.3623375\ttotal: 1m 36s\tremaining: 2m 5s\n",
      "435:\tlearn: 0.3619746\ttotal: 1m 37s\tremaining: 2m 5s\n",
      "436:\tlearn: 0.3613609\ttotal: 1m 37s\tremaining: 2m 5s\n",
      "437:\tlearn: 0.3609471\ttotal: 1m 37s\tremaining: 2m 5s\n",
      "438:\tlearn: 0.3604433\ttotal: 1m 37s\tremaining: 2m 5s\n",
      "439:\tlearn: 0.3600146\ttotal: 1m 38s\tremaining: 2m 4s\n",
      "440:\tlearn: 0.3597442\ttotal: 1m 38s\tremaining: 2m 4s\n",
      "441:\tlearn: 0.3594215\ttotal: 1m 38s\tremaining: 2m 4s\n",
      "442:\tlearn: 0.3591089\ttotal: 1m 38s\tremaining: 2m 4s\n",
      "443:\tlearn: 0.3588103\ttotal: 1m 38s\tremaining: 2m 3s\n",
      "444:\tlearn: 0.3584633\ttotal: 1m 39s\tremaining: 2m 3s\n",
      "445:\tlearn: 0.3581983\ttotal: 1m 39s\tremaining: 2m 3s\n",
      "446:\tlearn: 0.3576815\ttotal: 1m 39s\tremaining: 2m 3s\n",
      "447:\tlearn: 0.3572837\ttotal: 1m 39s\tremaining: 2m 2s\n",
      "448:\tlearn: 0.3569560\ttotal: 1m 40s\tremaining: 2m 2s\n",
      "449:\tlearn: 0.3565692\ttotal: 1m 40s\tremaining: 2m 2s\n",
      "450:\tlearn: 0.3561687\ttotal: 1m 40s\tremaining: 2m 2s\n",
      "451:\tlearn: 0.3557491\ttotal: 1m 40s\tremaining: 2m 2s\n",
      "452:\tlearn: 0.3554375\ttotal: 1m 40s\tremaining: 2m 1s\n",
      "453:\tlearn: 0.3551905\ttotal: 1m 41s\tremaining: 2m 1s\n",
      "454:\tlearn: 0.3547616\ttotal: 1m 41s\tremaining: 2m 1s\n",
      "455:\tlearn: 0.3544221\ttotal: 1m 41s\tremaining: 2m 1s\n",
      "456:\tlearn: 0.3540467\ttotal: 1m 41s\tremaining: 2m\n",
      "457:\tlearn: 0.3536317\ttotal: 1m 41s\tremaining: 2m\n",
      "458:\tlearn: 0.3532669\ttotal: 1m 42s\tremaining: 2m\n",
      "459:\tlearn: 0.3528537\ttotal: 1m 42s\tremaining: 2m\n",
      "460:\tlearn: 0.3522986\ttotal: 1m 42s\tremaining: 1m 59s\n",
      "461:\tlearn: 0.3517756\ttotal: 1m 42s\tremaining: 1m 59s\n",
      "462:\tlearn: 0.3514838\ttotal: 1m 43s\tremaining: 1m 59s\n",
      "463:\tlearn: 0.3511072\ttotal: 1m 43s\tremaining: 1m 59s\n",
      "464:\tlearn: 0.3508147\ttotal: 1m 43s\tremaining: 1m 59s\n",
      "465:\tlearn: 0.3505429\ttotal: 1m 43s\tremaining: 1m 58s\n",
      "466:\tlearn: 0.3501587\ttotal: 1m 43s\tremaining: 1m 58s\n",
      "467:\tlearn: 0.3498344\ttotal: 1m 44s\tremaining: 1m 58s\n",
      "468:\tlearn: 0.3493434\ttotal: 1m 44s\tremaining: 1m 58s\n",
      "469:\tlearn: 0.3490673\ttotal: 1m 44s\tremaining: 1m 57s\n",
      "470:\tlearn: 0.3488236\ttotal: 1m 44s\tremaining: 1m 57s\n",
      "471:\tlearn: 0.3485515\ttotal: 1m 44s\tremaining: 1m 57s\n",
      "472:\tlearn: 0.3481558\ttotal: 1m 45s\tremaining: 1m 57s\n",
      "473:\tlearn: 0.3478989\ttotal: 1m 45s\tremaining: 1m 56s\n",
      "474:\tlearn: 0.3475618\ttotal: 1m 45s\tremaining: 1m 56s\n",
      "475:\tlearn: 0.3472107\ttotal: 1m 45s\tremaining: 1m 56s\n",
      "476:\tlearn: 0.3467901\ttotal: 1m 46s\tremaining: 1m 56s\n",
      "477:\tlearn: 0.3463443\ttotal: 1m 46s\tremaining: 1m 56s\n",
      "478:\tlearn: 0.3457451\ttotal: 1m 46s\tremaining: 1m 55s\n",
      "479:\tlearn: 0.3453572\ttotal: 1m 46s\tremaining: 1m 55s\n",
      "480:\tlearn: 0.3449472\ttotal: 1m 46s\tremaining: 1m 55s\n",
      "481:\tlearn: 0.3446268\ttotal: 1m 47s\tremaining: 1m 55s\n",
      "482:\tlearn: 0.3441906\ttotal: 1m 47s\tremaining: 1m 54s\n",
      "483:\tlearn: 0.3439228\ttotal: 1m 47s\tremaining: 1m 54s\n",
      "484:\tlearn: 0.3434888\ttotal: 1m 47s\tremaining: 1m 54s\n",
      "485:\tlearn: 0.3429933\ttotal: 1m 47s\tremaining: 1m 54s\n",
      "486:\tlearn: 0.3426748\ttotal: 1m 48s\tremaining: 1m 53s\n",
      "487:\tlearn: 0.3424320\ttotal: 1m 48s\tremaining: 1m 53s\n",
      "488:\tlearn: 0.3419896\ttotal: 1m 48s\tremaining: 1m 53s\n",
      "489:\tlearn: 0.3416903\ttotal: 1m 48s\tremaining: 1m 53s\n",
      "490:\tlearn: 0.3414911\ttotal: 1m 49s\tremaining: 1m 53s\n",
      "491:\tlearn: 0.3412279\ttotal: 1m 49s\tremaining: 1m 52s\n",
      "492:\tlearn: 0.3408710\ttotal: 1m 49s\tremaining: 1m 52s\n",
      "493:\tlearn: 0.3403495\ttotal: 1m 49s\tremaining: 1m 52s\n",
      "494:\tlearn: 0.3399141\ttotal: 1m 49s\tremaining: 1m 52s\n",
      "495:\tlearn: 0.3395028\ttotal: 1m 50s\tremaining: 1m 51s\n",
      "496:\tlearn: 0.3391672\ttotal: 1m 50s\tremaining: 1m 51s\n",
      "497:\tlearn: 0.3387545\ttotal: 1m 50s\tremaining: 1m 51s\n",
      "498:\tlearn: 0.3382689\ttotal: 1m 50s\tremaining: 1m 51s\n",
      "499:\tlearn: 0.3379005\ttotal: 1m 50s\tremaining: 1m 50s\n",
      "500:\tlearn: 0.3375797\ttotal: 1m 51s\tremaining: 1m 50s\n",
      "501:\tlearn: 0.3372043\ttotal: 1m 51s\tremaining: 1m 50s\n",
      "502:\tlearn: 0.3367632\ttotal: 1m 51s\tremaining: 1m 50s\n",
      "503:\tlearn: 0.3364656\ttotal: 1m 51s\tremaining: 1m 50s\n",
      "504:\tlearn: 0.3360283\ttotal: 1m 52s\tremaining: 1m 49s\n",
      "505:\tlearn: 0.3356823\ttotal: 1m 52s\tremaining: 1m 49s\n",
      "506:\tlearn: 0.3353582\ttotal: 1m 52s\tremaining: 1m 49s\n",
      "507:\tlearn: 0.3349658\ttotal: 1m 52s\tremaining: 1m 49s\n",
      "508:\tlearn: 0.3345244\ttotal: 1m 52s\tremaining: 1m 48s\n",
      "509:\tlearn: 0.3341563\ttotal: 1m 53s\tremaining: 1m 48s\n",
      "510:\tlearn: 0.3337128\ttotal: 1m 53s\tremaining: 1m 48s\n",
      "511:\tlearn: 0.3333652\ttotal: 1m 53s\tremaining: 1m 48s\n",
      "512:\tlearn: 0.3330904\ttotal: 1m 53s\tremaining: 1m 47s\n",
      "513:\tlearn: 0.3327405\ttotal: 1m 53s\tremaining: 1m 47s\n",
      "514:\tlearn: 0.3324194\ttotal: 1m 54s\tremaining: 1m 47s\n",
      "515:\tlearn: 0.3321856\ttotal: 1m 54s\tremaining: 1m 47s\n",
      "516:\tlearn: 0.3318705\ttotal: 1m 54s\tremaining: 1m 47s\n",
      "517:\tlearn: 0.3315874\ttotal: 1m 54s\tremaining: 1m 46s\n",
      "518:\tlearn: 0.3312269\ttotal: 1m 55s\tremaining: 1m 46s\n",
      "519:\tlearn: 0.3309436\ttotal: 1m 55s\tremaining: 1m 46s\n",
      "520:\tlearn: 0.3306518\ttotal: 1m 55s\tremaining: 1m 46s\n",
      "521:\tlearn: 0.3303314\ttotal: 1m 55s\tremaining: 1m 45s\n",
      "522:\tlearn: 0.3300440\ttotal: 1m 55s\tremaining: 1m 45s\n",
      "523:\tlearn: 0.3295763\ttotal: 1m 56s\tremaining: 1m 45s\n",
      "524:\tlearn: 0.3291997\ttotal: 1m 56s\tremaining: 1m 45s\n",
      "525:\tlearn: 0.3286972\ttotal: 1m 56s\tremaining: 1m 45s\n",
      "526:\tlearn: 0.3283645\ttotal: 1m 56s\tremaining: 1m 44s\n",
      "527:\tlearn: 0.3280568\ttotal: 1m 57s\tremaining: 1m 44s\n",
      "528:\tlearn: 0.3278139\ttotal: 1m 57s\tremaining: 1m 44s\n",
      "529:\tlearn: 0.3273767\ttotal: 1m 57s\tremaining: 1m 44s\n",
      "530:\tlearn: 0.3269153\ttotal: 1m 57s\tremaining: 1m 43s\n",
      "531:\tlearn: 0.3265286\ttotal: 1m 57s\tremaining: 1m 43s\n",
      "532:\tlearn: 0.3263262\ttotal: 1m 58s\tremaining: 1m 43s\n",
      "533:\tlearn: 0.3258220\ttotal: 1m 58s\tremaining: 1m 43s\n",
      "534:\tlearn: 0.3254735\ttotal: 1m 58s\tremaining: 1m 42s\n",
      "535:\tlearn: 0.3252078\ttotal: 1m 58s\tremaining: 1m 42s\n",
      "536:\tlearn: 0.3248738\ttotal: 1m 58s\tremaining: 1m 42s\n",
      "537:\tlearn: 0.3243885\ttotal: 1m 59s\tremaining: 1m 42s\n",
      "538:\tlearn: 0.3239733\ttotal: 1m 59s\tremaining: 1m 42s\n",
      "539:\tlearn: 0.3235740\ttotal: 1m 59s\tremaining: 1m 41s\n",
      "540:\tlearn: 0.3232060\ttotal: 1m 59s\tremaining: 1m 41s\n",
      "541:\tlearn: 0.3229513\ttotal: 2m\tremaining: 1m 41s\n",
      "542:\tlearn: 0.3225970\ttotal: 2m\tremaining: 1m 41s\n",
      "543:\tlearn: 0.3221574\ttotal: 2m\tremaining: 1m 40s\n",
      "544:\tlearn: 0.3218352\ttotal: 2m\tremaining: 1m 40s\n",
      "545:\tlearn: 0.3214469\ttotal: 2m\tremaining: 1m 40s\n",
      "546:\tlearn: 0.3210247\ttotal: 2m 1s\tremaining: 1m 40s\n",
      "547:\tlearn: 0.3207186\ttotal: 2m 1s\tremaining: 1m 40s\n",
      "548:\tlearn: 0.3204251\ttotal: 2m 1s\tremaining: 1m 39s\n",
      "549:\tlearn: 0.3202393\ttotal: 2m 1s\tremaining: 1m 39s\n",
      "550:\tlearn: 0.3200117\ttotal: 2m 2s\tremaining: 1m 39s\n",
      "551:\tlearn: 0.3195670\ttotal: 2m 2s\tremaining: 1m 39s\n",
      "552:\tlearn: 0.3192870\ttotal: 2m 2s\tremaining: 1m 38s\n",
      "553:\tlearn: 0.3188097\ttotal: 2m 2s\tremaining: 1m 38s\n",
      "554:\tlearn: 0.3184763\ttotal: 2m 2s\tremaining: 1m 38s\n",
      "555:\tlearn: 0.3181883\ttotal: 2m 3s\tremaining: 1m 38s\n",
      "556:\tlearn: 0.3177174\ttotal: 2m 3s\tremaining: 1m 38s\n",
      "557:\tlearn: 0.3173393\ttotal: 2m 3s\tremaining: 1m 37s\n",
      "558:\tlearn: 0.3169338\ttotal: 2m 3s\tremaining: 1m 37s\n",
      "559:\tlearn: 0.3165615\ttotal: 2m 3s\tremaining: 1m 37s\n",
      "560:\tlearn: 0.3160564\ttotal: 2m 4s\tremaining: 1m 37s\n",
      "561:\tlearn: 0.3157449\ttotal: 2m 4s\tremaining: 1m 36s\n",
      "562:\tlearn: 0.3154772\ttotal: 2m 4s\tremaining: 1m 36s\n",
      "563:\tlearn: 0.3152085\ttotal: 2m 4s\tremaining: 1m 36s\n",
      "564:\tlearn: 0.3146440\ttotal: 2m 5s\tremaining: 1m 36s\n",
      "565:\tlearn: 0.3143085\ttotal: 2m 5s\tremaining: 1m 36s\n",
      "566:\tlearn: 0.3140525\ttotal: 2m 5s\tremaining: 1m 35s\n",
      "567:\tlearn: 0.3135757\ttotal: 2m 5s\tremaining: 1m 35s\n",
      "568:\tlearn: 0.3131652\ttotal: 2m 5s\tremaining: 1m 35s\n",
      "569:\tlearn: 0.3126017\ttotal: 2m 6s\tremaining: 1m 35s\n",
      "570:\tlearn: 0.3121024\ttotal: 2m 6s\tremaining: 1m 34s\n",
      "571:\tlearn: 0.3118432\ttotal: 2m 6s\tremaining: 1m 34s\n",
      "572:\tlearn: 0.3115279\ttotal: 2m 6s\tremaining: 1m 34s\n",
      "573:\tlearn: 0.3112223\ttotal: 2m 7s\tremaining: 1m 34s\n",
      "574:\tlearn: 0.3108667\ttotal: 2m 7s\tremaining: 1m 34s\n",
      "575:\tlearn: 0.3103655\ttotal: 2m 7s\tremaining: 1m 33s\n",
      "576:\tlearn: 0.3099051\ttotal: 2m 7s\tremaining: 1m 33s\n",
      "577:\tlearn: 0.3095072\ttotal: 2m 7s\tremaining: 1m 33s\n",
      "578:\tlearn: 0.3090691\ttotal: 2m 8s\tremaining: 1m 33s\n",
      "579:\tlearn: 0.3085613\ttotal: 2m 8s\tremaining: 1m 32s\n",
      "580:\tlearn: 0.3082104\ttotal: 2m 8s\tremaining: 1m 32s\n",
      "581:\tlearn: 0.3078423\ttotal: 2m 8s\tremaining: 1m 32s\n",
      "582:\tlearn: 0.3075350\ttotal: 2m 9s\tremaining: 1m 32s\n",
      "583:\tlearn: 0.3070848\ttotal: 2m 9s\tremaining: 1m 32s\n",
      "584:\tlearn: 0.3066472\ttotal: 2m 9s\tremaining: 1m 31s\n",
      "585:\tlearn: 0.3062210\ttotal: 2m 9s\tremaining: 1m 31s\n",
      "586:\tlearn: 0.3057825\ttotal: 2m 9s\tremaining: 1m 31s\n",
      "587:\tlearn: 0.3054995\ttotal: 2m 10s\tremaining: 1m 31s\n",
      "588:\tlearn: 0.3051361\ttotal: 2m 10s\tremaining: 1m 30s\n",
      "589:\tlearn: 0.3047284\ttotal: 2m 10s\tremaining: 1m 30s\n",
      "590:\tlearn: 0.3043643\ttotal: 2m 10s\tremaining: 1m 30s\n",
      "591:\tlearn: 0.3039166\ttotal: 2m 10s\tremaining: 1m 30s\n",
      "592:\tlearn: 0.3035972\ttotal: 2m 11s\tremaining: 1m 30s\n",
      "593:\tlearn: 0.3030684\ttotal: 2m 11s\tremaining: 1m 29s\n",
      "594:\tlearn: 0.3027176\ttotal: 2m 11s\tremaining: 1m 29s\n",
      "595:\tlearn: 0.3024407\ttotal: 2m 11s\tremaining: 1m 29s\n",
      "596:\tlearn: 0.3020941\ttotal: 2m 12s\tremaining: 1m 29s\n",
      "597:\tlearn: 0.3016701\ttotal: 2m 12s\tremaining: 1m 28s\n",
      "598:\tlearn: 0.3012520\ttotal: 2m 12s\tremaining: 1m 28s\n",
      "599:\tlearn: 0.3009645\ttotal: 2m 12s\tremaining: 1m 28s\n",
      "600:\tlearn: 0.3006161\ttotal: 2m 12s\tremaining: 1m 28s\n",
      "601:\tlearn: 0.3003111\ttotal: 2m 13s\tremaining: 1m 28s\n",
      "602:\tlearn: 0.2999010\ttotal: 2m 13s\tremaining: 1m 27s\n",
      "603:\tlearn: 0.2994841\ttotal: 2m 13s\tremaining: 1m 27s\n",
      "604:\tlearn: 0.2992069\ttotal: 2m 13s\tremaining: 1m 27s\n",
      "605:\tlearn: 0.2987770\ttotal: 2m 13s\tremaining: 1m 27s\n",
      "606:\tlearn: 0.2984150\ttotal: 2m 14s\tremaining: 1m 26s\n",
      "607:\tlearn: 0.2978909\ttotal: 2m 14s\tremaining: 1m 26s\n",
      "608:\tlearn: 0.2974433\ttotal: 2m 14s\tremaining: 1m 26s\n",
      "609:\tlearn: 0.2970129\ttotal: 2m 14s\tremaining: 1m 26s\n",
      "610:\tlearn: 0.2965555\ttotal: 2m 15s\tremaining: 1m 25s\n",
      "611:\tlearn: 0.2963043\ttotal: 2m 15s\tremaining: 1m 25s\n",
      "612:\tlearn: 0.2959377\ttotal: 2m 15s\tremaining: 1m 25s\n",
      "613:\tlearn: 0.2955952\ttotal: 2m 15s\tremaining: 1m 25s\n",
      "614:\tlearn: 0.2951896\ttotal: 2m 15s\tremaining: 1m 25s\n",
      "615:\tlearn: 0.2947880\ttotal: 2m 16s\tremaining: 1m 24s\n",
      "616:\tlearn: 0.2945368\ttotal: 2m 16s\tremaining: 1m 24s\n",
      "617:\tlearn: 0.2941553\ttotal: 2m 16s\tremaining: 1m 24s\n",
      "618:\tlearn: 0.2936441\ttotal: 2m 16s\tremaining: 1m 24s\n",
      "619:\tlearn: 0.2932333\ttotal: 2m 17s\tremaining: 1m 23s\n",
      "620:\tlearn: 0.2928555\ttotal: 2m 17s\tremaining: 1m 23s\n",
      "621:\tlearn: 0.2923113\ttotal: 2m 17s\tremaining: 1m 23s\n",
      "622:\tlearn: 0.2920016\ttotal: 2m 17s\tremaining: 1m 23s\n",
      "623:\tlearn: 0.2915148\ttotal: 2m 17s\tremaining: 1m 23s\n",
      "624:\tlearn: 0.2911172\ttotal: 2m 18s\tremaining: 1m 22s\n",
      "625:\tlearn: 0.2907579\ttotal: 2m 18s\tremaining: 1m 22s\n",
      "626:\tlearn: 0.2904479\ttotal: 2m 18s\tremaining: 1m 22s\n",
      "627:\tlearn: 0.2901179\ttotal: 2m 18s\tremaining: 1m 22s\n",
      "628:\tlearn: 0.2897831\ttotal: 2m 18s\tremaining: 1m 21s\n",
      "629:\tlearn: 0.2892537\ttotal: 2m 19s\tremaining: 1m 21s\n",
      "630:\tlearn: 0.2886764\ttotal: 2m 19s\tremaining: 1m 21s\n",
      "631:\tlearn: 0.2882516\ttotal: 2m 19s\tremaining: 1m 21s\n",
      "632:\tlearn: 0.2878927\ttotal: 2m 19s\tremaining: 1m 21s\n",
      "633:\tlearn: 0.2875456\ttotal: 2m 19s\tremaining: 1m 20s\n",
      "634:\tlearn: 0.2870521\ttotal: 2m 20s\tremaining: 1m 20s\n",
      "635:\tlearn: 0.2866328\ttotal: 2m 20s\tremaining: 1m 20s\n",
      "636:\tlearn: 0.2861656\ttotal: 2m 20s\tremaining: 1m 20s\n",
      "637:\tlearn: 0.2858619\ttotal: 2m 20s\tremaining: 1m 19s\n",
      "638:\tlearn: 0.2854802\ttotal: 2m 21s\tremaining: 1m 19s\n",
      "639:\tlearn: 0.2849200\ttotal: 2m 21s\tremaining: 1m 19s\n",
      "640:\tlearn: 0.2844786\ttotal: 2m 21s\tremaining: 1m 19s\n",
      "641:\tlearn: 0.2841280\ttotal: 2m 21s\tremaining: 1m 19s\n",
      "642:\tlearn: 0.2836781\ttotal: 2m 21s\tremaining: 1m 18s\n",
      "643:\tlearn: 0.2832262\ttotal: 2m 22s\tremaining: 1m 18s\n",
      "644:\tlearn: 0.2829061\ttotal: 2m 22s\tremaining: 1m 18s\n",
      "645:\tlearn: 0.2824357\ttotal: 2m 22s\tremaining: 1m 18s\n",
      "646:\tlearn: 0.2821108\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "647:\tlearn: 0.2817597\ttotal: 2m 22s\tremaining: 1m 17s\n",
      "648:\tlearn: 0.2813171\ttotal: 2m 23s\tremaining: 1m 17s\n",
      "649:\tlearn: 0.2810201\ttotal: 2m 23s\tremaining: 1m 17s\n",
      "650:\tlearn: 0.2806316\ttotal: 2m 23s\tremaining: 1m 17s\n",
      "651:\tlearn: 0.2802625\ttotal: 2m 23s\tremaining: 1m 16s\n",
      "652:\tlearn: 0.2796796\ttotal: 2m 24s\tremaining: 1m 16s\n",
      "653:\tlearn: 0.2793549\ttotal: 2m 24s\tremaining: 1m 16s\n",
      "654:\tlearn: 0.2789207\ttotal: 2m 24s\tremaining: 1m 16s\n",
      "655:\tlearn: 0.2784654\ttotal: 2m 24s\tremaining: 1m 15s\n",
      "656:\tlearn: 0.2779402\ttotal: 2m 24s\tremaining: 1m 15s\n",
      "657:\tlearn: 0.2775858\ttotal: 2m 25s\tremaining: 1m 15s\n",
      "658:\tlearn: 0.2770720\ttotal: 2m 25s\tremaining: 1m 15s\n",
      "659:\tlearn: 0.2766938\ttotal: 2m 25s\tremaining: 1m 14s\n",
      "660:\tlearn: 0.2762935\ttotal: 2m 25s\tremaining: 1m 14s\n",
      "661:\tlearn: 0.2758228\ttotal: 2m 26s\tremaining: 1m 14s\n",
      "662:\tlearn: 0.2755451\ttotal: 2m 26s\tremaining: 1m 14s\n",
      "663:\tlearn: 0.2750117\ttotal: 2m 26s\tremaining: 1m 14s\n",
      "664:\tlearn: 0.2744616\ttotal: 2m 26s\tremaining: 1m 13s\n",
      "665:\tlearn: 0.2741371\ttotal: 2m 26s\tremaining: 1m 13s\n",
      "666:\tlearn: 0.2736573\ttotal: 2m 27s\tremaining: 1m 13s\n",
      "667:\tlearn: 0.2733703\ttotal: 2m 27s\tremaining: 1m 13s\n",
      "668:\tlearn: 0.2730576\ttotal: 2m 27s\tremaining: 1m 12s\n",
      "669:\tlearn: 0.2728078\ttotal: 2m 27s\tremaining: 1m 12s\n",
      "670:\tlearn: 0.2723783\ttotal: 2m 27s\tremaining: 1m 12s\n",
      "671:\tlearn: 0.2718805\ttotal: 2m 28s\tremaining: 1m 12s\n",
      "672:\tlearn: 0.2714527\ttotal: 2m 28s\tremaining: 1m 12s\n",
      "673:\tlearn: 0.2711255\ttotal: 2m 28s\tremaining: 1m 11s\n",
      "674:\tlearn: 0.2705506\ttotal: 2m 28s\tremaining: 1m 11s\n",
      "675:\tlearn: 0.2701745\ttotal: 2m 29s\tremaining: 1m 11s\n",
      "676:\tlearn: 0.2698676\ttotal: 2m 29s\tremaining: 1m 11s\n",
      "677:\tlearn: 0.2694729\ttotal: 2m 29s\tremaining: 1m 10s\n",
      "678:\tlearn: 0.2690930\ttotal: 2m 29s\tremaining: 1m 10s\n",
      "679:\tlearn: 0.2686181\ttotal: 2m 29s\tremaining: 1m 10s\n",
      "680:\tlearn: 0.2682729\ttotal: 2m 30s\tremaining: 1m 10s\n",
      "681:\tlearn: 0.2678815\ttotal: 2m 30s\tremaining: 1m 10s\n",
      "682:\tlearn: 0.2675149\ttotal: 2m 30s\tremaining: 1m 9s\n",
      "683:\tlearn: 0.2672306\ttotal: 2m 30s\tremaining: 1m 9s\n",
      "684:\tlearn: 0.2668212\ttotal: 2m 30s\tremaining: 1m 9s\n",
      "685:\tlearn: 0.2664753\ttotal: 2m 31s\tremaining: 1m 9s\n",
      "686:\tlearn: 0.2661658\ttotal: 2m 31s\tremaining: 1m 8s\n",
      "687:\tlearn: 0.2658232\ttotal: 2m 31s\tremaining: 1m 8s\n",
      "688:\tlearn: 0.2654461\ttotal: 2m 31s\tremaining: 1m 8s\n",
      "689:\tlearn: 0.2650262\ttotal: 2m 32s\tremaining: 1m 8s\n",
      "690:\tlearn: 0.2645894\ttotal: 2m 32s\tremaining: 1m 8s\n",
      "691:\tlearn: 0.2641822\ttotal: 2m 32s\tremaining: 1m 7s\n",
      "692:\tlearn: 0.2637819\ttotal: 2m 32s\tremaining: 1m 7s\n",
      "693:\tlearn: 0.2634656\ttotal: 2m 32s\tremaining: 1m 7s\n",
      "694:\tlearn: 0.2630984\ttotal: 2m 33s\tremaining: 1m 7s\n",
      "695:\tlearn: 0.2627609\ttotal: 2m 33s\tremaining: 1m 6s\n",
      "696:\tlearn: 0.2623607\ttotal: 2m 33s\tremaining: 1m 6s\n",
      "697:\tlearn: 0.2619254\ttotal: 2m 33s\tremaining: 1m 6s\n",
      "698:\tlearn: 0.2614910\ttotal: 2m 33s\tremaining: 1m 6s\n",
      "699:\tlearn: 0.2610653\ttotal: 2m 34s\tremaining: 1m 6s\n",
      "700:\tlearn: 0.2605773\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "701:\tlearn: 0.2601370\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "702:\tlearn: 0.2598245\ttotal: 2m 34s\tremaining: 1m 5s\n",
      "703:\tlearn: 0.2594054\ttotal: 2m 35s\tremaining: 1m 5s\n",
      "704:\tlearn: 0.2590395\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "705:\tlearn: 0.2586748\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "706:\tlearn: 0.2583336\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "707:\tlearn: 0.2580144\ttotal: 2m 35s\tremaining: 1m 4s\n",
      "708:\tlearn: 0.2576050\ttotal: 2m 36s\tremaining: 1m 4s\n",
      "709:\tlearn: 0.2572422\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "710:\tlearn: 0.2568469\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "711:\tlearn: 0.2565320\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "712:\tlearn: 0.2560864\ttotal: 2m 36s\tremaining: 1m 3s\n",
      "713:\tlearn: 0.2557071\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "714:\tlearn: 0.2552346\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "715:\tlearn: 0.2548687\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "716:\tlearn: 0.2545431\ttotal: 2m 37s\tremaining: 1m 2s\n",
      "717:\tlearn: 0.2541771\ttotal: 2m 38s\tremaining: 1m 2s\n",
      "718:\tlearn: 0.2538344\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "719:\tlearn: 0.2535374\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "720:\tlearn: 0.2532365\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "721:\tlearn: 0.2528600\ttotal: 2m 38s\tremaining: 1m 1s\n",
      "722:\tlearn: 0.2524731\ttotal: 2m 39s\tremaining: 1m\n",
      "723:\tlearn: 0.2520861\ttotal: 2m 39s\tremaining: 1m\n",
      "724:\tlearn: 0.2516238\ttotal: 2m 39s\tremaining: 1m\n",
      "725:\tlearn: 0.2512947\ttotal: 2m 39s\tremaining: 1m\n",
      "726:\tlearn: 0.2509511\ttotal: 2m 39s\tremaining: 1m\n",
      "727:\tlearn: 0.2506359\ttotal: 2m 40s\tremaining: 59.9s\n",
      "728:\tlearn: 0.2502589\ttotal: 2m 40s\tremaining: 59.6s\n",
      "729:\tlearn: 0.2499417\ttotal: 2m 40s\tremaining: 59.4s\n",
      "730:\tlearn: 0.2495187\ttotal: 2m 40s\tremaining: 59.2s\n",
      "731:\tlearn: 0.2491651\ttotal: 2m 41s\tremaining: 59s\n",
      "732:\tlearn: 0.2487601\ttotal: 2m 41s\tremaining: 58.7s\n",
      "733:\tlearn: 0.2484116\ttotal: 2m 41s\tremaining: 58.5s\n",
      "734:\tlearn: 0.2479989\ttotal: 2m 41s\tremaining: 58.3s\n",
      "735:\tlearn: 0.2476107\ttotal: 2m 41s\tremaining: 58.1s\n",
      "736:\tlearn: 0.2473096\ttotal: 2m 42s\tremaining: 57.9s\n",
      "737:\tlearn: 0.2470393\ttotal: 2m 42s\tremaining: 57.6s\n",
      "738:\tlearn: 0.2467920\ttotal: 2m 42s\tremaining: 57.4s\n",
      "739:\tlearn: 0.2464014\ttotal: 2m 42s\tremaining: 57.2s\n",
      "740:\tlearn: 0.2459235\ttotal: 2m 42s\tremaining: 57s\n",
      "741:\tlearn: 0.2455813\ttotal: 2m 43s\tremaining: 56.7s\n",
      "742:\tlearn: 0.2452421\ttotal: 2m 43s\tremaining: 56.5s\n",
      "743:\tlearn: 0.2449620\ttotal: 2m 43s\tremaining: 56.3s\n",
      "744:\tlearn: 0.2447094\ttotal: 2m 43s\tremaining: 56.1s\n",
      "745:\tlearn: 0.2443447\ttotal: 2m 44s\tremaining: 55.9s\n",
      "746:\tlearn: 0.2439799\ttotal: 2m 44s\tremaining: 55.6s\n",
      "747:\tlearn: 0.2436727\ttotal: 2m 44s\tremaining: 55.4s\n",
      "748:\tlearn: 0.2434735\ttotal: 2m 44s\tremaining: 55.2s\n",
      "749:\tlearn: 0.2430053\ttotal: 2m 44s\tremaining: 55s\n",
      "750:\tlearn: 0.2427675\ttotal: 2m 45s\tremaining: 54.7s\n",
      "751:\tlearn: 0.2424160\ttotal: 2m 45s\tremaining: 54.5s\n",
      "752:\tlearn: 0.2421122\ttotal: 2m 45s\tremaining: 54.3s\n",
      "753:\tlearn: 0.2417278\ttotal: 2m 45s\tremaining: 54.1s\n",
      "754:\tlearn: 0.2413825\ttotal: 2m 45s\tremaining: 53.9s\n",
      "755:\tlearn: 0.2410796\ttotal: 2m 46s\tremaining: 53.6s\n",
      "756:\tlearn: 0.2407235\ttotal: 2m 46s\tremaining: 53.4s\n",
      "757:\tlearn: 0.2403349\ttotal: 2m 46s\tremaining: 53.2s\n",
      "758:\tlearn: 0.2399434\ttotal: 2m 46s\tremaining: 53s\n",
      "759:\tlearn: 0.2395747\ttotal: 2m 47s\tremaining: 52.7s\n",
      "760:\tlearn: 0.2391687\ttotal: 2m 47s\tremaining: 52.5s\n",
      "761:\tlearn: 0.2387383\ttotal: 2m 47s\tremaining: 52.3s\n",
      "762:\tlearn: 0.2383316\ttotal: 2m 47s\tremaining: 52.1s\n",
      "763:\tlearn: 0.2380050\ttotal: 2m 47s\tremaining: 51.9s\n",
      "764:\tlearn: 0.2375755\ttotal: 2m 48s\tremaining: 51.6s\n",
      "765:\tlearn: 0.2372809\ttotal: 2m 48s\tremaining: 51.4s\n",
      "766:\tlearn: 0.2369837\ttotal: 2m 48s\tremaining: 51.2s\n",
      "767:\tlearn: 0.2365494\ttotal: 2m 48s\tremaining: 51s\n",
      "768:\tlearn: 0.2361514\ttotal: 2m 48s\tremaining: 50.8s\n",
      "769:\tlearn: 0.2358067\ttotal: 2m 49s\tremaining: 50.5s\n",
      "770:\tlearn: 0.2355260\ttotal: 2m 49s\tremaining: 50.3s\n",
      "771:\tlearn: 0.2351851\ttotal: 2m 49s\tremaining: 50.1s\n",
      "772:\tlearn: 0.2348019\ttotal: 2m 49s\tremaining: 49.9s\n",
      "773:\tlearn: 0.2344298\ttotal: 2m 50s\tremaining: 49.6s\n",
      "774:\tlearn: 0.2340494\ttotal: 2m 50s\tremaining: 49.4s\n",
      "775:\tlearn: 0.2336145\ttotal: 2m 50s\tremaining: 49.2s\n",
      "776:\tlearn: 0.2331642\ttotal: 2m 50s\tremaining: 49s\n",
      "777:\tlearn: 0.2328618\ttotal: 2m 50s\tremaining: 48.8s\n",
      "778:\tlearn: 0.2324714\ttotal: 2m 51s\tremaining: 48.5s\n",
      "779:\tlearn: 0.2321789\ttotal: 2m 51s\tremaining: 48.3s\n",
      "780:\tlearn: 0.2318498\ttotal: 2m 51s\tremaining: 48.1s\n",
      "781:\tlearn: 0.2315376\ttotal: 2m 51s\tremaining: 47.9s\n",
      "782:\tlearn: 0.2311949\ttotal: 2m 51s\tremaining: 47.7s\n",
      "783:\tlearn: 0.2307697\ttotal: 2m 52s\tremaining: 47.4s\n",
      "784:\tlearn: 0.2303768\ttotal: 2m 52s\tremaining: 47.2s\n",
      "785:\tlearn: 0.2299811\ttotal: 2m 52s\tremaining: 47s\n",
      "786:\tlearn: 0.2295418\ttotal: 2m 52s\tremaining: 46.8s\n",
      "787:\tlearn: 0.2291343\ttotal: 2m 53s\tremaining: 46.5s\n",
      "788:\tlearn: 0.2288677\ttotal: 2m 53s\tremaining: 46.3s\n",
      "789:\tlearn: 0.2286032\ttotal: 2m 53s\tremaining: 46.1s\n",
      "790:\tlearn: 0.2282542\ttotal: 2m 53s\tremaining: 45.9s\n",
      "791:\tlearn: 0.2279161\ttotal: 2m 53s\tremaining: 45.7s\n",
      "792:\tlearn: 0.2276090\ttotal: 2m 54s\tremaining: 45.4s\n",
      "793:\tlearn: 0.2271984\ttotal: 2m 54s\tremaining: 45.2s\n",
      "794:\tlearn: 0.2268666\ttotal: 2m 54s\tremaining: 45s\n",
      "795:\tlearn: 0.2265580\ttotal: 2m 54s\tremaining: 44.8s\n",
      "796:\tlearn: 0.2261734\ttotal: 2m 54s\tremaining: 44.6s\n",
      "797:\tlearn: 0.2258322\ttotal: 2m 55s\tremaining: 44.3s\n",
      "798:\tlearn: 0.2255352\ttotal: 2m 55s\tremaining: 44.1s\n",
      "799:\tlearn: 0.2252296\ttotal: 2m 55s\tremaining: 43.9s\n",
      "800:\tlearn: 0.2249087\ttotal: 2m 55s\tremaining: 43.7s\n",
      "801:\tlearn: 0.2247489\ttotal: 2m 56s\tremaining: 43.5s\n",
      "802:\tlearn: 0.2244630\ttotal: 2m 56s\tremaining: 43.2s\n",
      "803:\tlearn: 0.2241650\ttotal: 2m 56s\tremaining: 43s\n",
      "804:\tlearn: 0.2237880\ttotal: 2m 56s\tremaining: 42.8s\n",
      "805:\tlearn: 0.2234480\ttotal: 2m 56s\tremaining: 42.6s\n",
      "806:\tlearn: 0.2231160\ttotal: 2m 57s\tremaining: 42.4s\n",
      "807:\tlearn: 0.2227840\ttotal: 2m 57s\tremaining: 42.1s\n",
      "808:\tlearn: 0.2225019\ttotal: 2m 57s\tremaining: 41.9s\n",
      "809:\tlearn: 0.2222161\ttotal: 2m 57s\tremaining: 41.7s\n",
      "810:\tlearn: 0.2219167\ttotal: 2m 57s\tremaining: 41.5s\n",
      "811:\tlearn: 0.2216569\ttotal: 2m 58s\tremaining: 41.3s\n",
      "812:\tlearn: 0.2214256\ttotal: 2m 58s\tremaining: 41s\n",
      "813:\tlearn: 0.2211054\ttotal: 2m 58s\tremaining: 40.8s\n",
      "814:\tlearn: 0.2207963\ttotal: 2m 58s\tremaining: 40.6s\n",
      "815:\tlearn: 0.2205516\ttotal: 2m 59s\tremaining: 40.4s\n",
      "816:\tlearn: 0.2201489\ttotal: 2m 59s\tremaining: 40.1s\n",
      "817:\tlearn: 0.2198767\ttotal: 2m 59s\tremaining: 39.9s\n",
      "818:\tlearn: 0.2195710\ttotal: 2m 59s\tremaining: 39.7s\n",
      "819:\tlearn: 0.2192450\ttotal: 2m 59s\tremaining: 39.5s\n",
      "820:\tlearn: 0.2189557\ttotal: 3m\tremaining: 39.3s\n",
      "821:\tlearn: 0.2185473\ttotal: 3m\tremaining: 39.1s\n",
      "822:\tlearn: 0.2183259\ttotal: 3m\tremaining: 38.8s\n",
      "823:\tlearn: 0.2180207\ttotal: 3m\tremaining: 38.6s\n",
      "824:\tlearn: 0.2178011\ttotal: 3m\tremaining: 38.4s\n",
      "825:\tlearn: 0.2175371\ttotal: 3m 1s\tremaining: 38.2s\n",
      "826:\tlearn: 0.2172561\ttotal: 3m 1s\tremaining: 37.9s\n",
      "827:\tlearn: 0.2168727\ttotal: 3m 1s\tremaining: 37.7s\n",
      "828:\tlearn: 0.2165649\ttotal: 3m 1s\tremaining: 37.5s\n",
      "829:\tlearn: 0.2162257\ttotal: 3m 2s\tremaining: 37.3s\n",
      "830:\tlearn: 0.2158464\ttotal: 3m 2s\tremaining: 37.1s\n",
      "831:\tlearn: 0.2155548\ttotal: 3m 2s\tremaining: 36.8s\n",
      "832:\tlearn: 0.2152527\ttotal: 3m 2s\tremaining: 36.6s\n",
      "833:\tlearn: 0.2149393\ttotal: 3m 2s\tremaining: 36.4s\n",
      "834:\tlearn: 0.2146675\ttotal: 3m 3s\tremaining: 36.2s\n",
      "835:\tlearn: 0.2143313\ttotal: 3m 3s\tremaining: 36s\n",
      "836:\tlearn: 0.2141143\ttotal: 3m 3s\tremaining: 35.7s\n",
      "837:\tlearn: 0.2137684\ttotal: 3m 3s\tremaining: 35.5s\n",
      "838:\tlearn: 0.2134247\ttotal: 3m 3s\tremaining: 35.3s\n",
      "839:\tlearn: 0.2132091\ttotal: 3m 4s\tremaining: 35.1s\n",
      "840:\tlearn: 0.2128690\ttotal: 3m 4s\tremaining: 34.9s\n",
      "841:\tlearn: 0.2125612\ttotal: 3m 4s\tremaining: 34.6s\n",
      "842:\tlearn: 0.2122706\ttotal: 3m 4s\tremaining: 34.4s\n",
      "843:\tlearn: 0.2119137\ttotal: 3m 5s\tremaining: 34.2s\n",
      "844:\tlearn: 0.2116742\ttotal: 3m 5s\tremaining: 34s\n",
      "845:\tlearn: 0.2112895\ttotal: 3m 5s\tremaining: 33.8s\n",
      "846:\tlearn: 0.2110652\ttotal: 3m 5s\tremaining: 33.5s\n",
      "847:\tlearn: 0.2108159\ttotal: 3m 5s\tremaining: 33.3s\n",
      "848:\tlearn: 0.2105798\ttotal: 3m 6s\tremaining: 33.1s\n",
      "849:\tlearn: 0.2103518\ttotal: 3m 6s\tremaining: 32.9s\n",
      "850:\tlearn: 0.2099768\ttotal: 3m 6s\tremaining: 32.7s\n",
      "851:\tlearn: 0.2096669\ttotal: 3m 6s\tremaining: 32.5s\n",
      "852:\tlearn: 0.2093719\ttotal: 3m 7s\tremaining: 32.2s\n",
      "853:\tlearn: 0.2091258\ttotal: 3m 7s\tremaining: 32s\n",
      "854:\tlearn: 0.2087998\ttotal: 3m 7s\tremaining: 31.8s\n",
      "855:\tlearn: 0.2085204\ttotal: 3m 7s\tremaining: 31.6s\n",
      "856:\tlearn: 0.2082317\ttotal: 3m 7s\tremaining: 31.4s\n",
      "857:\tlearn: 0.2079052\ttotal: 3m 8s\tremaining: 31.1s\n",
      "858:\tlearn: 0.2075281\ttotal: 3m 8s\tremaining: 30.9s\n",
      "859:\tlearn: 0.2072384\ttotal: 3m 8s\tremaining: 30.7s\n",
      "860:\tlearn: 0.2069496\ttotal: 3m 8s\tremaining: 30.5s\n",
      "861:\tlearn: 0.2066180\ttotal: 3m 9s\tremaining: 30.3s\n",
      "862:\tlearn: 0.2062425\ttotal: 3m 9s\tremaining: 30s\n",
      "863:\tlearn: 0.2060139\ttotal: 3m 9s\tremaining: 29.8s\n",
      "864:\tlearn: 0.2057405\ttotal: 3m 9s\tremaining: 29.6s\n",
      "865:\tlearn: 0.2054907\ttotal: 3m 9s\tremaining: 29.4s\n",
      "866:\tlearn: 0.2051112\ttotal: 3m 10s\tremaining: 29.2s\n",
      "867:\tlearn: 0.2047776\ttotal: 3m 10s\tremaining: 28.9s\n",
      "868:\tlearn: 0.2045570\ttotal: 3m 10s\tremaining: 28.7s\n",
      "869:\tlearn: 0.2042649\ttotal: 3m 10s\tremaining: 28.5s\n",
      "870:\tlearn: 0.2040192\ttotal: 3m 10s\tremaining: 28.3s\n",
      "871:\tlearn: 0.2037633\ttotal: 3m 11s\tremaining: 28.1s\n",
      "872:\tlearn: 0.2035622\ttotal: 3m 11s\tremaining: 27.8s\n",
      "873:\tlearn: 0.2031601\ttotal: 3m 11s\tremaining: 27.6s\n",
      "874:\tlearn: 0.2029134\ttotal: 3m 11s\tremaining: 27.4s\n",
      "875:\tlearn: 0.2026409\ttotal: 3m 12s\tremaining: 27.2s\n",
      "876:\tlearn: 0.2022615\ttotal: 3m 12s\tremaining: 27s\n",
      "877:\tlearn: 0.2020303\ttotal: 3m 12s\tremaining: 26.7s\n",
      "878:\tlearn: 0.2016900\ttotal: 3m 12s\tremaining: 26.5s\n",
      "879:\tlearn: 0.2013704\ttotal: 3m 12s\tremaining: 26.3s\n",
      "880:\tlearn: 0.2010802\ttotal: 3m 13s\tremaining: 26.1s\n",
      "881:\tlearn: 0.2007901\ttotal: 3m 13s\tremaining: 25.9s\n",
      "882:\tlearn: 0.2005402\ttotal: 3m 13s\tremaining: 25.6s\n",
      "883:\tlearn: 0.2002590\ttotal: 3m 13s\tremaining: 25.4s\n",
      "884:\tlearn: 0.1998883\ttotal: 3m 13s\tremaining: 25.2s\n",
      "885:\tlearn: 0.1996151\ttotal: 3m 14s\tremaining: 25s\n",
      "886:\tlearn: 0.1993173\ttotal: 3m 14s\tremaining: 24.8s\n",
      "887:\tlearn: 0.1990461\ttotal: 3m 14s\tremaining: 24.5s\n",
      "888:\tlearn: 0.1987859\ttotal: 3m 14s\tremaining: 24.3s\n",
      "889:\tlearn: 0.1984717\ttotal: 3m 15s\tremaining: 24.1s\n",
      "890:\tlearn: 0.1982031\ttotal: 3m 15s\tremaining: 23.9s\n",
      "891:\tlearn: 0.1979111\ttotal: 3m 15s\tremaining: 23.7s\n",
      "892:\tlearn: 0.1976673\ttotal: 3m 15s\tremaining: 23.4s\n",
      "893:\tlearn: 0.1973393\ttotal: 3m 15s\tremaining: 23.2s\n",
      "894:\tlearn: 0.1969620\ttotal: 3m 16s\tremaining: 23s\n",
      "895:\tlearn: 0.1966212\ttotal: 3m 16s\tremaining: 22.8s\n",
      "896:\tlearn: 0.1963665\ttotal: 3m 16s\tremaining: 22.6s\n",
      "897:\tlearn: 0.1961304\ttotal: 3m 16s\tremaining: 22.4s\n",
      "898:\tlearn: 0.1957967\ttotal: 3m 16s\tremaining: 22.1s\n",
      "899:\tlearn: 0.1954983\ttotal: 3m 17s\tremaining: 21.9s\n",
      "900:\tlearn: 0.1952676\ttotal: 3m 17s\tremaining: 21.7s\n",
      "901:\tlearn: 0.1950078\ttotal: 3m 17s\tremaining: 21.5s\n",
      "902:\tlearn: 0.1946444\ttotal: 3m 17s\tremaining: 21.3s\n",
      "903:\tlearn: 0.1943510\ttotal: 3m 18s\tremaining: 21s\n",
      "904:\tlearn: 0.1940856\ttotal: 3m 18s\tremaining: 20.8s\n",
      "905:\tlearn: 0.1937994\ttotal: 3m 18s\tremaining: 20.6s\n",
      "906:\tlearn: 0.1934718\ttotal: 3m 18s\tremaining: 20.4s\n",
      "907:\tlearn: 0.1931120\ttotal: 3m 18s\tremaining: 20.2s\n",
      "908:\tlearn: 0.1928497\ttotal: 3m 19s\tremaining: 19.9s\n",
      "909:\tlearn: 0.1926445\ttotal: 3m 19s\tremaining: 19.7s\n",
      "910:\tlearn: 0.1923725\ttotal: 3m 19s\tremaining: 19.5s\n",
      "911:\tlearn: 0.1921704\ttotal: 3m 19s\tremaining: 19.3s\n",
      "912:\tlearn: 0.1918949\ttotal: 3m 20s\tremaining: 19.1s\n",
      "913:\tlearn: 0.1915696\ttotal: 3m 20s\tremaining: 18.8s\n",
      "914:\tlearn: 0.1913442\ttotal: 3m 20s\tremaining: 18.6s\n",
      "915:\tlearn: 0.1910509\ttotal: 3m 20s\tremaining: 18.4s\n",
      "916:\tlearn: 0.1908363\ttotal: 3m 20s\tremaining: 18.2s\n",
      "917:\tlearn: 0.1905347\ttotal: 3m 21s\tremaining: 18s\n",
      "918:\tlearn: 0.1903345\ttotal: 3m 21s\tremaining: 17.7s\n",
      "919:\tlearn: 0.1900307\ttotal: 3m 21s\tremaining: 17.5s\n",
      "920:\tlearn: 0.1896878\ttotal: 3m 21s\tremaining: 17.3s\n",
      "921:\tlearn: 0.1894639\ttotal: 3m 21s\tremaining: 17.1s\n",
      "922:\tlearn: 0.1892010\ttotal: 3m 22s\tremaining: 16.9s\n",
      "923:\tlearn: 0.1888751\ttotal: 3m 22s\tremaining: 16.6s\n",
      "924:\tlearn: 0.1886367\ttotal: 3m 22s\tremaining: 16.4s\n",
      "925:\tlearn: 0.1883893\ttotal: 3m 22s\tremaining: 16.2s\n",
      "926:\tlearn: 0.1881089\ttotal: 3m 23s\tremaining: 16s\n",
      "927:\tlearn: 0.1878730\ttotal: 3m 23s\tremaining: 15.8s\n",
      "928:\tlearn: 0.1876144\ttotal: 3m 23s\tremaining: 15.5s\n",
      "929:\tlearn: 0.1872841\ttotal: 3m 23s\tremaining: 15.3s\n",
      "930:\tlearn: 0.1869622\ttotal: 3m 23s\tremaining: 15.1s\n",
      "931:\tlearn: 0.1866856\ttotal: 3m 24s\tremaining: 14.9s\n",
      "932:\tlearn: 0.1864348\ttotal: 3m 24s\tremaining: 14.7s\n",
      "933:\tlearn: 0.1861936\ttotal: 3m 24s\tremaining: 14.5s\n",
      "934:\tlearn: 0.1858859\ttotal: 3m 24s\tremaining: 14.2s\n",
      "935:\tlearn: 0.1857485\ttotal: 3m 24s\tremaining: 14s\n",
      "936:\tlearn: 0.1855445\ttotal: 3m 25s\tremaining: 13.8s\n",
      "937:\tlearn: 0.1853119\ttotal: 3m 25s\tremaining: 13.6s\n",
      "938:\tlearn: 0.1850756\ttotal: 3m 25s\tremaining: 13.4s\n",
      "939:\tlearn: 0.1848277\ttotal: 3m 25s\tremaining: 13.1s\n",
      "940:\tlearn: 0.1846633\ttotal: 3m 26s\tremaining: 12.9s\n",
      "941:\tlearn: 0.1843643\ttotal: 3m 26s\tremaining: 12.7s\n",
      "942:\tlearn: 0.1840383\ttotal: 3m 26s\tremaining: 12.5s\n",
      "943:\tlearn: 0.1838226\ttotal: 3m 26s\tremaining: 12.3s\n",
      "944:\tlearn: 0.1835758\ttotal: 3m 26s\tremaining: 12s\n",
      "945:\tlearn: 0.1833760\ttotal: 3m 27s\tremaining: 11.8s\n",
      "946:\tlearn: 0.1832052\ttotal: 3m 27s\tremaining: 11.6s\n",
      "947:\tlearn: 0.1829972\ttotal: 3m 27s\tremaining: 11.4s\n",
      "948:\tlearn: 0.1826989\ttotal: 3m 27s\tremaining: 11.2s\n",
      "949:\tlearn: 0.1825133\ttotal: 3m 27s\tremaining: 10.9s\n",
      "950:\tlearn: 0.1822407\ttotal: 3m 28s\tremaining: 10.7s\n",
      "951:\tlearn: 0.1819585\ttotal: 3m 28s\tremaining: 10.5s\n",
      "952:\tlearn: 0.1816780\ttotal: 3m 28s\tremaining: 10.3s\n",
      "953:\tlearn: 0.1814304\ttotal: 3m 28s\tremaining: 10.1s\n",
      "954:\tlearn: 0.1811203\ttotal: 3m 29s\tremaining: 9.85s\n",
      "955:\tlearn: 0.1808956\ttotal: 3m 29s\tremaining: 9.63s\n",
      "956:\tlearn: 0.1807098\ttotal: 3m 29s\tremaining: 9.41s\n",
      "957:\tlearn: 0.1803747\ttotal: 3m 29s\tremaining: 9.19s\n",
      "958:\tlearn: 0.1801492\ttotal: 3m 29s\tremaining: 8.97s\n",
      "959:\tlearn: 0.1798887\ttotal: 3m 30s\tremaining: 8.75s\n",
      "960:\tlearn: 0.1796558\ttotal: 3m 30s\tremaining: 8.54s\n",
      "961:\tlearn: 0.1794524\ttotal: 3m 30s\tremaining: 8.32s\n",
      "962:\tlearn: 0.1792421\ttotal: 3m 30s\tremaining: 8.1s\n",
      "963:\tlearn: 0.1790301\ttotal: 3m 30s\tremaining: 7.88s\n",
      "964:\tlearn: 0.1787461\ttotal: 3m 31s\tremaining: 7.66s\n",
      "965:\tlearn: 0.1785265\ttotal: 3m 31s\tremaining: 7.44s\n",
      "966:\tlearn: 0.1782350\ttotal: 3m 31s\tremaining: 7.22s\n",
      "967:\tlearn: 0.1780118\ttotal: 3m 31s\tremaining: 7s\n",
      "968:\tlearn: 0.1777372\ttotal: 3m 32s\tremaining: 6.78s\n",
      "969:\tlearn: 0.1774670\ttotal: 3m 32s\tremaining: 6.57s\n",
      "970:\tlearn: 0.1772380\ttotal: 3m 32s\tremaining: 6.35s\n",
      "971:\tlearn: 0.1770094\ttotal: 3m 32s\tremaining: 6.13s\n",
      "972:\tlearn: 0.1768566\ttotal: 3m 32s\tremaining: 5.91s\n",
      "973:\tlearn: 0.1765598\ttotal: 3m 33s\tremaining: 5.69s\n",
      "974:\tlearn: 0.1763345\ttotal: 3m 33s\tremaining: 5.47s\n",
      "975:\tlearn: 0.1760838\ttotal: 3m 33s\tremaining: 5.25s\n",
      "976:\tlearn: 0.1759046\ttotal: 3m 33s\tremaining: 5.03s\n",
      "977:\tlearn: 0.1756704\ttotal: 3m 34s\tremaining: 4.81s\n",
      "978:\tlearn: 0.1754105\ttotal: 3m 34s\tremaining: 4.59s\n",
      "979:\tlearn: 0.1752125\ttotal: 3m 34s\tremaining: 4.38s\n",
      "980:\tlearn: 0.1749491\ttotal: 3m 34s\tremaining: 4.16s\n",
      "981:\tlearn: 0.1746036\ttotal: 3m 34s\tremaining: 3.94s\n",
      "982:\tlearn: 0.1743301\ttotal: 3m 35s\tremaining: 3.72s\n",
      "983:\tlearn: 0.1740745\ttotal: 3m 35s\tremaining: 3.5s\n",
      "984:\tlearn: 0.1737522\ttotal: 3m 35s\tremaining: 3.28s\n",
      "985:\tlearn: 0.1735145\ttotal: 3m 35s\tremaining: 3.06s\n",
      "986:\tlearn: 0.1732832\ttotal: 3m 35s\tremaining: 2.84s\n",
      "987:\tlearn: 0.1730364\ttotal: 3m 36s\tremaining: 2.63s\n",
      "988:\tlearn: 0.1727451\ttotal: 3m 36s\tremaining: 2.41s\n",
      "989:\tlearn: 0.1724772\ttotal: 3m 36s\tremaining: 2.19s\n",
      "990:\tlearn: 0.1721827\ttotal: 3m 36s\tremaining: 1.97s\n",
      "991:\tlearn: 0.1719474\ttotal: 3m 37s\tremaining: 1.75s\n",
      "992:\tlearn: 0.1718102\ttotal: 3m 37s\tremaining: 1.53s\n",
      "993:\tlearn: 0.1715943\ttotal: 3m 37s\tremaining: 1.31s\n",
      "994:\tlearn: 0.1713716\ttotal: 3m 37s\tremaining: 1.09s\n",
      "995:\tlearn: 0.1711302\ttotal: 3m 37s\tremaining: 875ms\n",
      "996:\tlearn: 0.1708852\ttotal: 3m 38s\tremaining: 656ms\n",
      "997:\tlearn: 0.1706800\ttotal: 3m 38s\tremaining: 437ms\n",
      "998:\tlearn: 0.1705075\ttotal: 3m 38s\tremaining: 219ms\n",
      "999:\tlearn: 0.1702543\ttotal: 3m 38s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "#CatBoostVecsOnly\n",
    "cols = [\n",
    "    col for col in X_train.columns if col not in\n",
    "    [\"freq_err_corp\",\"freq_err_corr\",\"freq_corr\",\"freq_corr_corp\"]\n",
    "]\n",
    "CatBoostVecsOnly = CatBoostClassifier(random_state=42)\n",
    "train_and_save_clf(CatBoostVecsOnly, cols, \"CatBoostVecsOnly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.014415\n",
      "0:\tlearn: 0.6895905\ttotal: 238ms\tremaining: 3m 57s\n",
      "1:\tlearn: 0.6853986\ttotal: 513ms\tremaining: 4m 16s\n",
      "2:\tlearn: 0.6811346\ttotal: 862ms\tremaining: 4m 46s\n",
      "3:\tlearn: 0.6777428\ttotal: 1.15s\tremaining: 4m 47s\n",
      "4:\tlearn: 0.6743187\ttotal: 1.45s\tremaining: 4m 49s\n",
      "5:\tlearn: 0.6706569\ttotal: 1.83s\tremaining: 5m 3s\n",
      "6:\tlearn: 0.6676443\ttotal: 2.15s\tremaining: 5m 5s\n",
      "7:\tlearn: 0.6642849\ttotal: 2.47s\tremaining: 5m 6s\n",
      "8:\tlearn: 0.6613035\ttotal: 2.76s\tremaining: 5m 3s\n",
      "9:\tlearn: 0.6582429\ttotal: 3.09s\tremaining: 5m 5s\n",
      "10:\tlearn: 0.6553908\ttotal: 3.4s\tremaining: 5m 6s\n",
      "11:\tlearn: 0.6523948\ttotal: 3.71s\tremaining: 5m 5s\n",
      "12:\tlearn: 0.6495172\ttotal: 4.07s\tremaining: 5m 8s\n",
      "13:\tlearn: 0.6472625\ttotal: 4.35s\tremaining: 5m 6s\n",
      "14:\tlearn: 0.6435950\ttotal: 4.66s\tremaining: 5m 5s\n",
      "15:\tlearn: 0.6403236\ttotal: 4.93s\tremaining: 5m 2s\n",
      "16:\tlearn: 0.6377447\ttotal: 5.2s\tremaining: 5m\n",
      "17:\tlearn: 0.6351513\ttotal: 5.5s\tremaining: 5m\n",
      "18:\tlearn: 0.6329328\ttotal: 5.76s\tremaining: 4m 57s\n",
      "19:\tlearn: 0.6302794\ttotal: 6.09s\tremaining: 4m 58s\n",
      "20:\tlearn: 0.6282098\ttotal: 6.37s\tremaining: 4m 57s\n",
      "21:\tlearn: 0.6259919\ttotal: 6.62s\tremaining: 4m 54s\n",
      "22:\tlearn: 0.6238957\ttotal: 6.88s\tremaining: 4m 52s\n",
      "23:\tlearn: 0.6215579\ttotal: 7.12s\tremaining: 4m 49s\n",
      "24:\tlearn: 0.6192650\ttotal: 7.35s\tremaining: 4m 46s\n",
      "25:\tlearn: 0.6168808\ttotal: 7.62s\tremaining: 4m 45s\n",
      "26:\tlearn: 0.6144467\ttotal: 7.87s\tremaining: 4m 43s\n",
      "27:\tlearn: 0.6116663\ttotal: 8.08s\tremaining: 4m 40s\n",
      "28:\tlearn: 0.6099921\ttotal: 8.26s\tremaining: 4m 36s\n",
      "29:\tlearn: 0.6085747\ttotal: 8.44s\tremaining: 4m 33s\n",
      "30:\tlearn: 0.6070510\ttotal: 8.63s\tremaining: 4m 29s\n",
      "31:\tlearn: 0.6049029\ttotal: 8.82s\tremaining: 4m 26s\n",
      "32:\tlearn: 0.6029882\ttotal: 9.01s\tremaining: 4m 24s\n",
      "33:\tlearn: 0.6004927\ttotal: 9.2s\tremaining: 4m 21s\n",
      "34:\tlearn: 0.5986952\ttotal: 9.39s\tremaining: 4m 18s\n",
      "35:\tlearn: 0.5970676\ttotal: 9.58s\tremaining: 4m 16s\n",
      "36:\tlearn: 0.5956893\ttotal: 9.77s\tremaining: 4m 14s\n",
      "37:\tlearn: 0.5937522\ttotal: 9.96s\tremaining: 4m 12s\n",
      "38:\tlearn: 0.5923466\ttotal: 10.2s\tremaining: 4m 10s\n",
      "39:\tlearn: 0.5911608\ttotal: 10.4s\tremaining: 4m 9s\n",
      "40:\tlearn: 0.5896320\ttotal: 10.6s\tremaining: 4m 7s\n",
      "41:\tlearn: 0.5882251\ttotal: 10.8s\tremaining: 4m 5s\n",
      "42:\tlearn: 0.5866123\ttotal: 11s\tremaining: 4m 3s\n",
      "43:\tlearn: 0.5847302\ttotal: 11.1s\tremaining: 4m 2s\n",
      "44:\tlearn: 0.5830976\ttotal: 11.3s\tremaining: 4m\n",
      "45:\tlearn: 0.5814760\ttotal: 11.5s\tremaining: 3m 58s\n",
      "46:\tlearn: 0.5795728\ttotal: 11.7s\tremaining: 3m 57s\n",
      "47:\tlearn: 0.5783520\ttotal: 11.9s\tremaining: 3m 55s\n",
      "48:\tlearn: 0.5760488\ttotal: 12.1s\tremaining: 3m 54s\n",
      "49:\tlearn: 0.5747960\ttotal: 12.3s\tremaining: 3m 52s\n",
      "50:\tlearn: 0.5734825\ttotal: 12.4s\tremaining: 3m 51s\n",
      "51:\tlearn: 0.5723884\ttotal: 12.6s\tremaining: 3m 50s\n",
      "52:\tlearn: 0.5711340\ttotal: 12.8s\tremaining: 3m 48s\n",
      "53:\tlearn: 0.5698174\ttotal: 13s\tremaining: 3m 47s\n",
      "54:\tlearn: 0.5683896\ttotal: 13.2s\tremaining: 3m 46s\n",
      "55:\tlearn: 0.5666845\ttotal: 13.4s\tremaining: 3m 45s\n",
      "56:\tlearn: 0.5657798\ttotal: 13.6s\tremaining: 3m 44s\n",
      "57:\tlearn: 0.5647891\ttotal: 13.7s\tremaining: 3m 43s\n",
      "58:\tlearn: 0.5636430\ttotal: 13.9s\tremaining: 3m 42s\n",
      "59:\tlearn: 0.5620549\ttotal: 14.1s\tremaining: 3m 41s\n",
      "60:\tlearn: 0.5607375\ttotal: 14.3s\tremaining: 3m 40s\n",
      "61:\tlearn: 0.5597401\ttotal: 14.5s\tremaining: 3m 39s\n",
      "62:\tlearn: 0.5588172\ttotal: 14.7s\tremaining: 3m 38s\n",
      "63:\tlearn: 0.5577341\ttotal: 14.9s\tremaining: 3m 37s\n",
      "64:\tlearn: 0.5564823\ttotal: 15s\tremaining: 3m 36s\n",
      "65:\tlearn: 0.5552398\ttotal: 15.2s\tremaining: 3m 35s\n",
      "66:\tlearn: 0.5539967\ttotal: 15.4s\tremaining: 3m 34s\n",
      "67:\tlearn: 0.5529306\ttotal: 15.6s\tremaining: 3m 33s\n",
      "68:\tlearn: 0.5517693\ttotal: 15.8s\tremaining: 3m 32s\n",
      "69:\tlearn: 0.5506999\ttotal: 15.9s\tremaining: 3m 31s\n",
      "70:\tlearn: 0.5496036\ttotal: 16.1s\tremaining: 3m 31s\n",
      "71:\tlearn: 0.5482841\ttotal: 16.3s\tremaining: 3m 30s\n",
      "72:\tlearn: 0.5470751\ttotal: 16.5s\tremaining: 3m 29s\n",
      "73:\tlearn: 0.5462273\ttotal: 16.7s\tremaining: 3m 28s\n",
      "74:\tlearn: 0.5450666\ttotal: 16.9s\tremaining: 3m 28s\n",
      "75:\tlearn: 0.5442872\ttotal: 17.1s\tremaining: 3m 27s\n",
      "76:\tlearn: 0.5427102\ttotal: 17.2s\tremaining: 3m 26s\n",
      "77:\tlearn: 0.5420374\ttotal: 17.4s\tremaining: 3m 25s\n",
      "78:\tlearn: 0.5412212\ttotal: 17.6s\tremaining: 3m 25s\n",
      "79:\tlearn: 0.5400931\ttotal: 17.8s\tremaining: 3m 24s\n",
      "80:\tlearn: 0.5391129\ttotal: 18s\tremaining: 3m 23s\n",
      "81:\tlearn: 0.5381165\ttotal: 18.2s\tremaining: 3m 23s\n",
      "82:\tlearn: 0.5374201\ttotal: 18.3s\tremaining: 3m 22s\n",
      "83:\tlearn: 0.5363866\ttotal: 18.5s\tremaining: 3m 22s\n",
      "84:\tlearn: 0.5353894\ttotal: 18.7s\tremaining: 3m 21s\n",
      "85:\tlearn: 0.5347378\ttotal: 18.9s\tremaining: 3m 20s\n",
      "86:\tlearn: 0.5336284\ttotal: 19.1s\tremaining: 3m 20s\n",
      "87:\tlearn: 0.5324017\ttotal: 19.3s\tremaining: 3m 19s\n",
      "88:\tlearn: 0.5316421\ttotal: 19.4s\tremaining: 3m 19s\n",
      "89:\tlearn: 0.5307986\ttotal: 19.6s\tremaining: 3m 18s\n",
      "90:\tlearn: 0.5298803\ttotal: 19.8s\tremaining: 3m 17s\n",
      "91:\tlearn: 0.5291132\ttotal: 20s\tremaining: 3m 17s\n",
      "92:\tlearn: 0.5282458\ttotal: 20.2s\tremaining: 3m 16s\n",
      "93:\tlearn: 0.5273804\ttotal: 20.4s\tremaining: 3m 16s\n",
      "94:\tlearn: 0.5265904\ttotal: 20.6s\tremaining: 3m 15s\n",
      "95:\tlearn: 0.5254468\ttotal: 20.7s\tremaining: 3m 15s\n",
      "96:\tlearn: 0.5246045\ttotal: 20.9s\tremaining: 3m 14s\n",
      "97:\tlearn: 0.5238252\ttotal: 21.1s\tremaining: 3m 14s\n",
      "98:\tlearn: 0.5232151\ttotal: 21.3s\tremaining: 3m 13s\n",
      "99:\tlearn: 0.5225441\ttotal: 21.5s\tremaining: 3m 13s\n",
      "100:\tlearn: 0.5216977\ttotal: 21.7s\tremaining: 3m 12s\n",
      "101:\tlearn: 0.5210213\ttotal: 21.9s\tremaining: 3m 12s\n",
      "102:\tlearn: 0.5203286\ttotal: 22s\tremaining: 3m 11s\n",
      "103:\tlearn: 0.5193384\ttotal: 22.2s\tremaining: 3m 11s\n",
      "104:\tlearn: 0.5184984\ttotal: 22.4s\tremaining: 3m 11s\n",
      "105:\tlearn: 0.5177324\ttotal: 22.6s\tremaining: 3m 10s\n",
      "106:\tlearn: 0.5169933\ttotal: 22.8s\tremaining: 3m 10s\n",
      "107:\tlearn: 0.5162304\ttotal: 23s\tremaining: 3m 9s\n",
      "108:\tlearn: 0.5151546\ttotal: 23.2s\tremaining: 3m 9s\n",
      "109:\tlearn: 0.5143091\ttotal: 23.3s\tremaining: 3m 8s\n",
      "110:\tlearn: 0.5131689\ttotal: 23.5s\tremaining: 3m 8s\n",
      "111:\tlearn: 0.5124228\ttotal: 23.7s\tremaining: 3m 7s\n",
      "112:\tlearn: 0.5117041\ttotal: 23.9s\tremaining: 3m 7s\n",
      "113:\tlearn: 0.5109375\ttotal: 24.1s\tremaining: 3m 7s\n",
      "114:\tlearn: 0.5104337\ttotal: 24.3s\tremaining: 3m 6s\n",
      "115:\tlearn: 0.5098392\ttotal: 24.4s\tremaining: 3m 6s\n",
      "116:\tlearn: 0.5089450\ttotal: 24.6s\tremaining: 3m 5s\n",
      "117:\tlearn: 0.5082940\ttotal: 24.8s\tremaining: 3m 5s\n",
      "118:\tlearn: 0.5075859\ttotal: 25s\tremaining: 3m 5s\n",
      "119:\tlearn: 0.5066512\ttotal: 25.2s\tremaining: 3m 4s\n",
      "120:\tlearn: 0.5058970\ttotal: 25.4s\tremaining: 3m 4s\n",
      "121:\tlearn: 0.5050600\ttotal: 25.6s\tremaining: 3m 3s\n",
      "122:\tlearn: 0.5044140\ttotal: 25.7s\tremaining: 3m 3s\n",
      "123:\tlearn: 0.5037465\ttotal: 25.9s\tremaining: 3m 3s\n",
      "124:\tlearn: 0.5030413\ttotal: 26.1s\tremaining: 3m 2s\n",
      "125:\tlearn: 0.5022580\ttotal: 26.3s\tremaining: 3m 2s\n",
      "126:\tlearn: 0.5016992\ttotal: 26.5s\tremaining: 3m 1s\n",
      "127:\tlearn: 0.5011047\ttotal: 26.7s\tremaining: 3m 1s\n",
      "128:\tlearn: 0.5002520\ttotal: 26.8s\tremaining: 3m 1s\n",
      "129:\tlearn: 0.4993149\ttotal: 27s\tremaining: 3m\n",
      "130:\tlearn: 0.4984893\ttotal: 27.2s\tremaining: 3m\n",
      "131:\tlearn: 0.4978874\ttotal: 27.4s\tremaining: 3m\n",
      "132:\tlearn: 0.4972536\ttotal: 27.6s\tremaining: 2m 59s\n",
      "133:\tlearn: 0.4966123\ttotal: 27.8s\tremaining: 2m 59s\n",
      "134:\tlearn: 0.4961040\ttotal: 27.9s\tremaining: 2m 59s\n",
      "135:\tlearn: 0.4954532\ttotal: 28.1s\tremaining: 2m 58s\n",
      "136:\tlearn: 0.4949118\ttotal: 28.3s\tremaining: 2m 58s\n",
      "137:\tlearn: 0.4941132\ttotal: 28.5s\tremaining: 2m 57s\n",
      "138:\tlearn: 0.4933881\ttotal: 28.7s\tremaining: 2m 57s\n",
      "139:\tlearn: 0.4928788\ttotal: 28.9s\tremaining: 2m 57s\n",
      "140:\tlearn: 0.4919927\ttotal: 29s\tremaining: 2m 56s\n",
      "141:\tlearn: 0.4910139\ttotal: 29.2s\tremaining: 2m 56s\n",
      "142:\tlearn: 0.4900860\ttotal: 29.4s\tremaining: 2m 56s\n",
      "143:\tlearn: 0.4895108\ttotal: 29.6s\tremaining: 2m 55s\n",
      "144:\tlearn: 0.4891121\ttotal: 29.8s\tremaining: 2m 55s\n",
      "145:\tlearn: 0.4885008\ttotal: 30s\tremaining: 2m 55s\n",
      "146:\tlearn: 0.4878640\ttotal: 30.1s\tremaining: 2m 54s\n",
      "147:\tlearn: 0.4870901\ttotal: 30.3s\tremaining: 2m 54s\n",
      "148:\tlearn: 0.4865004\ttotal: 30.5s\tremaining: 2m 54s\n",
      "149:\tlearn: 0.4858668\ttotal: 30.7s\tremaining: 2m 54s\n",
      "150:\tlearn: 0.4853007\ttotal: 30.9s\tremaining: 2m 53s\n",
      "151:\tlearn: 0.4844500\ttotal: 31.1s\tremaining: 2m 53s\n",
      "152:\tlearn: 0.4839846\ttotal: 31.3s\tremaining: 2m 53s\n",
      "153:\tlearn: 0.4831577\ttotal: 31.5s\tremaining: 2m 52s\n",
      "154:\tlearn: 0.4826502\ttotal: 31.7s\tremaining: 2m 52s\n",
      "155:\tlearn: 0.4819455\ttotal: 31.9s\tremaining: 2m 52s\n",
      "156:\tlearn: 0.4813199\ttotal: 32s\tremaining: 2m 52s\n",
      "157:\tlearn: 0.4806111\ttotal: 32.2s\tremaining: 2m 51s\n",
      "158:\tlearn: 0.4800335\ttotal: 32.4s\tremaining: 2m 51s\n",
      "159:\tlearn: 0.4794166\ttotal: 32.6s\tremaining: 2m 51s\n",
      "160:\tlearn: 0.4789390\ttotal: 32.8s\tremaining: 2m 50s\n",
      "161:\tlearn: 0.4784299\ttotal: 33s\tremaining: 2m 50s\n",
      "162:\tlearn: 0.4778208\ttotal: 33.2s\tremaining: 2m 50s\n",
      "163:\tlearn: 0.4775347\ttotal: 33.4s\tremaining: 2m 50s\n",
      "164:\tlearn: 0.4768068\ttotal: 33.6s\tremaining: 2m 49s\n",
      "165:\tlearn: 0.4761616\ttotal: 33.8s\tremaining: 2m 49s\n",
      "166:\tlearn: 0.4755444\ttotal: 34s\tremaining: 2m 49s\n",
      "167:\tlearn: 0.4750162\ttotal: 34.1s\tremaining: 2m 49s\n",
      "168:\tlearn: 0.4744734\ttotal: 34.3s\tremaining: 2m 48s\n",
      "169:\tlearn: 0.4738664\ttotal: 34.5s\tremaining: 2m 48s\n",
      "170:\tlearn: 0.4735163\ttotal: 34.7s\tremaining: 2m 48s\n",
      "171:\tlearn: 0.4731480\ttotal: 34.9s\tremaining: 2m 48s\n",
      "172:\tlearn: 0.4724984\ttotal: 35.1s\tremaining: 2m 47s\n",
      "173:\tlearn: 0.4718133\ttotal: 35.3s\tremaining: 2m 47s\n",
      "174:\tlearn: 0.4712005\ttotal: 35.5s\tremaining: 2m 47s\n",
      "175:\tlearn: 0.4706667\ttotal: 35.7s\tremaining: 2m 46s\n",
      "176:\tlearn: 0.4701129\ttotal: 35.9s\tremaining: 2m 46s\n",
      "177:\tlearn: 0.4695234\ttotal: 36.1s\tremaining: 2m 46s\n",
      "178:\tlearn: 0.4688544\ttotal: 36.2s\tremaining: 2m 46s\n",
      "179:\tlearn: 0.4682626\ttotal: 36.4s\tremaining: 2m 46s\n",
      "180:\tlearn: 0.4675956\ttotal: 36.6s\tremaining: 2m 45s\n",
      "181:\tlearn: 0.4669978\ttotal: 36.8s\tremaining: 2m 45s\n",
      "182:\tlearn: 0.4664753\ttotal: 37s\tremaining: 2m 45s\n",
      "183:\tlearn: 0.4658273\ttotal: 37.2s\tremaining: 2m 44s\n",
      "184:\tlearn: 0.4652079\ttotal: 37.4s\tremaining: 2m 44s\n",
      "185:\tlearn: 0.4645391\ttotal: 37.5s\tremaining: 2m 44s\n",
      "186:\tlearn: 0.4640063\ttotal: 37.7s\tremaining: 2m 43s\n",
      "187:\tlearn: 0.4633583\ttotal: 37.9s\tremaining: 2m 43s\n",
      "188:\tlearn: 0.4626533\ttotal: 38.1s\tremaining: 2m 43s\n",
      "189:\tlearn: 0.4619235\ttotal: 38.3s\tremaining: 2m 43s\n",
      "190:\tlearn: 0.4613909\ttotal: 38.5s\tremaining: 2m 43s\n",
      "191:\tlearn: 0.4608766\ttotal: 38.7s\tremaining: 2m 42s\n",
      "192:\tlearn: 0.4605786\ttotal: 38.9s\tremaining: 2m 42s\n",
      "193:\tlearn: 0.4600692\ttotal: 39s\tremaining: 2m 42s\n",
      "194:\tlearn: 0.4594327\ttotal: 39.2s\tremaining: 2m 41s\n",
      "195:\tlearn: 0.4589735\ttotal: 39.4s\tremaining: 2m 41s\n",
      "196:\tlearn: 0.4584645\ttotal: 39.6s\tremaining: 2m 41s\n",
      "197:\tlearn: 0.4580610\ttotal: 39.8s\tremaining: 2m 41s\n",
      "198:\tlearn: 0.4575724\ttotal: 40s\tremaining: 2m 40s\n",
      "199:\tlearn: 0.4569053\ttotal: 40.1s\tremaining: 2m 40s\n",
      "200:\tlearn: 0.4565069\ttotal: 40.3s\tremaining: 2m 40s\n",
      "201:\tlearn: 0.4556646\ttotal: 40.5s\tremaining: 2m 40s\n",
      "202:\tlearn: 0.4552269\ttotal: 40.7s\tremaining: 2m 39s\n",
      "203:\tlearn: 0.4546855\ttotal: 40.9s\tremaining: 2m 39s\n",
      "204:\tlearn: 0.4542896\ttotal: 41.1s\tremaining: 2m 39s\n",
      "205:\tlearn: 0.4537357\ttotal: 41.2s\tremaining: 2m 38s\n",
      "206:\tlearn: 0.4531566\ttotal: 41.4s\tremaining: 2m 38s\n",
      "207:\tlearn: 0.4526632\ttotal: 41.6s\tremaining: 2m 38s\n",
      "208:\tlearn: 0.4518423\ttotal: 41.8s\tremaining: 2m 38s\n",
      "209:\tlearn: 0.4513131\ttotal: 42s\tremaining: 2m 37s\n",
      "210:\tlearn: 0.4508438\ttotal: 42.2s\tremaining: 2m 37s\n",
      "211:\tlearn: 0.4502098\ttotal: 42.3s\tremaining: 2m 37s\n",
      "212:\tlearn: 0.4496221\ttotal: 42.5s\tremaining: 2m 37s\n",
      "213:\tlearn: 0.4491721\ttotal: 42.7s\tremaining: 2m 36s\n",
      "214:\tlearn: 0.4484857\ttotal: 42.9s\tremaining: 2m 36s\n",
      "215:\tlearn: 0.4481270\ttotal: 43.1s\tremaining: 2m 36s\n",
      "216:\tlearn: 0.4476677\ttotal: 43.3s\tremaining: 2m 36s\n",
      "217:\tlearn: 0.4472417\ttotal: 43.5s\tremaining: 2m 35s\n",
      "218:\tlearn: 0.4467506\ttotal: 43.6s\tremaining: 2m 35s\n",
      "219:\tlearn: 0.4463229\ttotal: 43.8s\tremaining: 2m 35s\n",
      "220:\tlearn: 0.4456764\ttotal: 44s\tremaining: 2m 35s\n",
      "221:\tlearn: 0.4451652\ttotal: 44.2s\tremaining: 2m 34s\n",
      "222:\tlearn: 0.4446747\ttotal: 44.4s\tremaining: 2m 34s\n",
      "223:\tlearn: 0.4441596\ttotal: 44.6s\tremaining: 2m 34s\n",
      "224:\tlearn: 0.4436242\ttotal: 44.7s\tremaining: 2m 34s\n",
      "225:\tlearn: 0.4432308\ttotal: 44.9s\tremaining: 2m 33s\n",
      "226:\tlearn: 0.4426753\ttotal: 45.1s\tremaining: 2m 33s\n",
      "227:\tlearn: 0.4420190\ttotal: 45.3s\tremaining: 2m 33s\n",
      "228:\tlearn: 0.4412246\ttotal: 45.5s\tremaining: 2m 33s\n",
      "229:\tlearn: 0.4405508\ttotal: 45.7s\tremaining: 2m 32s\n",
      "230:\tlearn: 0.4399658\ttotal: 45.9s\tremaining: 2m 32s\n",
      "231:\tlearn: 0.4396324\ttotal: 46s\tremaining: 2m 32s\n",
      "232:\tlearn: 0.4390010\ttotal: 46.2s\tremaining: 2m 32s\n",
      "233:\tlearn: 0.4386605\ttotal: 46.4s\tremaining: 2m 31s\n",
      "234:\tlearn: 0.4383452\ttotal: 46.6s\tremaining: 2m 31s\n",
      "235:\tlearn: 0.4379160\ttotal: 46.8s\tremaining: 2m 31s\n",
      "236:\tlearn: 0.4374110\ttotal: 47s\tremaining: 2m 31s\n",
      "237:\tlearn: 0.4368183\ttotal: 47.1s\tremaining: 2m 30s\n",
      "238:\tlearn: 0.4364647\ttotal: 47.3s\tremaining: 2m 30s\n",
      "239:\tlearn: 0.4360865\ttotal: 47.5s\tremaining: 2m 30s\n",
      "240:\tlearn: 0.4356007\ttotal: 47.7s\tremaining: 2m 30s\n",
      "241:\tlearn: 0.4351629\ttotal: 47.9s\tremaining: 2m 29s\n",
      "242:\tlearn: 0.4347439\ttotal: 48.1s\tremaining: 2m 29s\n",
      "243:\tlearn: 0.4341865\ttotal: 48.2s\tremaining: 2m 29s\n",
      "244:\tlearn: 0.4338131\ttotal: 48.4s\tremaining: 2m 29s\n",
      "245:\tlearn: 0.4332361\ttotal: 48.6s\tremaining: 2m 29s\n",
      "246:\tlearn: 0.4327409\ttotal: 48.8s\tremaining: 2m 28s\n",
      "247:\tlearn: 0.4323596\ttotal: 49s\tremaining: 2m 28s\n",
      "248:\tlearn: 0.4318940\ttotal: 49.2s\tremaining: 2m 28s\n",
      "249:\tlearn: 0.4312896\ttotal: 49.4s\tremaining: 2m 28s\n",
      "250:\tlearn: 0.4308805\ttotal: 49.5s\tremaining: 2m 27s\n",
      "251:\tlearn: 0.4303924\ttotal: 49.7s\tremaining: 2m 27s\n",
      "252:\tlearn: 0.4299165\ttotal: 49.9s\tremaining: 2m 27s\n",
      "253:\tlearn: 0.4294092\ttotal: 50.1s\tremaining: 2m 27s\n",
      "254:\tlearn: 0.4286418\ttotal: 50.3s\tremaining: 2m 26s\n",
      "255:\tlearn: 0.4282106\ttotal: 50.5s\tremaining: 2m 26s\n",
      "256:\tlearn: 0.4276803\ttotal: 50.7s\tremaining: 2m 26s\n",
      "257:\tlearn: 0.4270280\ttotal: 51s\tremaining: 2m 26s\n",
      "258:\tlearn: 0.4264643\ttotal: 51.2s\tremaining: 2m 26s\n",
      "259:\tlearn: 0.4259352\ttotal: 51.4s\tremaining: 2m 26s\n",
      "260:\tlearn: 0.4253949\ttotal: 51.6s\tremaining: 2m 26s\n",
      "261:\tlearn: 0.4249507\ttotal: 51.8s\tremaining: 2m 26s\n",
      "262:\tlearn: 0.4243149\ttotal: 52s\tremaining: 2m 25s\n",
      "263:\tlearn: 0.4239283\ttotal: 52.2s\tremaining: 2m 25s\n",
      "264:\tlearn: 0.4235155\ttotal: 52.5s\tremaining: 2m 25s\n",
      "265:\tlearn: 0.4231351\ttotal: 52.7s\tremaining: 2m 25s\n",
      "266:\tlearn: 0.4226616\ttotal: 52.8s\tremaining: 2m 25s\n",
      "267:\tlearn: 0.4221032\ttotal: 53s\tremaining: 2m 24s\n",
      "268:\tlearn: 0.4217898\ttotal: 53.2s\tremaining: 2m 24s\n",
      "269:\tlearn: 0.4212613\ttotal: 53.4s\tremaining: 2m 24s\n",
      "270:\tlearn: 0.4207801\ttotal: 53.6s\tremaining: 2m 24s\n",
      "271:\tlearn: 0.4202475\ttotal: 53.8s\tremaining: 2m 24s\n",
      "272:\tlearn: 0.4199445\ttotal: 54.1s\tremaining: 2m 23s\n",
      "273:\tlearn: 0.4193738\ttotal: 54.3s\tremaining: 2m 23s\n",
      "274:\tlearn: 0.4190001\ttotal: 54.5s\tremaining: 2m 23s\n",
      "275:\tlearn: 0.4184502\ttotal: 54.7s\tremaining: 2m 23s\n",
      "276:\tlearn: 0.4180790\ttotal: 54.9s\tremaining: 2m 23s\n",
      "277:\tlearn: 0.4176981\ttotal: 55.1s\tremaining: 2m 23s\n",
      "278:\tlearn: 0.4172826\ttotal: 55.3s\tremaining: 2m 22s\n",
      "279:\tlearn: 0.4167433\ttotal: 55.5s\tremaining: 2m 22s\n",
      "280:\tlearn: 0.4161823\ttotal: 55.7s\tremaining: 2m 22s\n",
      "281:\tlearn: 0.4157516\ttotal: 55.9s\tremaining: 2m 22s\n",
      "282:\tlearn: 0.4152353\ttotal: 56.1s\tremaining: 2m 22s\n",
      "283:\tlearn: 0.4145686\ttotal: 56.3s\tremaining: 2m 21s\n",
      "284:\tlearn: 0.4142802\ttotal: 56.5s\tremaining: 2m 21s\n",
      "285:\tlearn: 0.4139562\ttotal: 56.7s\tremaining: 2m 21s\n",
      "286:\tlearn: 0.4137057\ttotal: 56.9s\tremaining: 2m 21s\n",
      "287:\tlearn: 0.4131583\ttotal: 57.1s\tremaining: 2m 21s\n",
      "288:\tlearn: 0.4126854\ttotal: 57.2s\tremaining: 2m 20s\n",
      "289:\tlearn: 0.4121201\ttotal: 57.4s\tremaining: 2m 20s\n",
      "290:\tlearn: 0.4116324\ttotal: 57.6s\tremaining: 2m 20s\n",
      "291:\tlearn: 0.4113016\ttotal: 57.8s\tremaining: 2m 20s\n",
      "292:\tlearn: 0.4108701\ttotal: 58s\tremaining: 2m 20s\n",
      "293:\tlearn: 0.4102794\ttotal: 58.2s\tremaining: 2m 19s\n",
      "294:\tlearn: 0.4098237\ttotal: 58.4s\tremaining: 2m 19s\n",
      "295:\tlearn: 0.4095177\ttotal: 58.6s\tremaining: 2m 19s\n",
      "296:\tlearn: 0.4091782\ttotal: 58.7s\tremaining: 2m 19s\n",
      "297:\tlearn: 0.4088040\ttotal: 58.9s\tremaining: 2m 18s\n",
      "298:\tlearn: 0.4083255\ttotal: 59.1s\tremaining: 2m 18s\n",
      "299:\tlearn: 0.4077766\ttotal: 59.3s\tremaining: 2m 18s\n",
      "300:\tlearn: 0.4072420\ttotal: 59.5s\tremaining: 2m 18s\n",
      "301:\tlearn: 0.4068549\ttotal: 59.7s\tremaining: 2m 17s\n",
      "302:\tlearn: 0.4064313\ttotal: 59.9s\tremaining: 2m 17s\n",
      "303:\tlearn: 0.4061437\ttotal: 1m\tremaining: 2m 17s\n",
      "304:\tlearn: 0.4057631\ttotal: 1m\tremaining: 2m 17s\n",
      "305:\tlearn: 0.4052612\ttotal: 1m\tremaining: 2m 17s\n",
      "306:\tlearn: 0.4047697\ttotal: 1m\tremaining: 2m 16s\n",
      "307:\tlearn: 0.4042334\ttotal: 1m\tremaining: 2m 16s\n",
      "308:\tlearn: 0.4036572\ttotal: 1m 1s\tremaining: 2m 16s\n",
      "309:\tlearn: 0.4033012\ttotal: 1m 1s\tremaining: 2m 16s\n",
      "310:\tlearn: 0.4026819\ttotal: 1m 1s\tremaining: 2m 16s\n",
      "311:\tlearn: 0.4021252\ttotal: 1m 1s\tremaining: 2m 15s\n",
      "312:\tlearn: 0.4018093\ttotal: 1m 1s\tremaining: 2m 15s\n",
      "313:\tlearn: 0.4012379\ttotal: 1m 2s\tremaining: 2m 15s\n",
      "314:\tlearn: 0.4008108\ttotal: 1m 2s\tremaining: 2m 15s\n",
      "315:\tlearn: 0.4005046\ttotal: 1m 2s\tremaining: 2m 15s\n",
      "316:\tlearn: 0.4001293\ttotal: 1m 2s\tremaining: 2m 14s\n",
      "317:\tlearn: 0.3996730\ttotal: 1m 2s\tremaining: 2m 14s\n",
      "318:\tlearn: 0.3991772\ttotal: 1m 2s\tremaining: 2m 14s\n",
      "319:\tlearn: 0.3987817\ttotal: 1m 3s\tremaining: 2m 14s\n",
      "320:\tlearn: 0.3982579\ttotal: 1m 3s\tremaining: 2m 14s\n",
      "321:\tlearn: 0.3978124\ttotal: 1m 3s\tremaining: 2m 13s\n",
      "322:\tlearn: 0.3972167\ttotal: 1m 3s\tremaining: 2m 13s\n",
      "323:\tlearn: 0.3969965\ttotal: 1m 4s\tremaining: 2m 13s\n",
      "324:\tlearn: 0.3965851\ttotal: 1m 4s\tremaining: 2m 13s\n",
      "325:\tlearn: 0.3960590\ttotal: 1m 4s\tremaining: 2m 13s\n",
      "326:\tlearn: 0.3956188\ttotal: 1m 4s\tremaining: 2m 13s\n",
      "327:\tlearn: 0.3952742\ttotal: 1m 4s\tremaining: 2m 12s\n",
      "328:\tlearn: 0.3947003\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "329:\tlearn: 0.3943163\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "330:\tlearn: 0.3939451\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "331:\tlearn: 0.3934136\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "332:\tlearn: 0.3929965\ttotal: 1m 5s\tremaining: 2m 12s\n",
      "333:\tlearn: 0.3925147\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "334:\tlearn: 0.3920206\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "335:\tlearn: 0.3916103\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "336:\tlearn: 0.3911812\ttotal: 1m 6s\tremaining: 2m 11s\n",
      "337:\tlearn: 0.3907467\ttotal: 1m 6s\tremaining: 2m 10s\n",
      "338:\tlearn: 0.3904186\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "339:\tlearn: 0.3900306\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "340:\tlearn: 0.3896106\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "341:\tlearn: 0.3891405\ttotal: 1m 7s\tremaining: 2m 10s\n",
      "342:\tlearn: 0.3887203\ttotal: 1m 7s\tremaining: 2m 9s\n",
      "343:\tlearn: 0.3883008\ttotal: 1m 7s\tremaining: 2m 9s\n",
      "344:\tlearn: 0.3879832\ttotal: 1m 8s\tremaining: 2m 9s\n",
      "345:\tlearn: 0.3875476\ttotal: 1m 8s\tremaining: 2m 9s\n",
      "346:\tlearn: 0.3871158\ttotal: 1m 8s\tremaining: 2m 8s\n",
      "347:\tlearn: 0.3867606\ttotal: 1m 8s\tremaining: 2m 8s\n",
      "348:\tlearn: 0.3863547\ttotal: 1m 8s\tremaining: 2m 8s\n",
      "349:\tlearn: 0.3858429\ttotal: 1m 9s\tremaining: 2m 8s\n",
      "350:\tlearn: 0.3854287\ttotal: 1m 9s\tremaining: 2m 8s\n",
      "351:\tlearn: 0.3850234\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "352:\tlearn: 0.3845628\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "353:\tlearn: 0.3841802\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "354:\tlearn: 0.3837892\ttotal: 1m 9s\tremaining: 2m 7s\n",
      "355:\tlearn: 0.3835086\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "356:\tlearn: 0.3831407\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "357:\tlearn: 0.3828028\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "358:\tlearn: 0.3824272\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "359:\tlearn: 0.3820052\ttotal: 1m 10s\tremaining: 2m 6s\n",
      "360:\tlearn: 0.3815923\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "361:\tlearn: 0.3813048\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "362:\tlearn: 0.3808027\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "363:\tlearn: 0.3804742\ttotal: 1m 11s\tremaining: 2m 5s\n",
      "364:\tlearn: 0.3800916\ttotal: 1m 11s\tremaining: 2m 4s\n",
      "365:\tlearn: 0.3796190\ttotal: 1m 12s\tremaining: 2m 4s\n",
      "366:\tlearn: 0.3793329\ttotal: 1m 12s\tremaining: 2m 4s\n",
      "367:\tlearn: 0.3790026\ttotal: 1m 12s\tremaining: 2m 4s\n",
      "368:\tlearn: 0.3786280\ttotal: 1m 12s\tremaining: 2m 4s\n",
      "369:\tlearn: 0.3781887\ttotal: 1m 12s\tremaining: 2m 3s\n",
      "370:\tlearn: 0.3778694\ttotal: 1m 12s\tremaining: 2m 3s\n",
      "371:\tlearn: 0.3775372\ttotal: 1m 13s\tremaining: 2m 3s\n",
      "372:\tlearn: 0.3772806\ttotal: 1m 13s\tremaining: 2m 3s\n",
      "373:\tlearn: 0.3769309\ttotal: 1m 13s\tremaining: 2m 3s\n",
      "374:\tlearn: 0.3765017\ttotal: 1m 13s\tremaining: 2m 3s\n",
      "375:\tlearn: 0.3759927\ttotal: 1m 14s\tremaining: 2m 2s\n",
      "376:\tlearn: 0.3755724\ttotal: 1m 14s\tremaining: 2m 2s\n",
      "377:\tlearn: 0.3751866\ttotal: 1m 14s\tremaining: 2m 2s\n",
      "378:\tlearn: 0.3748415\ttotal: 1m 14s\tremaining: 2m 2s\n",
      "379:\tlearn: 0.3745134\ttotal: 1m 14s\tremaining: 2m 2s\n",
      "380:\tlearn: 0.3741779\ttotal: 1m 14s\tremaining: 2m 1s\n",
      "381:\tlearn: 0.3737882\ttotal: 1m 15s\tremaining: 2m 1s\n",
      "382:\tlearn: 0.3734122\ttotal: 1m 15s\tremaining: 2m 1s\n",
      "383:\tlearn: 0.3728740\ttotal: 1m 15s\tremaining: 2m 1s\n",
      "384:\tlearn: 0.3724807\ttotal: 1m 15s\tremaining: 2m\n",
      "385:\tlearn: 0.3720693\ttotal: 1m 15s\tremaining: 2m\n",
      "386:\tlearn: 0.3715616\ttotal: 1m 16s\tremaining: 2m\n",
      "387:\tlearn: 0.3712371\ttotal: 1m 16s\tremaining: 2m\n",
      "388:\tlearn: 0.3708411\ttotal: 1m 16s\tremaining: 2m\n",
      "389:\tlearn: 0.3705777\ttotal: 1m 16s\tremaining: 1m 59s\n",
      "390:\tlearn: 0.3701404\ttotal: 1m 16s\tremaining: 1m 59s\n",
      "391:\tlearn: 0.3697270\ttotal: 1m 16s\tremaining: 1m 59s\n",
      "392:\tlearn: 0.3694575\ttotal: 1m 17s\tremaining: 1m 59s\n",
      "393:\tlearn: 0.3689691\ttotal: 1m 17s\tremaining: 1m 58s\n",
      "394:\tlearn: 0.3686333\ttotal: 1m 17s\tremaining: 1m 58s\n",
      "395:\tlearn: 0.3683015\ttotal: 1m 17s\tremaining: 1m 58s\n",
      "396:\tlearn: 0.3678468\ttotal: 1m 17s\tremaining: 1m 58s\n",
      "397:\tlearn: 0.3674724\ttotal: 1m 18s\tremaining: 1m 58s\n",
      "398:\tlearn: 0.3670759\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "399:\tlearn: 0.3668330\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "400:\tlearn: 0.3663442\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "401:\tlearn: 0.3659659\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "402:\tlearn: 0.3653757\ttotal: 1m 18s\tremaining: 1m 57s\n",
      "403:\tlearn: 0.3649881\ttotal: 1m 19s\tremaining: 1m 56s\n",
      "404:\tlearn: 0.3644534\ttotal: 1m 19s\tremaining: 1m 56s\n",
      "405:\tlearn: 0.3641410\ttotal: 1m 19s\tremaining: 1m 56s\n",
      "406:\tlearn: 0.3636944\ttotal: 1m 19s\tremaining: 1m 56s\n",
      "407:\tlearn: 0.3633606\ttotal: 1m 19s\tremaining: 1m 55s\n",
      "408:\tlearn: 0.3630197\ttotal: 1m 20s\tremaining: 1m 55s\n",
      "409:\tlearn: 0.3626154\ttotal: 1m 20s\tremaining: 1m 55s\n",
      "410:\tlearn: 0.3620890\ttotal: 1m 20s\tremaining: 1m 55s\n",
      "411:\tlearn: 0.3618314\ttotal: 1m 20s\tremaining: 1m 55s\n",
      "412:\tlearn: 0.3615016\ttotal: 1m 20s\tremaining: 1m 54s\n",
      "413:\tlearn: 0.3611442\ttotal: 1m 21s\tremaining: 1m 54s\n",
      "414:\tlearn: 0.3608356\ttotal: 1m 21s\tremaining: 1m 54s\n",
      "415:\tlearn: 0.3604904\ttotal: 1m 21s\tremaining: 1m 54s\n",
      "416:\tlearn: 0.3602271\ttotal: 1m 21s\tremaining: 1m 54s\n",
      "417:\tlearn: 0.3598397\ttotal: 1m 21s\tremaining: 1m 53s\n",
      "418:\tlearn: 0.3594438\ttotal: 1m 21s\tremaining: 1m 53s\n",
      "419:\tlearn: 0.3589752\ttotal: 1m 22s\tremaining: 1m 53s\n",
      "420:\tlearn: 0.3587053\ttotal: 1m 22s\tremaining: 1m 53s\n",
      "421:\tlearn: 0.3584178\ttotal: 1m 22s\tremaining: 1m 53s\n",
      "422:\tlearn: 0.3581430\ttotal: 1m 22s\tremaining: 1m 52s\n",
      "423:\tlearn: 0.3577580\ttotal: 1m 22s\tremaining: 1m 52s\n",
      "424:\tlearn: 0.3572997\ttotal: 1m 23s\tremaining: 1m 52s\n",
      "425:\tlearn: 0.3570340\ttotal: 1m 23s\tremaining: 1m 52s\n",
      "426:\tlearn: 0.3567333\ttotal: 1m 23s\tremaining: 1m 51s\n",
      "427:\tlearn: 0.3563976\ttotal: 1m 23s\tremaining: 1m 51s\n",
      "428:\tlearn: 0.3560647\ttotal: 1m 23s\tremaining: 1m 51s\n",
      "429:\tlearn: 0.3557181\ttotal: 1m 24s\tremaining: 1m 51s\n",
      "430:\tlearn: 0.3553526\ttotal: 1m 24s\tremaining: 1m 51s\n",
      "431:\tlearn: 0.3550053\ttotal: 1m 24s\tremaining: 1m 50s\n",
      "432:\tlearn: 0.3547129\ttotal: 1m 24s\tremaining: 1m 50s\n",
      "433:\tlearn: 0.3544442\ttotal: 1m 24s\tremaining: 1m 50s\n",
      "434:\tlearn: 0.3541358\ttotal: 1m 24s\tremaining: 1m 50s\n",
      "435:\tlearn: 0.3537395\ttotal: 1m 25s\tremaining: 1m 50s\n",
      "436:\tlearn: 0.3533737\ttotal: 1m 25s\tremaining: 1m 49s\n",
      "437:\tlearn: 0.3530197\ttotal: 1m 25s\tremaining: 1m 49s\n",
      "438:\tlearn: 0.3525302\ttotal: 1m 25s\tremaining: 1m 49s\n",
      "439:\tlearn: 0.3521680\ttotal: 1m 25s\tremaining: 1m 49s\n",
      "440:\tlearn: 0.3517898\ttotal: 1m 26s\tremaining: 1m 49s\n",
      "441:\tlearn: 0.3513994\ttotal: 1m 26s\tremaining: 1m 48s\n",
      "442:\tlearn: 0.3510582\ttotal: 1m 26s\tremaining: 1m 48s\n",
      "443:\tlearn: 0.3507345\ttotal: 1m 26s\tremaining: 1m 48s\n",
      "444:\tlearn: 0.3503758\ttotal: 1m 26s\tremaining: 1m 48s\n",
      "445:\tlearn: 0.3499468\ttotal: 1m 27s\tremaining: 1m 48s\n",
      "446:\tlearn: 0.3495965\ttotal: 1m 27s\tremaining: 1m 47s\n",
      "447:\tlearn: 0.3492379\ttotal: 1m 27s\tremaining: 1m 47s\n",
      "448:\tlearn: 0.3489279\ttotal: 1m 27s\tremaining: 1m 47s\n",
      "449:\tlearn: 0.3486223\ttotal: 1m 27s\tremaining: 1m 47s\n",
      "450:\tlearn: 0.3482210\ttotal: 1m 28s\tremaining: 1m 47s\n",
      "451:\tlearn: 0.3480001\ttotal: 1m 28s\tremaining: 1m 47s\n",
      "452:\tlearn: 0.3476116\ttotal: 1m 28s\tremaining: 1m 46s\n",
      "453:\tlearn: 0.3473470\ttotal: 1m 28s\tremaining: 1m 46s\n",
      "454:\tlearn: 0.3471156\ttotal: 1m 28s\tremaining: 1m 46s\n",
      "455:\tlearn: 0.3468057\ttotal: 1m 29s\tremaining: 1m 46s\n",
      "456:\tlearn: 0.3465724\ttotal: 1m 29s\tremaining: 1m 46s\n",
      "457:\tlearn: 0.3462313\ttotal: 1m 29s\tremaining: 1m 45s\n",
      "458:\tlearn: 0.3458383\ttotal: 1m 29s\tremaining: 1m 45s\n",
      "459:\tlearn: 0.3455152\ttotal: 1m 29s\tremaining: 1m 45s\n",
      "460:\tlearn: 0.3451749\ttotal: 1m 30s\tremaining: 1m 45s\n",
      "461:\tlearn: 0.3447874\ttotal: 1m 30s\tremaining: 1m 45s\n",
      "462:\tlearn: 0.3444057\ttotal: 1m 30s\tremaining: 1m 44s\n",
      "463:\tlearn: 0.3441702\ttotal: 1m 30s\tremaining: 1m 44s\n",
      "464:\tlearn: 0.3437957\ttotal: 1m 30s\tremaining: 1m 44s\n",
      "465:\tlearn: 0.3435199\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "466:\tlearn: 0.3431181\ttotal: 1m 31s\tremaining: 1m 44s\n",
      "467:\tlearn: 0.3428083\ttotal: 1m 31s\tremaining: 1m 43s\n",
      "468:\tlearn: 0.3424546\ttotal: 1m 31s\tremaining: 1m 43s\n",
      "469:\tlearn: 0.3421686\ttotal: 1m 31s\tremaining: 1m 43s\n",
      "470:\tlearn: 0.3417982\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "471:\tlearn: 0.3414388\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "472:\tlearn: 0.3410966\ttotal: 1m 32s\tremaining: 1m 43s\n",
      "473:\tlearn: 0.3406457\ttotal: 1m 32s\tremaining: 1m 42s\n",
      "474:\tlearn: 0.3403348\ttotal: 1m 32s\tremaining: 1m 42s\n",
      "475:\tlearn: 0.3398091\ttotal: 1m 33s\tremaining: 1m 42s\n",
      "476:\tlearn: 0.3394569\ttotal: 1m 33s\tremaining: 1m 42s\n",
      "477:\tlearn: 0.3391051\ttotal: 1m 33s\tremaining: 1m 42s\n",
      "478:\tlearn: 0.3386756\ttotal: 1m 33s\tremaining: 1m 41s\n",
      "479:\tlearn: 0.3383519\ttotal: 1m 33s\tremaining: 1m 41s\n",
      "480:\tlearn: 0.3380294\ttotal: 1m 34s\tremaining: 1m 41s\n",
      "481:\tlearn: 0.3375642\ttotal: 1m 34s\tremaining: 1m 41s\n",
      "482:\tlearn: 0.3372620\ttotal: 1m 34s\tremaining: 1m 41s\n",
      "483:\tlearn: 0.3369996\ttotal: 1m 34s\tremaining: 1m 40s\n",
      "484:\tlearn: 0.3367852\ttotal: 1m 34s\tremaining: 1m 40s\n",
      "485:\tlearn: 0.3364907\ttotal: 1m 35s\tremaining: 1m 40s\n",
      "486:\tlearn: 0.3360383\ttotal: 1m 35s\tremaining: 1m 40s\n",
      "487:\tlearn: 0.3357328\ttotal: 1m 35s\tremaining: 1m 40s\n",
      "488:\tlearn: 0.3354073\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "489:\tlearn: 0.3350541\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "490:\tlearn: 0.3348370\ttotal: 1m 35s\tremaining: 1m 39s\n",
      "491:\tlearn: 0.3343855\ttotal: 1m 36s\tremaining: 1m 39s\n",
      "492:\tlearn: 0.3340278\ttotal: 1m 36s\tremaining: 1m 39s\n",
      "493:\tlearn: 0.3336836\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "494:\tlearn: 0.3333398\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "495:\tlearn: 0.3329210\ttotal: 1m 36s\tremaining: 1m 38s\n",
      "496:\tlearn: 0.3325916\ttotal: 1m 37s\tremaining: 1m 38s\n",
      "497:\tlearn: 0.3321792\ttotal: 1m 37s\tremaining: 1m 38s\n",
      "498:\tlearn: 0.3318187\ttotal: 1m 37s\tremaining: 1m 37s\n",
      "499:\tlearn: 0.3313119\ttotal: 1m 37s\tremaining: 1m 37s\n",
      "500:\tlearn: 0.3309984\ttotal: 1m 37s\tremaining: 1m 37s\n",
      "501:\tlearn: 0.3306819\ttotal: 1m 38s\tremaining: 1m 37s\n",
      "502:\tlearn: 0.3302746\ttotal: 1m 38s\tremaining: 1m 37s\n",
      "503:\tlearn: 0.3298634\ttotal: 1m 38s\tremaining: 1m 36s\n",
      "504:\tlearn: 0.3295583\ttotal: 1m 38s\tremaining: 1m 36s\n",
      "505:\tlearn: 0.3291904\ttotal: 1m 38s\tremaining: 1m 36s\n",
      "506:\tlearn: 0.3287805\ttotal: 1m 39s\tremaining: 1m 36s\n",
      "507:\tlearn: 0.3284270\ttotal: 1m 39s\tremaining: 1m 36s\n",
      "508:\tlearn: 0.3281141\ttotal: 1m 39s\tremaining: 1m 35s\n",
      "509:\tlearn: 0.3278249\ttotal: 1m 39s\tremaining: 1m 35s\n",
      "510:\tlearn: 0.3274899\ttotal: 1m 39s\tremaining: 1m 35s\n",
      "511:\tlearn: 0.3272058\ttotal: 1m 39s\tremaining: 1m 35s\n",
      "512:\tlearn: 0.3268051\ttotal: 1m 40s\tremaining: 1m 35s\n",
      "513:\tlearn: 0.3265186\ttotal: 1m 40s\tremaining: 1m 34s\n",
      "514:\tlearn: 0.3261540\ttotal: 1m 40s\tremaining: 1m 34s\n",
      "515:\tlearn: 0.3257985\ttotal: 1m 40s\tremaining: 1m 34s\n",
      "516:\tlearn: 0.3255089\ttotal: 1m 40s\tremaining: 1m 34s\n",
      "517:\tlearn: 0.3251788\ttotal: 1m 41s\tremaining: 1m 34s\n",
      "518:\tlearn: 0.3247963\ttotal: 1m 41s\tremaining: 1m 33s\n",
      "519:\tlearn: 0.3243752\ttotal: 1m 41s\tremaining: 1m 33s\n",
      "520:\tlearn: 0.3240630\ttotal: 1m 41s\tremaining: 1m 33s\n",
      "521:\tlearn: 0.3237319\ttotal: 1m 41s\tremaining: 1m 33s\n",
      "522:\tlearn: 0.3234115\ttotal: 1m 42s\tremaining: 1m 33s\n",
      "523:\tlearn: 0.3231011\ttotal: 1m 42s\tremaining: 1m 32s\n",
      "524:\tlearn: 0.3228109\ttotal: 1m 42s\tremaining: 1m 32s\n",
      "525:\tlearn: 0.3223939\ttotal: 1m 42s\tremaining: 1m 32s\n",
      "526:\tlearn: 0.3219966\ttotal: 1m 42s\tremaining: 1m 32s\n",
      "527:\tlearn: 0.3216682\ttotal: 1m 42s\tremaining: 1m 32s\n",
      "528:\tlearn: 0.3210843\ttotal: 1m 43s\tremaining: 1m 31s\n",
      "529:\tlearn: 0.3206584\ttotal: 1m 43s\tremaining: 1m 31s\n",
      "530:\tlearn: 0.3203840\ttotal: 1m 43s\tremaining: 1m 31s\n",
      "531:\tlearn: 0.3200525\ttotal: 1m 43s\tremaining: 1m 31s\n",
      "532:\tlearn: 0.3196100\ttotal: 1m 43s\tremaining: 1m 31s\n",
      "533:\tlearn: 0.3193528\ttotal: 1m 44s\tremaining: 1m 30s\n",
      "534:\tlearn: 0.3189813\ttotal: 1m 44s\tremaining: 1m 30s\n",
      "535:\tlearn: 0.3185787\ttotal: 1m 44s\tremaining: 1m 30s\n",
      "536:\tlearn: 0.3182734\ttotal: 1m 44s\tremaining: 1m 30s\n",
      "537:\tlearn: 0.3179916\ttotal: 1m 44s\tremaining: 1m 30s\n",
      "538:\tlearn: 0.3175239\ttotal: 1m 45s\tremaining: 1m 29s\n",
      "539:\tlearn: 0.3172575\ttotal: 1m 45s\tremaining: 1m 29s\n",
      "540:\tlearn: 0.3169357\ttotal: 1m 45s\tremaining: 1m 29s\n",
      "541:\tlearn: 0.3165807\ttotal: 1m 45s\tremaining: 1m 29s\n",
      "542:\tlearn: 0.3161254\ttotal: 1m 45s\tremaining: 1m 29s\n",
      "543:\tlearn: 0.3156567\ttotal: 1m 45s\tremaining: 1m 28s\n",
      "544:\tlearn: 0.3152188\ttotal: 1m 46s\tremaining: 1m 28s\n",
      "545:\tlearn: 0.3147862\ttotal: 1m 46s\tremaining: 1m 28s\n",
      "546:\tlearn: 0.3142932\ttotal: 1m 46s\tremaining: 1m 28s\n",
      "547:\tlearn: 0.3138043\ttotal: 1m 46s\tremaining: 1m 27s\n",
      "548:\tlearn: 0.3135507\ttotal: 1m 46s\tremaining: 1m 27s\n",
      "549:\tlearn: 0.3129295\ttotal: 1m 47s\tremaining: 1m 27s\n",
      "550:\tlearn: 0.3126814\ttotal: 1m 47s\tremaining: 1m 27s\n",
      "551:\tlearn: 0.3122881\ttotal: 1m 47s\tremaining: 1m 27s\n",
      "552:\tlearn: 0.3119245\ttotal: 1m 47s\tremaining: 1m 26s\n",
      "553:\tlearn: 0.3116341\ttotal: 1m 47s\tremaining: 1m 26s\n",
      "554:\tlearn: 0.3111515\ttotal: 1m 47s\tremaining: 1m 26s\n",
      "555:\tlearn: 0.3107843\ttotal: 1m 48s\tremaining: 1m 26s\n",
      "556:\tlearn: 0.3103796\ttotal: 1m 48s\tremaining: 1m 26s\n",
      "557:\tlearn: 0.3100048\ttotal: 1m 48s\tremaining: 1m 25s\n",
      "558:\tlearn: 0.3096603\ttotal: 1m 48s\tremaining: 1m 25s\n",
      "559:\tlearn: 0.3092211\ttotal: 1m 48s\tremaining: 1m 25s\n",
      "560:\tlearn: 0.3086974\ttotal: 1m 49s\tremaining: 1m 25s\n",
      "561:\tlearn: 0.3083364\ttotal: 1m 49s\tremaining: 1m 25s\n",
      "562:\tlearn: 0.3077604\ttotal: 1m 49s\tremaining: 1m 24s\n",
      "563:\tlearn: 0.3074056\ttotal: 1m 49s\tremaining: 1m 24s\n",
      "564:\tlearn: 0.3071022\ttotal: 1m 49s\tremaining: 1m 24s\n",
      "565:\tlearn: 0.3067888\ttotal: 1m 50s\tremaining: 1m 24s\n",
      "566:\tlearn: 0.3063797\ttotal: 1m 50s\tremaining: 1m 24s\n",
      "567:\tlearn: 0.3060522\ttotal: 1m 50s\tremaining: 1m 24s\n",
      "568:\tlearn: 0.3056629\ttotal: 1m 50s\tremaining: 1m 23s\n",
      "569:\tlearn: 0.3054067\ttotal: 1m 50s\tremaining: 1m 23s\n",
      "570:\tlearn: 0.3050619\ttotal: 1m 51s\tremaining: 1m 23s\n",
      "571:\tlearn: 0.3046053\ttotal: 1m 51s\tremaining: 1m 23s\n",
      "572:\tlearn: 0.3042471\ttotal: 1m 51s\tremaining: 1m 23s\n",
      "573:\tlearn: 0.3038502\ttotal: 1m 51s\tremaining: 1m 22s\n",
      "574:\tlearn: 0.3034898\ttotal: 1m 51s\tremaining: 1m 22s\n",
      "575:\tlearn: 0.3030543\ttotal: 1m 51s\tremaining: 1m 22s\n",
      "576:\tlearn: 0.3026698\ttotal: 1m 52s\tremaining: 1m 22s\n",
      "577:\tlearn: 0.3022754\ttotal: 1m 52s\tremaining: 1m 22s\n",
      "578:\tlearn: 0.3019094\ttotal: 1m 52s\tremaining: 1m 21s\n",
      "579:\tlearn: 0.3013850\ttotal: 1m 52s\tremaining: 1m 21s\n",
      "580:\tlearn: 0.3009692\ttotal: 1m 52s\tremaining: 1m 21s\n",
      "581:\tlearn: 0.3006616\ttotal: 1m 53s\tremaining: 1m 21s\n",
      "582:\tlearn: 0.3002421\ttotal: 1m 53s\tremaining: 1m 21s\n",
      "583:\tlearn: 0.2998987\ttotal: 1m 53s\tremaining: 1m 20s\n",
      "584:\tlearn: 0.2995214\ttotal: 1m 53s\tremaining: 1m 20s\n",
      "585:\tlearn: 0.2990462\ttotal: 1m 53s\tremaining: 1m 20s\n",
      "586:\tlearn: 0.2987827\ttotal: 1m 54s\tremaining: 1m 20s\n",
      "587:\tlearn: 0.2981798\ttotal: 1m 54s\tremaining: 1m 20s\n",
      "588:\tlearn: 0.2977571\ttotal: 1m 54s\tremaining: 1m 19s\n",
      "589:\tlearn: 0.2972596\ttotal: 1m 54s\tremaining: 1m 19s\n",
      "590:\tlearn: 0.2968262\ttotal: 1m 54s\tremaining: 1m 19s\n",
      "591:\tlearn: 0.2963561\ttotal: 1m 55s\tremaining: 1m 19s\n",
      "592:\tlearn: 0.2960654\ttotal: 1m 55s\tremaining: 1m 19s\n",
      "593:\tlearn: 0.2956598\ttotal: 1m 55s\tremaining: 1m 18s\n",
      "594:\tlearn: 0.2952647\ttotal: 1m 55s\tremaining: 1m 18s\n",
      "595:\tlearn: 0.2948543\ttotal: 1m 55s\tremaining: 1m 18s\n",
      "596:\tlearn: 0.2943887\ttotal: 1m 56s\tremaining: 1m 18s\n",
      "597:\tlearn: 0.2937894\ttotal: 1m 56s\tremaining: 1m 18s\n",
      "598:\tlearn: 0.2933865\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "599:\tlearn: 0.2929022\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "600:\tlearn: 0.2924567\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "601:\tlearn: 0.2920655\ttotal: 1m 56s\tremaining: 1m 17s\n",
      "602:\tlearn: 0.2916862\ttotal: 1m 57s\tremaining: 1m 17s\n",
      "603:\tlearn: 0.2912419\ttotal: 1m 57s\tremaining: 1m 16s\n",
      "604:\tlearn: 0.2908321\ttotal: 1m 57s\tremaining: 1m 16s\n",
      "605:\tlearn: 0.2903884\ttotal: 1m 57s\tremaining: 1m 16s\n",
      "606:\tlearn: 0.2900955\ttotal: 1m 57s\tremaining: 1m 16s\n",
      "607:\tlearn: 0.2895963\ttotal: 1m 58s\tremaining: 1m 16s\n",
      "608:\tlearn: 0.2892552\ttotal: 1m 58s\tremaining: 1m 15s\n",
      "609:\tlearn: 0.2888795\ttotal: 1m 58s\tremaining: 1m 15s\n",
      "610:\tlearn: 0.2884238\ttotal: 1m 58s\tremaining: 1m 15s\n",
      "611:\tlearn: 0.2881351\ttotal: 1m 58s\tremaining: 1m 15s\n",
      "612:\tlearn: 0.2877671\ttotal: 1m 59s\tremaining: 1m 15s\n",
      "613:\tlearn: 0.2871533\ttotal: 1m 59s\tremaining: 1m 14s\n",
      "614:\tlearn: 0.2868762\ttotal: 1m 59s\tremaining: 1m 14s\n",
      "615:\tlearn: 0.2865495\ttotal: 1m 59s\tremaining: 1m 14s\n",
      "616:\tlearn: 0.2861599\ttotal: 1m 59s\tremaining: 1m 14s\n",
      "617:\tlearn: 0.2857671\ttotal: 2m\tremaining: 1m 14s\n",
      "618:\tlearn: 0.2852580\ttotal: 2m\tremaining: 1m 14s\n",
      "619:\tlearn: 0.2847305\ttotal: 2m\tremaining: 1m 13s\n",
      "620:\tlearn: 0.2843508\ttotal: 2m\tremaining: 1m 13s\n",
      "621:\tlearn: 0.2839912\ttotal: 2m\tremaining: 1m 13s\n",
      "622:\tlearn: 0.2836114\ttotal: 2m 1s\tremaining: 1m 13s\n",
      "623:\tlearn: 0.2831563\ttotal: 2m 1s\tremaining: 1m 13s\n",
      "624:\tlearn: 0.2828369\ttotal: 2m 1s\tremaining: 1m 12s\n",
      "625:\tlearn: 0.2824863\ttotal: 2m 1s\tremaining: 1m 12s\n",
      "626:\tlearn: 0.2820729\ttotal: 2m 1s\tremaining: 1m 12s\n",
      "627:\tlearn: 0.2817953\ttotal: 2m 2s\tremaining: 1m 12s\n",
      "628:\tlearn: 0.2814725\ttotal: 2m 2s\tremaining: 1m 12s\n",
      "629:\tlearn: 0.2811019\ttotal: 2m 2s\tremaining: 1m 11s\n",
      "630:\tlearn: 0.2807416\ttotal: 2m 2s\tremaining: 1m 11s\n",
      "631:\tlearn: 0.2804074\ttotal: 2m 2s\tremaining: 1m 11s\n",
      "632:\tlearn: 0.2800568\ttotal: 2m 3s\tremaining: 1m 11s\n",
      "633:\tlearn: 0.2797166\ttotal: 2m 3s\tremaining: 1m 11s\n",
      "634:\tlearn: 0.2791792\ttotal: 2m 3s\tremaining: 1m 10s\n",
      "635:\tlearn: 0.2788756\ttotal: 2m 3s\tremaining: 1m 10s\n",
      "636:\tlearn: 0.2786165\ttotal: 2m 3s\tremaining: 1m 10s\n",
      "637:\tlearn: 0.2781781\ttotal: 2m 4s\tremaining: 1m 10s\n",
      "638:\tlearn: 0.2777499\ttotal: 2m 4s\tremaining: 1m 10s\n",
      "639:\tlearn: 0.2773136\ttotal: 2m 4s\tremaining: 1m 9s\n",
      "640:\tlearn: 0.2768860\ttotal: 2m 4s\tremaining: 1m 9s\n",
      "641:\tlearn: 0.2765616\ttotal: 2m 4s\tremaining: 1m 9s\n",
      "642:\tlearn: 0.2761294\ttotal: 2m 4s\tremaining: 1m 9s\n",
      "643:\tlearn: 0.2757395\ttotal: 2m 5s\tremaining: 1m 9s\n",
      "644:\tlearn: 0.2754007\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "645:\tlearn: 0.2750810\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "646:\tlearn: 0.2745380\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "647:\tlearn: 0.2739868\ttotal: 2m 5s\tremaining: 1m 8s\n",
      "648:\tlearn: 0.2736788\ttotal: 2m 6s\tremaining: 1m 8s\n",
      "649:\tlearn: 0.2733298\ttotal: 2m 6s\tremaining: 1m 8s\n",
      "650:\tlearn: 0.2728371\ttotal: 2m 6s\tremaining: 1m 7s\n",
      "651:\tlearn: 0.2725206\ttotal: 2m 6s\tremaining: 1m 7s\n",
      "652:\tlearn: 0.2721524\ttotal: 2m 6s\tremaining: 1m 7s\n",
      "653:\tlearn: 0.2717988\ttotal: 2m 7s\tremaining: 1m 7s\n",
      "654:\tlearn: 0.2714591\ttotal: 2m 7s\tremaining: 1m 7s\n",
      "655:\tlearn: 0.2711458\ttotal: 2m 7s\tremaining: 1m 6s\n",
      "656:\tlearn: 0.2708155\ttotal: 2m 7s\tremaining: 1m 6s\n",
      "657:\tlearn: 0.2704002\ttotal: 2m 7s\tremaining: 1m 6s\n",
      "658:\tlearn: 0.2700280\ttotal: 2m 8s\tremaining: 1m 6s\n",
      "659:\tlearn: 0.2696644\ttotal: 2m 8s\tremaining: 1m 6s\n",
      "660:\tlearn: 0.2690561\ttotal: 2m 8s\tremaining: 1m 5s\n",
      "661:\tlearn: 0.2687596\ttotal: 2m 8s\tremaining: 1m 5s\n",
      "662:\tlearn: 0.2682980\ttotal: 2m 8s\tremaining: 1m 5s\n",
      "663:\tlearn: 0.2679586\ttotal: 2m 9s\tremaining: 1m 5s\n",
      "664:\tlearn: 0.2675447\ttotal: 2m 9s\tremaining: 1m 5s\n",
      "665:\tlearn: 0.2672230\ttotal: 2m 9s\tremaining: 1m 4s\n",
      "666:\tlearn: 0.2667883\ttotal: 2m 9s\tremaining: 1m 4s\n",
      "667:\tlearn: 0.2664830\ttotal: 2m 9s\tremaining: 1m 4s\n",
      "668:\tlearn: 0.2660884\ttotal: 2m 10s\tremaining: 1m 4s\n",
      "669:\tlearn: 0.2655964\ttotal: 2m 10s\tremaining: 1m 4s\n",
      "670:\tlearn: 0.2652206\ttotal: 2m 10s\tremaining: 1m 3s\n",
      "671:\tlearn: 0.2648932\ttotal: 2m 10s\tremaining: 1m 3s\n",
      "672:\tlearn: 0.2645579\ttotal: 2m 10s\tremaining: 1m 3s\n",
      "673:\tlearn: 0.2641568\ttotal: 2m 11s\tremaining: 1m 3s\n",
      "674:\tlearn: 0.2637732\ttotal: 2m 11s\tremaining: 1m 3s\n",
      "675:\tlearn: 0.2634444\ttotal: 2m 11s\tremaining: 1m 2s\n",
      "676:\tlearn: 0.2631886\ttotal: 2m 11s\tremaining: 1m 2s\n",
      "677:\tlearn: 0.2626632\ttotal: 2m 11s\tremaining: 1m 2s\n",
      "678:\tlearn: 0.2622328\ttotal: 2m 12s\tremaining: 1m 2s\n",
      "679:\tlearn: 0.2617980\ttotal: 2m 12s\tremaining: 1m 2s\n",
      "680:\tlearn: 0.2614306\ttotal: 2m 12s\tremaining: 1m 2s\n",
      "681:\tlearn: 0.2611829\ttotal: 2m 12s\tremaining: 1m 1s\n",
      "682:\tlearn: 0.2608085\ttotal: 2m 12s\tremaining: 1m 1s\n",
      "683:\tlearn: 0.2604468\ttotal: 2m 12s\tremaining: 1m 1s\n",
      "684:\tlearn: 0.2600771\ttotal: 2m 13s\tremaining: 1m 1s\n",
      "685:\tlearn: 0.2597777\ttotal: 2m 13s\tremaining: 1m 1s\n",
      "686:\tlearn: 0.2593971\ttotal: 2m 13s\tremaining: 1m\n",
      "687:\tlearn: 0.2590362\ttotal: 2m 13s\tremaining: 1m\n",
      "688:\tlearn: 0.2585967\ttotal: 2m 13s\tremaining: 1m\n",
      "689:\tlearn: 0.2581978\ttotal: 2m 14s\tremaining: 1m\n",
      "690:\tlearn: 0.2578445\ttotal: 2m 14s\tremaining: 1m\n",
      "691:\tlearn: 0.2574013\ttotal: 2m 14s\tremaining: 59.9s\n",
      "692:\tlearn: 0.2570648\ttotal: 2m 14s\tremaining: 59.7s\n",
      "693:\tlearn: 0.2567928\ttotal: 2m 14s\tremaining: 59.5s\n",
      "694:\tlearn: 0.2562346\ttotal: 2m 15s\tremaining: 59.3s\n",
      "695:\tlearn: 0.2558836\ttotal: 2m 15s\tremaining: 59.1s\n",
      "696:\tlearn: 0.2554373\ttotal: 2m 15s\tremaining: 58.9s\n",
      "697:\tlearn: 0.2551691\ttotal: 2m 15s\tremaining: 58.7s\n",
      "698:\tlearn: 0.2547446\ttotal: 2m 15s\tremaining: 58.5s\n",
      "699:\tlearn: 0.2543120\ttotal: 2m 16s\tremaining: 58.3s\n",
      "700:\tlearn: 0.2539295\ttotal: 2m 16s\tremaining: 58.1s\n",
      "701:\tlearn: 0.2535421\ttotal: 2m 16s\tremaining: 57.9s\n",
      "702:\tlearn: 0.2532359\ttotal: 2m 16s\tremaining: 57.7s\n",
      "703:\tlearn: 0.2528469\ttotal: 2m 16s\tremaining: 57.5s\n",
      "704:\tlearn: 0.2523536\ttotal: 2m 16s\tremaining: 57.3s\n",
      "705:\tlearn: 0.2520112\ttotal: 2m 17s\tremaining: 57.1s\n",
      "706:\tlearn: 0.2517295\ttotal: 2m 17s\tremaining: 56.9s\n",
      "707:\tlearn: 0.2513556\ttotal: 2m 17s\tremaining: 56.7s\n",
      "708:\tlearn: 0.2508951\ttotal: 2m 17s\tremaining: 56.5s\n",
      "709:\tlearn: 0.2504925\ttotal: 2m 17s\tremaining: 56.3s\n",
      "710:\tlearn: 0.2501046\ttotal: 2m 18s\tremaining: 56.1s\n",
      "711:\tlearn: 0.2498385\ttotal: 2m 18s\tremaining: 56s\n",
      "712:\tlearn: 0.2494774\ttotal: 2m 18s\tremaining: 55.8s\n",
      "713:\tlearn: 0.2491114\ttotal: 2m 18s\tremaining: 55.6s\n",
      "714:\tlearn: 0.2487844\ttotal: 2m 18s\tremaining: 55.4s\n",
      "715:\tlearn: 0.2484847\ttotal: 2m 19s\tremaining: 55.2s\n",
      "716:\tlearn: 0.2481779\ttotal: 2m 19s\tremaining: 55s\n",
      "717:\tlearn: 0.2478462\ttotal: 2m 19s\tremaining: 54.8s\n",
      "718:\tlearn: 0.2474585\ttotal: 2m 19s\tremaining: 54.6s\n",
      "719:\tlearn: 0.2470643\ttotal: 2m 19s\tremaining: 54.4s\n",
      "720:\tlearn: 0.2467096\ttotal: 2m 20s\tremaining: 54.2s\n",
      "721:\tlearn: 0.2464519\ttotal: 2m 20s\tremaining: 54s\n",
      "722:\tlearn: 0.2461493\ttotal: 2m 20s\tremaining: 53.8s\n",
      "723:\tlearn: 0.2458464\ttotal: 2m 20s\tremaining: 53.6s\n",
      "724:\tlearn: 0.2455523\ttotal: 2m 20s\tremaining: 53.4s\n",
      "725:\tlearn: 0.2452174\ttotal: 2m 20s\tremaining: 53.2s\n",
      "726:\tlearn: 0.2448578\ttotal: 2m 21s\tremaining: 53s\n",
      "727:\tlearn: 0.2444566\ttotal: 2m 21s\tremaining: 52.8s\n",
      "728:\tlearn: 0.2440646\ttotal: 2m 21s\tremaining: 52.6s\n",
      "729:\tlearn: 0.2436846\ttotal: 2m 21s\tremaining: 52.4s\n",
      "730:\tlearn: 0.2434073\ttotal: 2m 21s\tremaining: 52.2s\n",
      "731:\tlearn: 0.2429895\ttotal: 2m 22s\tremaining: 52s\n",
      "732:\tlearn: 0.2427008\ttotal: 2m 22s\tremaining: 51.8s\n",
      "733:\tlearn: 0.2423971\ttotal: 2m 22s\tremaining: 51.6s\n",
      "734:\tlearn: 0.2419452\ttotal: 2m 22s\tremaining: 51.4s\n",
      "735:\tlearn: 0.2414534\ttotal: 2m 22s\tremaining: 51.2s\n",
      "736:\tlearn: 0.2410552\ttotal: 2m 22s\tremaining: 51s\n",
      "737:\tlearn: 0.2405468\ttotal: 2m 23s\tremaining: 50.8s\n",
      "738:\tlearn: 0.2401853\ttotal: 2m 23s\tremaining: 50.6s\n",
      "739:\tlearn: 0.2398431\ttotal: 2m 23s\tremaining: 50.4s\n",
      "740:\tlearn: 0.2394713\ttotal: 2m 23s\tremaining: 50.2s\n",
      "741:\tlearn: 0.2391450\ttotal: 2m 23s\tremaining: 50s\n",
      "742:\tlearn: 0.2387491\ttotal: 2m 24s\tremaining: 49.8s\n",
      "743:\tlearn: 0.2384424\ttotal: 2m 24s\tremaining: 49.7s\n",
      "744:\tlearn: 0.2380892\ttotal: 2m 24s\tremaining: 49.5s\n",
      "745:\tlearn: 0.2377539\ttotal: 2m 24s\tremaining: 49.3s\n",
      "746:\tlearn: 0.2373786\ttotal: 2m 24s\tremaining: 49.1s\n",
      "747:\tlearn: 0.2369802\ttotal: 2m 25s\tremaining: 48.9s\n",
      "748:\tlearn: 0.2366086\ttotal: 2m 25s\tremaining: 48.7s\n",
      "749:\tlearn: 0.2363325\ttotal: 2m 25s\tremaining: 48.5s\n",
      "750:\tlearn: 0.2358928\ttotal: 2m 25s\tremaining: 48.3s\n",
      "751:\tlearn: 0.2356110\ttotal: 2m 25s\tremaining: 48.1s\n",
      "752:\tlearn: 0.2352320\ttotal: 2m 25s\tremaining: 47.9s\n",
      "753:\tlearn: 0.2349114\ttotal: 2m 26s\tremaining: 47.7s\n",
      "754:\tlearn: 0.2345404\ttotal: 2m 26s\tremaining: 47.5s\n",
      "755:\tlearn: 0.2340489\ttotal: 2m 26s\tremaining: 47.3s\n",
      "756:\tlearn: 0.2336579\ttotal: 2m 26s\tremaining: 47.1s\n",
      "757:\tlearn: 0.2334288\ttotal: 2m 26s\tremaining: 46.9s\n",
      "758:\tlearn: 0.2330260\ttotal: 2m 27s\tremaining: 46.7s\n",
      "759:\tlearn: 0.2325971\ttotal: 2m 27s\tremaining: 46.5s\n",
      "760:\tlearn: 0.2322534\ttotal: 2m 27s\tremaining: 46.3s\n",
      "761:\tlearn: 0.2318850\ttotal: 2m 27s\tremaining: 46.1s\n",
      "762:\tlearn: 0.2314810\ttotal: 2m 27s\tremaining: 45.9s\n",
      "763:\tlearn: 0.2311728\ttotal: 2m 28s\tremaining: 45.7s\n",
      "764:\tlearn: 0.2308137\ttotal: 2m 28s\tremaining: 45.5s\n",
      "765:\tlearn: 0.2305166\ttotal: 2m 28s\tremaining: 45.3s\n",
      "766:\tlearn: 0.2301423\ttotal: 2m 28s\tremaining: 45.1s\n",
      "767:\tlearn: 0.2297800\ttotal: 2m 28s\tremaining: 44.9s\n",
      "768:\tlearn: 0.2294466\ttotal: 2m 28s\tremaining: 44.7s\n",
      "769:\tlearn: 0.2290701\ttotal: 2m 29s\tremaining: 44.5s\n",
      "770:\tlearn: 0.2286705\ttotal: 2m 29s\tremaining: 44.3s\n",
      "771:\tlearn: 0.2283336\ttotal: 2m 29s\tremaining: 44.1s\n",
      "772:\tlearn: 0.2280787\ttotal: 2m 29s\tremaining: 44s\n",
      "773:\tlearn: 0.2277758\ttotal: 2m 29s\tremaining: 43.8s\n",
      "774:\tlearn: 0.2274531\ttotal: 2m 30s\tremaining: 43.6s\n",
      "775:\tlearn: 0.2270153\ttotal: 2m 30s\tremaining: 43.4s\n",
      "776:\tlearn: 0.2266971\ttotal: 2m 30s\tremaining: 43.2s\n",
      "777:\tlearn: 0.2263566\ttotal: 2m 30s\tremaining: 43s\n",
      "778:\tlearn: 0.2259605\ttotal: 2m 30s\tremaining: 42.8s\n",
      "779:\tlearn: 0.2256397\ttotal: 2m 30s\tremaining: 42.6s\n",
      "780:\tlearn: 0.2251275\ttotal: 2m 31s\tremaining: 42.4s\n",
      "781:\tlearn: 0.2247172\ttotal: 2m 31s\tremaining: 42.2s\n",
      "782:\tlearn: 0.2242496\ttotal: 2m 31s\tremaining: 42s\n",
      "783:\tlearn: 0.2238836\ttotal: 2m 31s\tremaining: 41.8s\n",
      "784:\tlearn: 0.2234373\ttotal: 2m 31s\tremaining: 41.6s\n",
      "785:\tlearn: 0.2231895\ttotal: 2m 32s\tremaining: 41.4s\n",
      "786:\tlearn: 0.2228642\ttotal: 2m 32s\tremaining: 41.2s\n",
      "787:\tlearn: 0.2225557\ttotal: 2m 32s\tremaining: 41s\n",
      "788:\tlearn: 0.2222532\ttotal: 2m 32s\tremaining: 40.8s\n",
      "789:\tlearn: 0.2220136\ttotal: 2m 32s\tremaining: 40.6s\n",
      "790:\tlearn: 0.2216773\ttotal: 2m 32s\tremaining: 40.4s\n",
      "791:\tlearn: 0.2213715\ttotal: 2m 33s\tremaining: 40.2s\n",
      "792:\tlearn: 0.2210176\ttotal: 2m 33s\tremaining: 40s\n",
      "793:\tlearn: 0.2207160\ttotal: 2m 33s\tremaining: 39.8s\n",
      "794:\tlearn: 0.2202894\ttotal: 2m 33s\tremaining: 39.6s\n",
      "795:\tlearn: 0.2200039\ttotal: 2m 33s\tremaining: 39.5s\n",
      "796:\tlearn: 0.2196051\ttotal: 2m 34s\tremaining: 39.3s\n",
      "797:\tlearn: 0.2191678\ttotal: 2m 34s\tremaining: 39.1s\n",
      "798:\tlearn: 0.2188539\ttotal: 2m 34s\tremaining: 38.9s\n",
      "799:\tlearn: 0.2184839\ttotal: 2m 34s\tremaining: 38.7s\n",
      "800:\tlearn: 0.2181450\ttotal: 2m 34s\tremaining: 38.5s\n",
      "801:\tlearn: 0.2178529\ttotal: 2m 35s\tremaining: 38.3s\n",
      "802:\tlearn: 0.2176374\ttotal: 2m 35s\tremaining: 38.1s\n",
      "803:\tlearn: 0.2173451\ttotal: 2m 35s\tremaining: 37.9s\n",
      "804:\tlearn: 0.2170645\ttotal: 2m 35s\tremaining: 37.7s\n",
      "805:\tlearn: 0.2168214\ttotal: 2m 35s\tremaining: 37.5s\n",
      "806:\tlearn: 0.2164957\ttotal: 2m 36s\tremaining: 37.3s\n",
      "807:\tlearn: 0.2162356\ttotal: 2m 36s\tremaining: 37.1s\n",
      "808:\tlearn: 0.2158964\ttotal: 2m 36s\tremaining: 36.9s\n",
      "809:\tlearn: 0.2155668\ttotal: 2m 36s\tremaining: 36.7s\n",
      "810:\tlearn: 0.2152463\ttotal: 2m 36s\tremaining: 36.5s\n",
      "811:\tlearn: 0.2149610\ttotal: 2m 36s\tremaining: 36.3s\n",
      "812:\tlearn: 0.2146258\ttotal: 2m 37s\tremaining: 36.1s\n",
      "813:\tlearn: 0.2143114\ttotal: 2m 37s\tremaining: 35.9s\n",
      "814:\tlearn: 0.2139671\ttotal: 2m 37s\tremaining: 35.7s\n",
      "815:\tlearn: 0.2136878\ttotal: 2m 37s\tremaining: 35.6s\n",
      "816:\tlearn: 0.2133071\ttotal: 2m 37s\tremaining: 35.4s\n",
      "817:\tlearn: 0.2130191\ttotal: 2m 38s\tremaining: 35.2s\n",
      "818:\tlearn: 0.2126776\ttotal: 2m 38s\tremaining: 35s\n",
      "819:\tlearn: 0.2124416\ttotal: 2m 38s\tremaining: 34.8s\n",
      "820:\tlearn: 0.2121241\ttotal: 2m 38s\tremaining: 34.6s\n",
      "821:\tlearn: 0.2117643\ttotal: 2m 38s\tremaining: 34.4s\n",
      "822:\tlearn: 0.2114021\ttotal: 2m 38s\tremaining: 34.2s\n",
      "823:\tlearn: 0.2110328\ttotal: 2m 39s\tremaining: 34s\n",
      "824:\tlearn: 0.2107024\ttotal: 2m 39s\tremaining: 33.8s\n",
      "825:\tlearn: 0.2103473\ttotal: 2m 39s\tremaining: 33.6s\n",
      "826:\tlearn: 0.2099134\ttotal: 2m 39s\tremaining: 33.4s\n",
      "827:\tlearn: 0.2096459\ttotal: 2m 39s\tremaining: 33.2s\n",
      "828:\tlearn: 0.2092753\ttotal: 2m 40s\tremaining: 33s\n",
      "829:\tlearn: 0.2089843\ttotal: 2m 40s\tremaining: 32.8s\n",
      "830:\tlearn: 0.2086442\ttotal: 2m 40s\tremaining: 32.6s\n",
      "831:\tlearn: 0.2083303\ttotal: 2m 40s\tremaining: 32.5s\n",
      "832:\tlearn: 0.2080309\ttotal: 2m 40s\tremaining: 32.3s\n",
      "833:\tlearn: 0.2077095\ttotal: 2m 41s\tremaining: 32.1s\n",
      "834:\tlearn: 0.2074362\ttotal: 2m 41s\tremaining: 31.9s\n",
      "835:\tlearn: 0.2071569\ttotal: 2m 41s\tremaining: 31.7s\n",
      "836:\tlearn: 0.2069576\ttotal: 2m 41s\tremaining: 31.5s\n",
      "837:\tlearn: 0.2066671\ttotal: 2m 41s\tremaining: 31.3s\n",
      "838:\tlearn: 0.2063866\ttotal: 2m 42s\tremaining: 31.1s\n",
      "839:\tlearn: 0.2060809\ttotal: 2m 42s\tremaining: 30.9s\n",
      "840:\tlearn: 0.2056469\ttotal: 2m 42s\tremaining: 30.7s\n",
      "841:\tlearn: 0.2053847\ttotal: 2m 42s\tremaining: 30.5s\n",
      "842:\tlearn: 0.2051165\ttotal: 2m 42s\tremaining: 30.3s\n",
      "843:\tlearn: 0.2047879\ttotal: 2m 43s\tremaining: 30.1s\n",
      "844:\tlearn: 0.2045596\ttotal: 2m 43s\tremaining: 29.9s\n",
      "845:\tlearn: 0.2043217\ttotal: 2m 43s\tremaining: 29.7s\n",
      "846:\tlearn: 0.2040278\ttotal: 2m 43s\tremaining: 29.6s\n",
      "847:\tlearn: 0.2036818\ttotal: 2m 43s\tremaining: 29.4s\n",
      "848:\tlearn: 0.2033711\ttotal: 2m 43s\tremaining: 29.2s\n",
      "849:\tlearn: 0.2031449\ttotal: 2m 44s\tremaining: 29s\n",
      "850:\tlearn: 0.2028353\ttotal: 2m 44s\tremaining: 28.8s\n",
      "851:\tlearn: 0.2025633\ttotal: 2m 44s\tremaining: 28.6s\n",
      "852:\tlearn: 0.2022154\ttotal: 2m 44s\tremaining: 28.4s\n",
      "853:\tlearn: 0.2019682\ttotal: 2m 44s\tremaining: 28.2s\n",
      "854:\tlearn: 0.2017053\ttotal: 2m 45s\tremaining: 28s\n",
      "855:\tlearn: 0.2014438\ttotal: 2m 45s\tremaining: 27.8s\n",
      "856:\tlearn: 0.2010652\ttotal: 2m 45s\tremaining: 27.6s\n",
      "857:\tlearn: 0.2007053\ttotal: 2m 45s\tremaining: 27.4s\n",
      "858:\tlearn: 0.2003896\ttotal: 2m 45s\tremaining: 27.2s\n",
      "859:\tlearn: 0.2001177\ttotal: 2m 46s\tremaining: 27s\n",
      "860:\tlearn: 0.1999329\ttotal: 2m 46s\tremaining: 26.8s\n",
      "861:\tlearn: 0.1995791\ttotal: 2m 46s\tremaining: 26.6s\n",
      "862:\tlearn: 0.1992530\ttotal: 2m 46s\tremaining: 26.5s\n",
      "863:\tlearn: 0.1989865\ttotal: 2m 46s\tremaining: 26.3s\n",
      "864:\tlearn: 0.1986294\ttotal: 2m 47s\tremaining: 26.1s\n",
      "865:\tlearn: 0.1983548\ttotal: 2m 47s\tremaining: 25.9s\n",
      "866:\tlearn: 0.1980404\ttotal: 2m 47s\tremaining: 25.7s\n",
      "867:\tlearn: 0.1976807\ttotal: 2m 47s\tremaining: 25.5s\n",
      "868:\tlearn: 0.1973440\ttotal: 2m 47s\tremaining: 25.3s\n",
      "869:\tlearn: 0.1970319\ttotal: 2m 48s\tremaining: 25.1s\n",
      "870:\tlearn: 0.1967731\ttotal: 2m 48s\tremaining: 24.9s\n",
      "871:\tlearn: 0.1964511\ttotal: 2m 48s\tremaining: 24.7s\n",
      "872:\tlearn: 0.1960647\ttotal: 2m 48s\tremaining: 24.5s\n",
      "873:\tlearn: 0.1958258\ttotal: 2m 48s\tremaining: 24.3s\n",
      "874:\tlearn: 0.1955266\ttotal: 2m 48s\tremaining: 24.1s\n",
      "875:\tlearn: 0.1953291\ttotal: 2m 49s\tremaining: 23.9s\n",
      "876:\tlearn: 0.1950492\ttotal: 2m 49s\tremaining: 23.7s\n",
      "877:\tlearn: 0.1948025\ttotal: 2m 49s\tremaining: 23.6s\n",
      "878:\tlearn: 0.1945752\ttotal: 2m 49s\tremaining: 23.4s\n",
      "879:\tlearn: 0.1942606\ttotal: 2m 49s\tremaining: 23.2s\n",
      "880:\tlearn: 0.1939766\ttotal: 2m 50s\tremaining: 23s\n",
      "881:\tlearn: 0.1936677\ttotal: 2m 50s\tremaining: 22.8s\n",
      "882:\tlearn: 0.1933335\ttotal: 2m 50s\tremaining: 22.6s\n",
      "883:\tlearn: 0.1931449\ttotal: 2m 50s\tremaining: 22.4s\n",
      "884:\tlearn: 0.1928910\ttotal: 2m 50s\tremaining: 22.2s\n",
      "885:\tlearn: 0.1925526\ttotal: 2m 50s\tremaining: 22s\n",
      "886:\tlearn: 0.1922556\ttotal: 2m 51s\tremaining: 21.8s\n",
      "887:\tlearn: 0.1919301\ttotal: 2m 51s\tremaining: 21.6s\n",
      "888:\tlearn: 0.1915961\ttotal: 2m 51s\tremaining: 21.4s\n",
      "889:\tlearn: 0.1913190\ttotal: 2m 51s\tremaining: 21.2s\n",
      "890:\tlearn: 0.1910321\ttotal: 2m 51s\tremaining: 21s\n",
      "891:\tlearn: 0.1907167\ttotal: 2m 52s\tremaining: 20.8s\n",
      "892:\tlearn: 0.1904241\ttotal: 2m 52s\tremaining: 20.6s\n",
      "893:\tlearn: 0.1901855\ttotal: 2m 52s\tremaining: 20.4s\n",
      "894:\tlearn: 0.1899109\ttotal: 2m 52s\tremaining: 20.3s\n",
      "895:\tlearn: 0.1896676\ttotal: 2m 52s\tremaining: 20.1s\n",
      "896:\tlearn: 0.1894031\ttotal: 2m 53s\tremaining: 19.9s\n",
      "897:\tlearn: 0.1891628\ttotal: 2m 53s\tremaining: 19.7s\n",
      "898:\tlearn: 0.1888307\ttotal: 2m 53s\tremaining: 19.5s\n",
      "899:\tlearn: 0.1885431\ttotal: 2m 53s\tremaining: 19.3s\n",
      "900:\tlearn: 0.1882733\ttotal: 2m 53s\tremaining: 19.1s\n",
      "901:\tlearn: 0.1879389\ttotal: 2m 53s\tremaining: 18.9s\n",
      "902:\tlearn: 0.1877004\ttotal: 2m 54s\tremaining: 18.7s\n",
      "903:\tlearn: 0.1874503\ttotal: 2m 54s\tremaining: 18.5s\n",
      "904:\tlearn: 0.1870837\ttotal: 2m 54s\tremaining: 18.3s\n",
      "905:\tlearn: 0.1868644\ttotal: 2m 54s\tremaining: 18.1s\n",
      "906:\tlearn: 0.1865510\ttotal: 2m 54s\tremaining: 17.9s\n",
      "907:\tlearn: 0.1862749\ttotal: 2m 55s\tremaining: 17.7s\n",
      "908:\tlearn: 0.1860521\ttotal: 2m 55s\tremaining: 17.5s\n",
      "909:\tlearn: 0.1857819\ttotal: 2m 55s\tremaining: 17.3s\n",
      "910:\tlearn: 0.1854589\ttotal: 2m 55s\tremaining: 17.2s\n",
      "911:\tlearn: 0.1852233\ttotal: 2m 55s\tremaining: 17s\n",
      "912:\tlearn: 0.1849158\ttotal: 2m 55s\tremaining: 16.8s\n",
      "913:\tlearn: 0.1846806\ttotal: 2m 56s\tremaining: 16.6s\n",
      "914:\tlearn: 0.1844172\ttotal: 2m 56s\tremaining: 16.4s\n",
      "915:\tlearn: 0.1841243\ttotal: 2m 56s\tremaining: 16.2s\n",
      "916:\tlearn: 0.1839174\ttotal: 2m 56s\tremaining: 16s\n",
      "917:\tlearn: 0.1836471\ttotal: 2m 56s\tremaining: 15.8s\n",
      "918:\tlearn: 0.1834314\ttotal: 2m 57s\tremaining: 15.6s\n",
      "919:\tlearn: 0.1832004\ttotal: 2m 57s\tremaining: 15.4s\n",
      "920:\tlearn: 0.1829260\ttotal: 2m 57s\tremaining: 15.2s\n",
      "921:\tlearn: 0.1827042\ttotal: 2m 57s\tremaining: 15s\n",
      "922:\tlearn: 0.1824210\ttotal: 2m 57s\tremaining: 14.8s\n",
      "923:\tlearn: 0.1821653\ttotal: 2m 58s\tremaining: 14.6s\n",
      "924:\tlearn: 0.1818737\ttotal: 2m 58s\tremaining: 14.4s\n",
      "925:\tlearn: 0.1816605\ttotal: 2m 58s\tremaining: 14.3s\n",
      "926:\tlearn: 0.1814140\ttotal: 2m 58s\tremaining: 14.1s\n",
      "927:\tlearn: 0.1810473\ttotal: 2m 58s\tremaining: 13.9s\n",
      "928:\tlearn: 0.1807479\ttotal: 2m 58s\tremaining: 13.7s\n",
      "929:\tlearn: 0.1805705\ttotal: 2m 59s\tremaining: 13.5s\n",
      "930:\tlearn: 0.1803529\ttotal: 2m 59s\tremaining: 13.3s\n",
      "931:\tlearn: 0.1800163\ttotal: 2m 59s\tremaining: 13.1s\n",
      "932:\tlearn: 0.1797359\ttotal: 2m 59s\tremaining: 12.9s\n",
      "933:\tlearn: 0.1793884\ttotal: 2m 59s\tremaining: 12.7s\n",
      "934:\tlearn: 0.1790843\ttotal: 3m\tremaining: 12.5s\n",
      "935:\tlearn: 0.1788440\ttotal: 3m\tremaining: 12.3s\n",
      "936:\tlearn: 0.1786270\ttotal: 3m\tremaining: 12.1s\n",
      "937:\tlearn: 0.1783870\ttotal: 3m\tremaining: 11.9s\n",
      "938:\tlearn: 0.1781320\ttotal: 3m\tremaining: 11.7s\n",
      "939:\tlearn: 0.1779185\ttotal: 3m\tremaining: 11.6s\n",
      "940:\tlearn: 0.1776350\ttotal: 3m 1s\tremaining: 11.4s\n",
      "941:\tlearn: 0.1773945\ttotal: 3m 1s\tremaining: 11.2s\n",
      "942:\tlearn: 0.1771544\ttotal: 3m 1s\tremaining: 11s\n",
      "943:\tlearn: 0.1768975\ttotal: 3m 1s\tremaining: 10.8s\n",
      "944:\tlearn: 0.1766099\ttotal: 3m 1s\tremaining: 10.6s\n",
      "945:\tlearn: 0.1763998\ttotal: 3m 2s\tremaining: 10.4s\n",
      "946:\tlearn: 0.1762191\ttotal: 3m 2s\tremaining: 10.2s\n",
      "947:\tlearn: 0.1759508\ttotal: 3m 2s\tremaining: 10s\n",
      "948:\tlearn: 0.1756995\ttotal: 3m 2s\tremaining: 9.81s\n",
      "949:\tlearn: 0.1755091\ttotal: 3m 2s\tremaining: 9.62s\n",
      "950:\tlearn: 0.1753538\ttotal: 3m 3s\tremaining: 9.43s\n",
      "951:\tlearn: 0.1750715\ttotal: 3m 3s\tremaining: 9.24s\n",
      "952:\tlearn: 0.1748370\ttotal: 3m 3s\tremaining: 9.04s\n",
      "953:\tlearn: 0.1745332\ttotal: 3m 3s\tremaining: 8.85s\n",
      "954:\tlearn: 0.1743001\ttotal: 3m 3s\tremaining: 8.66s\n",
      "955:\tlearn: 0.1739936\ttotal: 3m 3s\tremaining: 8.46s\n",
      "956:\tlearn: 0.1737462\ttotal: 3m 4s\tremaining: 8.27s\n",
      "957:\tlearn: 0.1734953\ttotal: 3m 4s\tremaining: 8.08s\n",
      "958:\tlearn: 0.1732779\ttotal: 3m 4s\tremaining: 7.89s\n",
      "959:\tlearn: 0.1730585\ttotal: 3m 4s\tremaining: 7.69s\n",
      "960:\tlearn: 0.1728526\ttotal: 3m 4s\tremaining: 7.5s\n",
      "961:\tlearn: 0.1725486\ttotal: 3m 5s\tremaining: 7.31s\n",
      "962:\tlearn: 0.1723413\ttotal: 3m 5s\tremaining: 7.12s\n",
      "963:\tlearn: 0.1720850\ttotal: 3m 5s\tremaining: 6.92s\n",
      "964:\tlearn: 0.1718364\ttotal: 3m 5s\tremaining: 6.73s\n",
      "965:\tlearn: 0.1715824\ttotal: 3m 5s\tremaining: 6.54s\n",
      "966:\tlearn: 0.1713386\ttotal: 3m 5s\tremaining: 6.35s\n",
      "967:\tlearn: 0.1710472\ttotal: 3m 6s\tremaining: 6.15s\n",
      "968:\tlearn: 0.1708368\ttotal: 3m 6s\tremaining: 5.96s\n",
      "969:\tlearn: 0.1705441\ttotal: 3m 6s\tremaining: 5.77s\n",
      "970:\tlearn: 0.1703235\ttotal: 3m 6s\tremaining: 5.58s\n",
      "971:\tlearn: 0.1701168\ttotal: 3m 6s\tremaining: 5.38s\n",
      "972:\tlearn: 0.1698360\ttotal: 3m 7s\tremaining: 5.19s\n",
      "973:\tlearn: 0.1696373\ttotal: 3m 7s\tremaining: 5s\n",
      "974:\tlearn: 0.1693978\ttotal: 3m 7s\tremaining: 4.81s\n",
      "975:\tlearn: 0.1692240\ttotal: 3m 7s\tremaining: 4.62s\n",
      "976:\tlearn: 0.1689881\ttotal: 3m 7s\tremaining: 4.42s\n",
      "977:\tlearn: 0.1687564\ttotal: 3m 8s\tremaining: 4.23s\n",
      "978:\tlearn: 0.1684621\ttotal: 3m 8s\tremaining: 4.04s\n",
      "979:\tlearn: 0.1681762\ttotal: 3m 8s\tremaining: 3.85s\n",
      "980:\tlearn: 0.1678717\ttotal: 3m 8s\tremaining: 3.65s\n",
      "981:\tlearn: 0.1676627\ttotal: 3m 8s\tremaining: 3.46s\n",
      "982:\tlearn: 0.1674334\ttotal: 3m 8s\tremaining: 3.27s\n",
      "983:\tlearn: 0.1671757\ttotal: 3m 9s\tremaining: 3.08s\n",
      "984:\tlearn: 0.1669382\ttotal: 3m 9s\tremaining: 2.88s\n",
      "985:\tlearn: 0.1667610\ttotal: 3m 9s\tremaining: 2.69s\n",
      "986:\tlearn: 0.1665529\ttotal: 3m 9s\tremaining: 2.5s\n",
      "987:\tlearn: 0.1662752\ttotal: 3m 9s\tremaining: 2.31s\n",
      "988:\tlearn: 0.1660386\ttotal: 3m 10s\tremaining: 2.11s\n",
      "989:\tlearn: 0.1658327\ttotal: 3m 10s\tremaining: 1.92s\n",
      "990:\tlearn: 0.1656341\ttotal: 3m 10s\tremaining: 1.73s\n",
      "991:\tlearn: 0.1653953\ttotal: 3m 10s\tremaining: 1.54s\n",
      "992:\tlearn: 0.1651652\ttotal: 3m 10s\tremaining: 1.34s\n",
      "993:\tlearn: 0.1648482\ttotal: 3m 11s\tremaining: 1.15s\n",
      "994:\tlearn: 0.1646920\ttotal: 3m 11s\tremaining: 961ms\n",
      "995:\tlearn: 0.1644382\ttotal: 3m 11s\tremaining: 769ms\n",
      "996:\tlearn: 0.1641964\ttotal: 3m 11s\tremaining: 576ms\n",
      "997:\tlearn: 0.1639128\ttotal: 3m 11s\tremaining: 384ms\n",
      "998:\tlearn: 0.1637098\ttotal: 3m 11s\tremaining: 192ms\n",
      "999:\tlearn: 0.1634998\ttotal: 3m 12s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "#CatBoostFeatDrop\n",
    "cols = [\n",
    "    col for col in X_train.columns if not col.startswith(\"wvc\")\n",
    "]\n",
    "CatBoostFeatDrop = CatBoostClassifier(random_state=42)\n",
    "train_and_save_clf(CatBoostFeatDrop, cols, \"CatBoostFeatDrop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним золотой стандарт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard[\n",
    "    [\"Masked_sentence\",\"Right_answer\",\"Wrong_answer\"]\n",
    "].to_csv(\"gold_standard/gold_standard_input.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызовем из командной строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/76 [00:00<?, ?it/s]\n",
      " 11%|█         | 8/76 [00:00<00:00, 77.18it/s]\n",
      " 21%|██        | 16/76 [00:00<00:00, 74.50it/s]\n",
      " 32%|███▏      | 24/76 [00:00<00:00, 74.93it/s]\n",
      " 42%|████▏     | 32/76 [00:00<00:00, 70.69it/s]\n",
      " 53%|█████▎    | 40/76 [00:00<00:00, 72.11it/s]\n",
      " 63%|██████▎   | 48/76 [00:00<00:00, 71.22it/s]\n",
      " 74%|███████▎  | 56/76 [00:00<00:00, 72.55it/s]\n",
      " 84%|████████▍ | 64/76 [00:00<00:00, 72.62it/s]\n",
      " 95%|█████████▍| 72/76 [00:00<00:00, 71.63it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 72.54it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m distractor_generator --filename gold_standard/gold_standard_input.csv --clf_path XGBAllFeats/clf.pkl --cols_path XGBAllFeats/cols.json --output_filename XGBAllFeats/output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/76 [00:00<?, ?it/s]\n",
      " 11%|█         | 8/76 [00:00<00:00, 76.92it/s]\n",
      " 21%|██        | 16/76 [00:00<00:00, 73.20it/s]\n",
      " 32%|███▏      | 24/76 [00:00<00:00, 72.68it/s]\n",
      " 42%|████▏     | 32/76 [00:00<00:00, 73.22it/s]\n",
      " 53%|█████▎    | 40/76 [00:00<00:00, 74.27it/s]\n",
      " 63%|██████▎   | 48/76 [00:00<00:00, 74.91it/s]\n",
      " 74%|███████▎  | 56/76 [00:00<00:00, 75.79it/s]\n",
      " 84%|████████▍ | 64/76 [00:00<00:00, 76.36it/s]\n",
      " 95%|█████████▍| 72/76 [00:00<00:00, 76.92it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 75.52it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m distractor_generator --filename gold_standard/gold_standard_input.csv --clf_path RandomForestFreqsOnly/clf.pkl --cols_path RandomForestFreqsOnly/cols.json --output_filename RandomForestFreqsOnly/output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/76 [00:00<?, ?it/s]\n",
      " 11%|█         | 8/76 [00:00<00:00, 80.00it/s]\n",
      " 21%|██        | 16/76 [00:00<00:00, 77.18it/s]\n",
      " 32%|███▏      | 24/76 [00:00<00:00, 73.20it/s]\n",
      " 42%|████▏     | 32/76 [00:00<00:00, 74.08it/s]\n",
      " 53%|█████▎    | 40/76 [00:00<00:00, 75.59it/s]\n",
      " 63%|██████▎   | 48/76 [00:00<00:00, 76.53it/s]\n",
      " 74%|███████▎  | 56/76 [00:00<00:00, 77.06it/s]\n",
      " 84%|████████▍ | 64/76 [00:00<00:00, 77.49it/s]\n",
      " 95%|█████████▍| 72/76 [00:00<00:00, 77.78it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 76.72it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m distractor_generator --filename gold_standard/gold_standard_input.csv --clf_path CatBoostVecsOnly/clf.pkl --cols_path CatBoostVecsOnly/cols.json --output_filename CatBoostVecsOnly/output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/76 [00:00<?, ?it/s]\n",
      " 11%|█         | 8/76 [00:00<00:00, 74.77it/s]\n",
      " 21%|██        | 16/76 [00:00<00:00, 74.77it/s]\n",
      " 32%|███▏      | 24/76 [00:00<00:00, 76.39it/s]\n",
      " 42%|████▏     | 32/76 [00:00<00:00, 73.27it/s]\n",
      " 53%|█████▎    | 40/76 [00:00<00:00, 74.05it/s]\n",
      " 63%|██████▎   | 48/76 [00:00<00:00, 75.24it/s]\n",
      " 74%|███████▎  | 56/76 [00:00<00:00, 74.64it/s]\n",
      " 84%|████████▍ | 64/76 [00:00<00:00, 70.88it/s]\n",
      " 95%|█████████▍| 72/76 [00:00<00:00, 73.37it/s]\n",
      "100%|██████████| 76/76 [00:01<00:00, 74.03it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m distractor_generator --filename gold_standard/gold_standard_input.csv --clf_path CatBoostFeatDrop/clf.pkl --cols_path CatBoostFeatDrop/cols.json --output_filename CatBoostFeatDrop/output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем сравнивать результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бейзлайн - без классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    76.0\n",
       "mean     20.0\n",
       "std       0.0\n",
       "min      20.0\n",
       "25%      20.0\n",
       "50%      20.0\n",
       "75%      20.0\n",
       "max      20.0\n",
       "Name: variants, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = gold_standard[\"variants\"].apply(len)\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'Baseline (no clf)', 'Appropriate': 0.2710526315789474, 'Too bad': 0.6328947368421053, 'Too good': 0.09605263157894736, 'Appropriate (raw)': 5.421052631578948, 'Too bad (raw)': 12.657894736842104, 'Too good (raw)': 1.9210526315789473}\n"
     ]
    }
   ],
   "source": [
    "share_appr = gold_standard[\"Appropriate\"].apply(len).sum()/gold_standard[\"variants\"].apply(len).sum()\n",
    "share_tg = gold_standard[\"Too good\"].apply(len).sum()/gold_standard[\"variants\"].apply(len).sum()\n",
    "share_tb = gold_standard[\"Too bad\"].apply(len).sum()/gold_standard[\"variants\"].apply(len).sum()\n",
    "row = {\n",
    "    \"method\": \"Baseline (no clf)\",\n",
    "    \"Appropriate\": share_appr,\n",
    "    \"Too bad\": share_tb,\n",
    "    \"Too good\": share_tg,\n",
    "    \"Appropriate (raw)\": gold_standard[\"Appropriate\"].apply(len).mean(),\n",
    "    \"Too bad (raw)\": gold_standard[\"Too bad\"].apply(len).mean(),\n",
    "    \"Too good (raw)\": gold_standard[\"Too good\"].apply(len).mean()\n",
    "}\n",
    "print(row)\n",
    "Table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([153493,  83294,  77723,  74220,  53390, 159237, 136331,  49472,\n",
       "            145079, 112217,  37867,  51411,  73260,  75261, 147504, 139612,\n",
       "             70105, 166891, 163703, 136326, 112011,  73501, 117358, 144854,\n",
       "            162996,  83857,  64171,  72719,  95508,  53309,  51255, 104446,\n",
       "             81630, 119391, 134577,  58525, 104023,  47846, 137728,  87222,\n",
       "            151707, 150385, 101353, 142476,  69141, 164344,  40649,  74783,\n",
       "             86582,  48139,  75292,  53275, 101634,  65050, 102660,  99534,\n",
       "             71531, 158203,  82145, 163852, 160556, 154863,  45811, 148945,\n",
       "            152775,  66435, 134088,  77717, 120146, 104386,  82379, 153675,\n",
       "            138436,  73447, 160682,  42611],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBAllFeats_out = pd.read_csv(\"XGBAllFeats/output.csv\", index_col=\"Unnamed: 0\", sep=';')\n",
    "RandomForestFreqsOnly_out = pd.read_csv(\"RandomForestFreqsOnly/output.csv\", index_col=\"Unnamed: 0\", sep=';')\n",
    "CatBoostVecsOnly_out = pd.read_csv(\"CatBoostVecsOnly/output.csv\", index_col=\"Unnamed: 0\", sep=';')\n",
    "CatBoostFeatDrop_out = pd.read_csv(\"CatBoostFeatDrop/output.csv\", index_col=\"Unnamed: 0\", sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(XGBAllFeats_out.index) == list(gold_standard.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(RandomForestFreqsOnly_out.index) == list(gold_standard.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(CatBoostVecsOnly_out.index) == list(gold_standard.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(CatBoostFeatDrop_out.index) == list(gold_standard.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Masked_sentence</th>\n",
       "      <th>Right_answer</th>\n",
       "      <th>Wrong_answer</th>\n",
       "      <th>variants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153493</th>\n",
       "      <td>The amount of people who has no occupation in...</td>\n",
       "      <td>stable</td>\n",
       "      <td>the same</td>\n",
       "      <td>['state', 'dependable', 'consistent', 'steady'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83294</th>\n",
       "      <td>Some politicians have come up with an idea to ...</td>\n",
       "      <td>disadvantages</td>\n",
       "      <td>backwards</td>\n",
       "      <td>['cons', 'limitations', 'shortcomings', 'weakn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77723</th>\n",
       "      <td>As for disadvantages, global warming and air ...</td>\n",
       "      <td>number</td>\n",
       "      <td>amount</td>\n",
       "      <td>['amount', 'quantity', 'level', 'part', 'value...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74220</th>\n",
       "      <td>It is slightly below 30°C in Yakutsk and 30°C...</td>\n",
       "      <td>trend</td>\n",
       "      <td>tendency</td>\n",
       "      <td>['tendency', 'consistency', 'craze', 'fad', 'u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53390</th>\n",
       "      <td>The number of men who are aged between 15 and...</td>\n",
       "      <td>number</td>\n",
       "      <td>part</td>\n",
       "      <td>['amount', 'quantity', 'level', 'value', 'coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153675</th>\n",
       "      <td>So, the national population usually becomes p...</td>\n",
       "      <td>take</td>\n",
       "      <td>move</td>\n",
       "      <td>['make', 'go', 'get', 'taking', 'relinquish', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138436</th>\n",
       "      <td>7 minutes). Just in one case women's group of ...</td>\n",
       "      <td>doing</td>\n",
       "      <td>of goind</td>\n",
       "      <td>['making', 'for', 'getting', 'pursuing', 'acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73447</th>\n",
       "      <td>The decreasing unemployment in Latin America ...</td>\n",
       "      <td>acute</td>\n",
       "      <td>sharp</td>\n",
       "      <td>['sharp', 'chronic', 'symptomatic', 'febrile',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160682</th>\n",
       "      <td>Low discipline in schools tends to result in ...</td>\n",
       "      <td>improving</td>\n",
       "      <td>repairing</td>\n",
       "      <td>['repairing', 'enhancing', 'boosting', 'mainta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42611</th>\n",
       "      <td>Nowadays many people talk about food productio...</td>\n",
       "      <td>food</td>\n",
       "      <td>meal</td>\n",
       "      <td>['goal', 'seafood', 'nutrition', 'beverage', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Masked_sentence   Right_answer  \\\n",
       "153493   The amount of people who has no occupation in...         stable   \n",
       "83294   Some politicians have come up with an idea to ...  disadvantages   \n",
       "77723    As for disadvantages, global warming and air ...         number   \n",
       "74220    It is slightly below 30°C in Yakutsk and 30°C...          trend   \n",
       "53390    The number of men who are aged between 15 and...         number   \n",
       "...                                                   ...            ...   \n",
       "153675   So, the national population usually becomes p...           take   \n",
       "138436  7 minutes). Just in one case women's group of ...          doing   \n",
       "73447    The decreasing unemployment in Latin America ...          acute   \n",
       "160682   Low discipline in schools tends to result in ...      improving   \n",
       "42611   Nowadays many people talk about food productio...           food   \n",
       "\n",
       "       Wrong_answer                                           variants  \n",
       "153493     the same  ['state', 'dependable', 'consistent', 'steady'...  \n",
       "83294     backwards  ['cons', 'limitations', 'shortcomings', 'weakn...  \n",
       "77723        amount  ['amount', 'quantity', 'level', 'part', 'value...  \n",
       "74220      tendency  ['tendency', 'consistency', 'craze', 'fad', 'u...  \n",
       "53390          part  ['amount', 'quantity', 'level', 'value', 'coun...  \n",
       "...             ...                                                ...  \n",
       "153675         move  ['make', 'go', 'get', 'taking', 'relinquish', ...  \n",
       "138436     of goind  ['making', 'for', 'getting', 'pursuing', 'acco...  \n",
       "73447         sharp  ['sharp', 'chronic', 'symptomatic', 'febrile',...  \n",
       "160682    repairing  ['repairing', 'enhancing', 'boosting', 'mainta...  \n",
       "42611          meal  ['goal', 'seafood', 'nutrition', 'beverage', '...  \n",
       "\n",
       "[76 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBAllFeats_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_output(\n",
    "    output_df: pd.DataFrame,\n",
    "    method: str\n",
    "):\n",
    "    output_df[\"variants\"] = output_df[\"variants\"].apply(literal_eval)\n",
    "    s = output_df[\"variants\"].apply(len).sum()\n",
    "    appr, tg, tb = 0, 0, 0\n",
    "    for idx in gold_standard.index:\n",
    "        appr += len(set(output_df.loc[idx][\"variants\"]) & set(gold_standard.loc[idx][\"Appropriate\"]))\n",
    "        tg += len(set(output_df.loc[idx][\"variants\"]) & set(gold_standard.loc[idx][\"Too good\"]))\n",
    "        tb += len(set(output_df.loc[idx][\"variants\"]) & set(gold_standard.loc[idx][\"Too bad\"]))\n",
    "    \n",
    "    return {\n",
    "        \"method\": method,\n",
    "        \"Appropriate\": appr/s,\n",
    "        \"Too bad\": tb/s,\n",
    "        \"Too good\": tg/s\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name in zip(\n",
    "    [XGBAllFeats_out, RandomForestFreqsOnly_out, CatBoostVecsOnly_out, CatBoostFeatDrop_out],\n",
    "    [\"XGBAllFeats_out\", \"RandomForestFreqsOnly_out\", \"CatBoostVecsOnly_out\", \"CatBoostFeatDrop_out\"]\n",
    "):\n",
    "    Table.append(estimate_output(df,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table = pd.DataFrame(Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>Appropriate</th>\n",
       "      <th>Too bad</th>\n",
       "      <th>Too good</th>\n",
       "      <th>Appropriate (raw)</th>\n",
       "      <th>Too bad (raw)</th>\n",
       "      <th>Too good (raw)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>0.292776</td>\n",
       "      <td>0.601711</td>\n",
       "      <td>0.103612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestFreqsOnly_out</td>\n",
       "      <td>0.291705</td>\n",
       "      <td>0.607110</td>\n",
       "      <td>0.097539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoostFeatDrop_out</td>\n",
       "      <td>0.289116</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.106293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>0.275404</td>\n",
       "      <td>0.614435</td>\n",
       "      <td>0.108262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>5.421053</td>\n",
       "      <td>12.657895</td>\n",
       "      <td>1.921053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      method  Appropriate   Too bad  Too good  \\\n",
       "1            XGBAllFeats_out     0.292776  0.601711  0.103612   \n",
       "2  RandomForestFreqsOnly_out     0.291705  0.607110  0.097539   \n",
       "4       CatBoostFeatDrop_out     0.289116  0.601190  0.106293   \n",
       "3       CatBoostVecsOnly_out     0.275404  0.614435  0.108262   \n",
       "0          Baseline (no clf)     0.271053  0.632895  0.096053   \n",
       "\n",
       "   Appropriate (raw)  Too bad (raw)  Too good (raw)  \n",
       "1                NaN            NaN             NaN  \n",
       "2                NaN            NaN             NaN  \n",
       "4                NaN            NaN             NaN  \n",
       "3                NaN            NaN             NaN  \n",
       "0           5.421053      12.657895        1.921053  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table.sort_values(by=[\"Appropriate\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table.to_excel(\"data/gold_standard_performance.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё - можно попробовать поиграться с N - брать от 3 до 20 - можно заново не проводить классификацию, просто брать дистракторы в порядке старшинства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153493    [state, dependable, consistent, steady, prospe...\n",
       "83294     [cons, limitations, shortcomings, weaknesses, ...\n",
       "77723     [amount, quantity, level, part, value, member,...\n",
       "74220     [tendency, consistency, phenomenon, resurgence...\n",
       "53390     [amount, quantity, level, value, member, count...\n",
       "                                ...                        \n",
       "153675    [make, drink, get, move, share, go, taking, gi...\n",
       "138436    [making, for, getting, pursuing, accomplishing...\n",
       "73447     [sharp, chronic, symptomatic, febrile, respira...\n",
       "160682    [repairing, enhancing, reducing, strengthening...\n",
       "42611     [meal, goal, seafood, meat, nutrition, beverag...\n",
       "Name: variants, Length: 76, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard[\"variants\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ouput_with_N(\n",
    "    output_df: pd.DataFrame,\n",
    "    N: int,\n",
    "    method: str\n",
    "):\n",
    "    gs1 = gold_standard.copy()\n",
    "    df1 = output_df.copy()\n",
    "\n",
    "    gs1[\"variants\"] = gs1[\"variants\"].apply(lambda x: x[:N])\n",
    "    gs1[\"Appropriate\"] = gs1.apply(\n",
    "        lambda x: [i for i in x[\"Appropriate\"] if i in x[\"variants\"]],\n",
    "        axis=1\n",
    "    )\n",
    "    gs1[\"Too good\"] = gs1.apply(\n",
    "        lambda x: [i for i in x[\"Too good\"] if i in x[\"variants\"]],\n",
    "        axis=1\n",
    "    )\n",
    "    gs1[\"Too bad\"] = gs1.apply(\n",
    "        lambda x: [i for i in x[\"Too bad\"] if i in x[\"variants\"]],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df1[\"variants\"] = df1.apply(\n",
    "        lambda x: [i for i in x[\"variants\"] if i in gs1.loc[x.name][\"variants\"]],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    s = df1[\"variants\"].apply(len).sum()\n",
    "    c = df1[\"variants\"].apply(len).mean()\n",
    "    appr, tg, tb = 0, 0, 0\n",
    "    appr_sent = []\n",
    "    tg_sent = []\n",
    "    tb_sent = []\n",
    "    appr_raw, tg_raw, tb_raw = [], [], []\n",
    "\n",
    "    for idx in gs1.index:\n",
    "        appr_i = len(set(df1.loc[idx][\"variants\"]) & set(gs1.loc[idx][\"Appropriate\"]))\n",
    "        tg_i = len(set(df1.loc[idx][\"variants\"]) & set(gs1.loc[idx][\"Too good\"]))\n",
    "        tb_i = len(set(df1.loc[idx][\"variants\"]) & set(gs1.loc[idx][\"Too bad\"]))\n",
    "\n",
    "        appr += appr_i\n",
    "        tg += tg_i\n",
    "        tb += tb_i\n",
    "\n",
    "        if df1.loc[idx][\"variants\"]:\n",
    "            appr_sent.append(appr_i/len(df1.loc[idx][\"variants\"]))\n",
    "            tg_sent.append(tg_i/len(df1.loc[idx][\"variants\"]))\n",
    "            tb_sent.append(tb_i/len(df1.loc[idx][\"variants\"]))\n",
    "\n",
    "            appr_raw.append(appr_i)\n",
    "            tg_raw.append(tg_i)\n",
    "            tb_raw.append(tb_i)\n",
    "        else:\n",
    "            appr_sent.append(0)\n",
    "            tg_sent.append(0)\n",
    "            tb_sent.append(0)\n",
    "\n",
    "            appr_raw.append(0)\n",
    "            tg_raw.append(0)\n",
    "            tb_raw.append(0)\n",
    "    \n",
    "    return {\n",
    "        \"method\": method,\n",
    "        \"N\": N,\n",
    "        \"Appropriate (whole)\": appr/s,\n",
    "        \"Too bad (whole)\": tb/s,\n",
    "        \"Too good (whole)\": tg/s,\n",
    "        \"Appropriate (by sent)\": pd.Series(appr_sent).mean(),\n",
    "        \"Too bad (by sent)\": pd.Series(tb_sent).mean(),\n",
    "        \"Too good (by sent)\": pd.Series(tg_sent).mean(),\n",
    "        \"Appropriate (raw mean by sent)\":  pd.Series(appr_raw).mean(),\n",
    "        \"Too bad (raw mean by sent)\": pd.Series(tb_raw).mean(),\n",
    "        \"Too good (raw mean by sent):\": pd.Series(tg_raw).mean(),\n",
    "        \"N distractors\": c\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем перебирать N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14616\\41210362.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for N in tqdm_notebook(range(3, 21), total=18):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e22f270f1564390b0e0fb5dda2ff277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Table1 = []\n",
    "\n",
    "for N in tqdm_notebook(range(3, 21), total=18):\n",
    "    for df, name in zip(\n",
    "        [gold_standard, XGBAllFeats_out, RandomForestFreqsOnly_out, CatBoostVecsOnly_out, CatBoostFeatDrop_out],\n",
    "        [\"Baseline (no clf)\", \"XGBAllFeats_out\", \"RandomForestFreqsOnly_out\", \"CatBoostVecsOnly_out\", \"CatBoostFeatDrop_out\"]\n",
    "    ):\n",
    "        Table1.append(\n",
    "            estimate_ouput_with_N(df, N, name)\n",
    "        )\n",
    "\n",
    "Table1 = pd.DataFrame(Table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>N</th>\n",
       "      <th>Appropriate (whole)</th>\n",
       "      <th>Too bad (whole)</th>\n",
       "      <th>Too good (whole)</th>\n",
       "      <th>Appropriate (by sent)</th>\n",
       "      <th>Too bad (by sent)</th>\n",
       "      <th>Too good (by sent)</th>\n",
       "      <th>Appropriate (raw mean by sent)</th>\n",
       "      <th>Too bad (raw mean by sent)</th>\n",
       "      <th>Too good (raw mean by sent):</th>\n",
       "      <th>N distractors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>3</td>\n",
       "      <td>0.581152</td>\n",
       "      <td>0.293194</td>\n",
       "      <td>0.125654</td>\n",
       "      <td>0.598684</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.460526</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>2.513158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>3</td>\n",
       "      <td>0.578035</td>\n",
       "      <td>0.294798</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.574561</td>\n",
       "      <td>0.271930</td>\n",
       "      <td>0.127193</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>2.276316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoostFeatDrop_out</td>\n",
       "      <td>3</td>\n",
       "      <td>0.546798</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.550439</td>\n",
       "      <td>0.304825</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>1.460526</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>2.671053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>4</td>\n",
       "      <td>0.542986</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>0.536184</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>1.578947</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>2.907895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>4</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.326754</td>\n",
       "      <td>0.111842</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>3.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>18</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>5.171053</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.828947</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>19</td>\n",
       "      <td>0.284148</td>\n",
       "      <td>0.605184</td>\n",
       "      <td>0.110668</td>\n",
       "      <td>0.329655</td>\n",
       "      <td>0.574817</td>\n",
       "      <td>0.095529</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>7.986842</td>\n",
       "      <td>1.460526</td>\n",
       "      <td>13.197368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>19</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>11.802632</td>\n",
       "      <td>1.881579</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>20</td>\n",
       "      <td>0.275928</td>\n",
       "      <td>0.615604</td>\n",
       "      <td>0.108468</td>\n",
       "      <td>0.321604</td>\n",
       "      <td>0.584302</td>\n",
       "      <td>0.094094</td>\n",
       "      <td>3.815789</td>\n",
       "      <td>8.513158</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>13.828947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>5.421053</td>\n",
       "      <td>12.657895</td>\n",
       "      <td>1.921053</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  method   N  Appropriate (whole)  Too bad (whole)  \\\n",
       "1        XGBAllFeats_out   3             0.581152         0.293194   \n",
       "3   CatBoostVecsOnly_out   3             0.578035         0.294798   \n",
       "4   CatBoostFeatDrop_out   3             0.546798         0.315271   \n",
       "8   CatBoostVecsOnly_out   4             0.542986         0.325792   \n",
       "6        XGBAllFeats_out   4             0.536585         0.341463   \n",
       "..                   ...  ..                  ...              ...   \n",
       "75     Baseline (no clf)  18             0.287281         0.611111   \n",
       "83  CatBoostVecsOnly_out  19             0.284148         0.605184   \n",
       "80     Baseline (no clf)  19             0.279778         0.621191   \n",
       "88  CatBoostVecsOnly_out  20             0.275928         0.615604   \n",
       "85     Baseline (no clf)  20             0.271053         0.632895   \n",
       "\n",
       "    Too good (whole)  Appropriate (by sent)  Too bad (by sent)  \\\n",
       "1           0.125654               0.598684           0.276316   \n",
       "3           0.127168               0.574561           0.271930   \n",
       "4           0.137931               0.550439           0.304825   \n",
       "8           0.131222               0.536184           0.314693   \n",
       "6           0.121951               0.561404           0.326754   \n",
       "..               ...                    ...                ...   \n",
       "75          0.101608               0.287281           0.611111   \n",
       "83          0.110668               0.329655           0.574817   \n",
       "80          0.099030               0.279778           0.621191   \n",
       "88          0.108468               0.321604           0.584302   \n",
       "85          0.096053               0.271053           0.632895   \n",
       "\n",
       "    Too good (by sent)  Appropriate (raw mean by sent)  \\\n",
       "1             0.125000                        1.460526   \n",
       "3             0.127193                        1.315789   \n",
       "4             0.144737                        1.460526   \n",
       "8             0.122807                        1.578947   \n",
       "6             0.111842                        1.736842   \n",
       "..                 ...                             ...   \n",
       "75            0.101608                        5.171053   \n",
       "83            0.095529                        3.750000   \n",
       "80            0.099030                        5.315789   \n",
       "88            0.094094                        3.815789   \n",
       "85            0.096053                        5.421053   \n",
       "\n",
       "    Too bad (raw mean by sent)  Too good (raw mean by sent):  N distractors  \n",
       "1                     0.736842                      0.315789       2.513158  \n",
       "3                     0.671053                      0.289474       2.276316  \n",
       "4                     0.842105                      0.368421       2.671053  \n",
       "8                     0.947368                      0.381579       2.907895  \n",
       "6                     1.105263                      0.394737       3.236842  \n",
       "..                         ...                           ...            ...  \n",
       "75                   11.000000                      1.828947      18.000000  \n",
       "83                    7.986842                      1.460526      13.197368  \n",
       "80                   11.802632                      1.881579      19.000000  \n",
       "88                    8.513158                      1.500000      13.828947  \n",
       "85                   12.657895                      1.921053      20.000000  \n",
       "\n",
       "[90 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table1.sort_values(by=[\"Appropriate (whole)\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>N</th>\n",
       "      <th>Appropriate (whole)</th>\n",
       "      <th>Too bad (whole)</th>\n",
       "      <th>Too good (whole)</th>\n",
       "      <th>Appropriate (by sent)</th>\n",
       "      <th>Too bad (by sent)</th>\n",
       "      <th>Too good (by sent)</th>\n",
       "      <th>Appropriate (raw mean by sent)</th>\n",
       "      <th>Too bad (raw mean by sent)</th>\n",
       "      <th>Too good (raw mean by sent):</th>\n",
       "      <th>N distractors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>3</td>\n",
       "      <td>0.581152</td>\n",
       "      <td>0.293194</td>\n",
       "      <td>0.125654</td>\n",
       "      <td>0.598684</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.460526</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>2.513158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>3</td>\n",
       "      <td>0.578035</td>\n",
       "      <td>0.294798</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.574561</td>\n",
       "      <td>0.271930</td>\n",
       "      <td>0.127193</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>2.276316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>4</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.326754</td>\n",
       "      <td>0.111842</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>3.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoostFeatDrop_out</td>\n",
       "      <td>3</td>\n",
       "      <td>0.546798</td>\n",
       "      <td>0.315271</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.550439</td>\n",
       "      <td>0.304825</td>\n",
       "      <td>0.144737</td>\n",
       "      <td>1.460526</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>2.671053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoostVecsOnly_out</td>\n",
       "      <td>4</td>\n",
       "      <td>0.542986</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>0.536184</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>1.578947</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>2.907895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>17</td>\n",
       "      <td>0.298762</td>\n",
       "      <td>0.597523</td>\n",
       "      <td>0.103715</td>\n",
       "      <td>0.298762</td>\n",
       "      <td>0.597523</td>\n",
       "      <td>0.103715</td>\n",
       "      <td>5.078947</td>\n",
       "      <td>10.157895</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>RandomForestFreqsOnly_out</td>\n",
       "      <td>20</td>\n",
       "      <td>0.292772</td>\n",
       "      <td>0.609332</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.296627</td>\n",
       "      <td>0.610794</td>\n",
       "      <td>0.092579</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>8.763158</td>\n",
       "      <td>1.407895</td>\n",
       "      <td>14.381579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>18</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>5.171053</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.828947</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>19</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>11.802632</td>\n",
       "      <td>1.881579</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>5.421053</td>\n",
       "      <td>12.657895</td>\n",
       "      <td>1.921053</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       method   N  Appropriate (whole)  Too bad (whole)  \\\n",
       "1             XGBAllFeats_out   3             0.581152         0.293194   \n",
       "3        CatBoostVecsOnly_out   3             0.578035         0.294798   \n",
       "6             XGBAllFeats_out   4             0.536585         0.341463   \n",
       "4        CatBoostFeatDrop_out   3             0.546798         0.315271   \n",
       "8        CatBoostVecsOnly_out   4             0.542986         0.325792   \n",
       "..                        ...  ..                  ...              ...   \n",
       "70          Baseline (no clf)  17             0.298762         0.597523   \n",
       "87  RandomForestFreqsOnly_out  20             0.292772         0.609332   \n",
       "75          Baseline (no clf)  18             0.287281         0.611111   \n",
       "80          Baseline (no clf)  19             0.279778         0.621191   \n",
       "85          Baseline (no clf)  20             0.271053         0.632895   \n",
       "\n",
       "    Too good (whole)  Appropriate (by sent)  Too bad (by sent)  \\\n",
       "1           0.125654               0.598684           0.276316   \n",
       "3           0.127168               0.574561           0.271930   \n",
       "6           0.121951               0.561404           0.326754   \n",
       "4           0.137931               0.550439           0.304825   \n",
       "8           0.131222               0.536184           0.314693   \n",
       "..               ...                    ...                ...   \n",
       "70          0.103715               0.298762           0.597523   \n",
       "87          0.097896               0.296627           0.610794   \n",
       "75          0.101608               0.287281           0.611111   \n",
       "80          0.099030               0.279778           0.621191   \n",
       "85          0.096053               0.271053           0.632895   \n",
       "\n",
       "    Too good (by sent)  Appropriate (raw mean by sent)  \\\n",
       "1             0.125000                        1.460526   \n",
       "3             0.127193                        1.315789   \n",
       "6             0.111842                        1.736842   \n",
       "4             0.144737                        1.460526   \n",
       "8             0.122807                        1.578947   \n",
       "..                 ...                             ...   \n",
       "70            0.103715                        5.078947   \n",
       "87            0.092579                        4.210526   \n",
       "75            0.101608                        5.171053   \n",
       "80            0.099030                        5.315789   \n",
       "85            0.096053                        5.421053   \n",
       "\n",
       "    Too bad (raw mean by sent)  Too good (raw mean by sent):  N distractors  \n",
       "1                     0.736842                      0.315789       2.513158  \n",
       "3                     0.671053                      0.289474       2.276316  \n",
       "6                     1.105263                      0.394737       3.236842  \n",
       "4                     0.842105                      0.368421       2.671053  \n",
       "8                     0.947368                      0.381579       2.907895  \n",
       "..                         ...                           ...            ...  \n",
       "70                   10.157895                      1.763158      17.000000  \n",
       "87                    8.763158                      1.407895      14.381579  \n",
       "75                   11.000000                      1.828947      18.000000  \n",
       "80                   11.802632                      1.881579      19.000000  \n",
       "85                   12.657895                      1.921053      20.000000  \n",
       "\n",
       "[90 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table1.sort_values(by=[\"Appropriate (by sent)\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>N</th>\n",
       "      <th>Appropriate (whole)</th>\n",
       "      <th>Too bad (whole)</th>\n",
       "      <th>Too good (whole)</th>\n",
       "      <th>Appropriate (by sent)</th>\n",
       "      <th>Too bad (by sent)</th>\n",
       "      <th>Too good (by sent)</th>\n",
       "      <th>Appropriate (raw mean by sent)</th>\n",
       "      <th>Too bad (raw mean by sent)</th>\n",
       "      <th>Too good (raw mean by sent):</th>\n",
       "      <th>N distractors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>4</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.326754</td>\n",
       "      <td>0.111842</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>3.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>5</td>\n",
       "      <td>0.496622</td>\n",
       "      <td>0.385135</td>\n",
       "      <td>0.118243</td>\n",
       "      <td>0.535526</td>\n",
       "      <td>0.357675</td>\n",
       "      <td>0.106798</td>\n",
       "      <td>1.934211</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>3.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CatBoostFeatDrop_out</td>\n",
       "      <td>4</td>\n",
       "      <td>0.515267</td>\n",
       "      <td>0.347328</td>\n",
       "      <td>0.137405</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.140351</td>\n",
       "      <td>1.776316</td>\n",
       "      <td>1.197368</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>3.447368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>6</td>\n",
       "      <td>0.468023</td>\n",
       "      <td>0.409884</td>\n",
       "      <td>0.122093</td>\n",
       "      <td>0.510746</td>\n",
       "      <td>0.378509</td>\n",
       "      <td>0.110746</td>\n",
       "      <td>2.118421</td>\n",
       "      <td>1.855263</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>4.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>XGBAllFeats_out</td>\n",
       "      <td>7</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.420918</td>\n",
       "      <td>0.119898</td>\n",
       "      <td>0.502068</td>\n",
       "      <td>0.390257</td>\n",
       "      <td>0.107675</td>\n",
       "      <td>2.368421</td>\n",
       "      <td>2.171053</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>5.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>17</td>\n",
       "      <td>0.298762</td>\n",
       "      <td>0.597523</td>\n",
       "      <td>0.103715</td>\n",
       "      <td>0.298762</td>\n",
       "      <td>0.597523</td>\n",
       "      <td>0.103715</td>\n",
       "      <td>5.078947</td>\n",
       "      <td>10.157895</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>RandomForestFreqsOnly_out</td>\n",
       "      <td>20</td>\n",
       "      <td>0.292772</td>\n",
       "      <td>0.609332</td>\n",
       "      <td>0.097896</td>\n",
       "      <td>0.296627</td>\n",
       "      <td>0.610794</td>\n",
       "      <td>0.092579</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>8.763158</td>\n",
       "      <td>1.407895</td>\n",
       "      <td>14.381579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>18</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>5.171053</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.828947</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>19</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>0.279778</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.099030</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>11.802632</td>\n",
       "      <td>1.881579</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Baseline (no clf)</td>\n",
       "      <td>20</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>0.271053</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>5.421053</td>\n",
       "      <td>12.657895</td>\n",
       "      <td>1.921053</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       method   N  Appropriate (whole)  Too bad (whole)  \\\n",
       "6             XGBAllFeats_out   4             0.536585         0.341463   \n",
       "11            XGBAllFeats_out   5             0.496622         0.385135   \n",
       "9        CatBoostFeatDrop_out   4             0.515267         0.347328   \n",
       "16            XGBAllFeats_out   6             0.468023         0.409884   \n",
       "21            XGBAllFeats_out   7             0.459184         0.420918   \n",
       "..                        ...  ..                  ...              ...   \n",
       "70          Baseline (no clf)  17             0.298762         0.597523   \n",
       "87  RandomForestFreqsOnly_out  20             0.292772         0.609332   \n",
       "75          Baseline (no clf)  18             0.287281         0.611111   \n",
       "80          Baseline (no clf)  19             0.279778         0.621191   \n",
       "85          Baseline (no clf)  20             0.271053         0.632895   \n",
       "\n",
       "    Too good (whole)  Appropriate (by sent)  Too bad (by sent)  \\\n",
       "6           0.121951               0.561404           0.326754   \n",
       "11          0.118243               0.535526           0.357675   \n",
       "9           0.137405               0.526316           0.333333   \n",
       "16          0.122093               0.510746           0.378509   \n",
       "21          0.119898               0.502068           0.390257   \n",
       "..               ...                    ...                ...   \n",
       "70          0.103715               0.298762           0.597523   \n",
       "87          0.097896               0.296627           0.610794   \n",
       "75          0.101608               0.287281           0.611111   \n",
       "80          0.099030               0.279778           0.621191   \n",
       "85          0.096053               0.271053           0.632895   \n",
       "\n",
       "    Too good (by sent)  Appropriate (raw mean by sent)  \\\n",
       "6             0.111842                        1.736842   \n",
       "11            0.106798                        1.934211   \n",
       "9             0.140351                        1.776316   \n",
       "16            0.110746                        2.118421   \n",
       "21            0.107675                        2.368421   \n",
       "..                 ...                             ...   \n",
       "70            0.103715                        5.078947   \n",
       "87            0.092579                        4.210526   \n",
       "75            0.101608                        5.171053   \n",
       "80            0.099030                        5.315789   \n",
       "85            0.096053                        5.421053   \n",
       "\n",
       "    Too bad (raw mean by sent)  Too good (raw mean by sent):  N distractors  \n",
       "6                     1.105263                      0.394737       3.236842  \n",
       "11                    1.500000                      0.460526       3.894737  \n",
       "9                     1.197368                      0.473684       3.447368  \n",
       "16                    1.855263                      0.552632       4.526316  \n",
       "21                    2.171053                      0.618421       5.157895  \n",
       "..                         ...                           ...            ...  \n",
       "70                   10.157895                      1.763158      17.000000  \n",
       "87                    8.763158                      1.407895      14.381579  \n",
       "75                   11.000000                      1.828947      18.000000  \n",
       "80                   11.802632                      1.881579      19.000000  \n",
       "85                   12.657895                      1.921053      20.000000  \n",
       "\n",
       "[84 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Table1.loc[Table1[\"N distractors\"]>3].sort_values(by=[\"Appropriate (by sent)\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table1.to_csv(\"data/ParamAndClfSelection.csv\", sep=';')\n",
    "Table1.to_excel(\"data/ParamAndClfSelection.xlsx\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим аутпут от лучшей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  0%|          | 0/76 [00:00<?, ?it/s]\n",
      " 50%|█████     | 38/76 [00:00<00:00, 380.00it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 373.41it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 374.38it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m distractor_generator --n 4 --filename gold_standard/gold_standard_input.csv --clf_path XGBAllFeats/clf.pkl --cols_path XGBAllFeats/cols.json --output_filename gold_standard/best_model_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/dataset_lexics_final3.csv\",\n",
    "    sep=';',\n",
    "    index_col=\"Unnamed: 0\"\n",
    ")\n",
    "\n",
    "df = df.loc[df[\"Delete\"]!=1.0]\n",
    "\n",
    "df = df.drop([\"Delete\",\"Revisited1\"], axis=1)\n",
    "\n",
    "df = df.dropna(subset=[\"target_true\"])\n",
    "df[\"target_true\"] = df[\"target_true\"].astype(np.double).astype(np.int64)\n",
    "df[\"target\"] = df[\"target\"].astype(np.double).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>target</th>\n",
       "      <th>variant</th>\n",
       "      <th>correction</th>\n",
       "      <th>masked_sent</th>\n",
       "      <th>variant_count</th>\n",
       "      <th>correction_count</th>\n",
       "      <th>error_type</th>\n",
       "      <th>target_true</th>\n",
       "      <th>File</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>88.0</td>\n",
       "      <td>1</td>\n",
       "      <td>understandings</td>\n",
       "      <td>perceptions</td>\n",
       "      <td>To start with, happiness is a feeling of comf...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2014_EZa_13_2</td>\n",
       "      <td>exam/Exam2014</td>\n",
       "      <td>exam/Exam2014/2014_EZa_13_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>93.0</td>\n",
       "      <td>1</td>\n",
       "      <td>strictly</td>\n",
       "      <td>absolutely</td>\n",
       "      <td>I know that she was healthy and that her birt...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2014_EZa_13_2</td>\n",
       "      <td>exam/Exam2014</td>\n",
       "      <td>exam/Exam2014/2014_EZa_13_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0</td>\n",
       "      <td>completely</td>\n",
       "      <td>absolutely</td>\n",
       "      <td>I know that she was healthy and that her birt...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>0</td>\n",
       "      <td>2014_EZa_13_2</td>\n",
       "      <td>exam/Exam2014</td>\n",
       "      <td>exam/Exam2014/2014_EZa_13_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0</td>\n",
       "      <td>definitely</td>\n",
       "      <td>absolutely</td>\n",
       "      <td>I know that she was healthy and that her birt...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2014_EZa_13_2</td>\n",
       "      <td>exam/Exam2014</td>\n",
       "      <td>exam/Exam2014/2014_EZa_13_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>171.0</td>\n",
       "      <td>0</td>\n",
       "      <td>number</td>\n",
       "      <td>level</td>\n",
       "      <td>The chart below represents the information abo...</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2017_OBy_85_1</td>\n",
       "      <td>exam/Exam2017/OBy_1-99</td>\n",
       "      <td>exam/Exam2017/OBy_1-99/2017_OBy_85_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695022</th>\n",
       "      <td>37399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>bring</td>\n",
       "      <td>lead</td>\n",
       "      <td>That people should reduce the amount of air t...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2016_JSl_45_2</td>\n",
       "      <td>exam/Exam2016</td>\n",
       "      <td>exam/Exam2016/2016_JSl_45_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695024</th>\n",
       "      <td>37399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>leave</td>\n",
       "      <td>lead</td>\n",
       "      <td>That people should reduce the amount of air t...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>0</td>\n",
       "      <td>2016_JSl_45_2</td>\n",
       "      <td>exam/Exam2016</td>\n",
       "      <td>exam/Exam2016/2016_JSl_45_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695025</th>\n",
       "      <td>37399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>provide</td>\n",
       "      <td>lead</td>\n",
       "      <td>That people should reduce the amount of air t...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>0</td>\n",
       "      <td>2016_JSl_45_2</td>\n",
       "      <td>exam/Exam2016</td>\n",
       "      <td>exam/Exam2016/2016_JSl_45_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695026</th>\n",
       "      <td>37399.0</td>\n",
       "      <td>1</td>\n",
       "      <td>result</td>\n",
       "      <td>lead</td>\n",
       "      <td>That people should reduce the amount of air t...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2016_JSl_45_2</td>\n",
       "      <td>exam/Exam2016</td>\n",
       "      <td>exam/Exam2016/2016_JSl_45_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695029</th>\n",
       "      <td>37399.0</td>\n",
       "      <td>0</td>\n",
       "      <td>cause</td>\n",
       "      <td>lead</td>\n",
       "      <td>That people should reduce the amount of air t...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>lex_item_choice</td>\n",
       "      <td>1</td>\n",
       "      <td>2016_JSl_45_2</td>\n",
       "      <td>exam/Exam2016</td>\n",
       "      <td>exam/Exam2016/2016_JSl_45_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2838 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sent_id  target         variant   correction  \\\n",
       "Unnamed: 0                                                 \n",
       "1167           88.0       1  understandings  perceptions   \n",
       "1173           93.0       1        strictly   absolutely   \n",
       "1174           93.0       0      completely   absolutely   \n",
       "1187           93.0       0      definitely   absolutely   \n",
       "2332          171.0       0          number        level   \n",
       "...             ...     ...             ...          ...   \n",
       "695022      37399.0       0           bring         lead   \n",
       "695024      37399.0       0           leave         lead   \n",
       "695025      37399.0       0         provide         lead   \n",
       "695026      37399.0       1          result         lead   \n",
       "695029      37399.0       0           cause         lead   \n",
       "\n",
       "                                                  masked_sent  variant_count  \\\n",
       "Unnamed: 0                                                                     \n",
       "1167         To start with, happiness is a feeling of comf...              1   \n",
       "1173         I know that she was healthy and that her birt...              2   \n",
       "1174         I know that she was healthy and that her birt...              2   \n",
       "1187         I know that she was healthy and that her birt...              1   \n",
       "2332        The chart below represents the information abo...             13   \n",
       "...                                                       ...            ...   \n",
       "695022       That people should reduce the amount of air t...              2   \n",
       "695024       That people should reduce the amount of air t...              2   \n",
       "695025       That people should reduce the amount of air t...              1   \n",
       "695026       That people should reduce the amount of air t...              1   \n",
       "695029       That people should reduce the amount of air t...              1   \n",
       "\n",
       "            correction_count       error_type  target_true           File  \\\n",
       "Unnamed: 0                                                                  \n",
       "1167                       1  lex_item_choice            1  2014_EZa_13_2   \n",
       "1173                       5  lex_item_choice            1  2014_EZa_13_2   \n",
       "1174                       5  lex_item_choice            0  2014_EZa_13_2   \n",
       "1187                       5  lex_item_choice            1  2014_EZa_13_2   \n",
       "2332                      39  lex_item_choice            1  2017_OBy_85_1   \n",
       "...                      ...              ...          ...            ...   \n",
       "695022                    10  lex_item_choice            1  2016_JSl_45_2   \n",
       "695024                    10  lex_item_choice            0  2016_JSl_45_2   \n",
       "695025                    10  lex_item_choice            0  2016_JSl_45_2   \n",
       "695026                    10  lex_item_choice            1  2016_JSl_45_2   \n",
       "695029                    10  lex_item_choice            1  2016_JSl_45_2   \n",
       "\n",
       "                            Folder                              Filename  \n",
       "Unnamed: 0                                                                \n",
       "1167                 exam/Exam2014           exam/Exam2014/2014_EZa_13_2  \n",
       "1173                 exam/Exam2014           exam/Exam2014/2014_EZa_13_2  \n",
       "1174                 exam/Exam2014           exam/Exam2014/2014_EZa_13_2  \n",
       "1187                 exam/Exam2014           exam/Exam2014/2014_EZa_13_2  \n",
       "2332        exam/Exam2017/OBy_1-99  exam/Exam2017/OBy_1-99/2017_OBy_85_1  \n",
       "...                            ...                                   ...  \n",
       "695022               exam/Exam2016           exam/Exam2016/2016_JSl_45_2  \n",
       "695024               exam/Exam2016           exam/Exam2016/2016_JSl_45_2  \n",
       "695025               exam/Exam2016           exam/Exam2016/2016_JSl_45_2  \n",
       "695026               exam/Exam2016           exam/Exam2016/2016_JSl_45_2  \n",
       "695029               exam/Exam2016           exam/Exam2016/2016_JSl_45_2  \n",
       "\n",
       "[2838 rows x 12 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sent_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.629156010230179"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)/df[\"sent_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7135549872122762"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"target_true\"] == 1.0])/df[\"sent_id\"].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb027b1bff303369109a8d50bcc199fe80021bce48afce4744d88617980fbd0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
